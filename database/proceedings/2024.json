[
  {
    "title": "Formal Modeling of Structural Repetition Using Tree Compression",
    "author": [
      "Zeng Ren",
      "Yannis Rammos",
      "Martin A. Rohrmeier"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877282",
    "url": "https://doi.org/10.5281/zenodo.14877282",
    "ee": "https://zenodo.org/record/14877282/files/000001.pdf",
    "pages": "53-60",
    "abstract": "Repetition is central to musical structure as it gives rise both to piece-wise and stylistic coherence. Identifying repetitions in music is computationally not trivial, especially when they are varied or deeply hidden within tree-like structures. Rather than focusing on repetitions of musical events, we propose to pursue repeated structural relations between events. More specifically, given a context-free grammar that describes a tonal structure, we aim to computationally identify such relational repetitions within the derivation tree of the grammar. To this end, we first introduce the Template, a grammar-generic structure for generating trees that contain structural repetitions. We then approach the discovery of structural repetitions as a search for optimally compressible Templates that describe a corpus of pieces in the form of production-rule-labeled trees. To make it tractable, we develop a heuristic, inspired by tree compression algorithms, to approximate the optimally compressible Templates of the corpus. After implementing the algorithm in Haskell, we apply it to a corpus of jazz harmony trees, where we assess its performance based on the compressibility of the resulting Templates and the music-theoretical relevance of the identified repetitions.",
    "zenodo_id": 14877282,
    "dblp_key": null
  },
  {
    "title": "Saraga Audiovisual: A Large Multimodal Open Data Collection for the Analysis of Carnatic Music",
    "author": [
      "Adithi Shankar",
      "Gen\u00eds Plaja-Roglans",
      "Thomas Nuttall",
      "Mart\u00edn Rocamora",
      "Xavier Serra"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877279",
    "url": "https://doi.org/10.5281/zenodo.14877279",
    "ee": "https://zenodo.org/record/14877279/files/000002.pdf",
    "pages": "61-69",
    "abstract": "Carnatic music is a style of South Indian art music whose analysis using computational methods is an active area of research in Music Information Research (MIR). A core, open dataset for such analysis is the Saraga dataset, which includes multi-stem audio, expert annotations, and accompanying metadata. However, it has been noted that there are several limitations to the Saraga collections, and that additional relevant aspects of the tradition still need to be covered to facilitate musicologically important research lines. In this work, we present Saraga Audiovisual, a dataset that includes new and more diverse renditions of Carnatic vocal performances, totalling 42 concerts and more than 60 hours of music. A major contribution of this dataset is the inclusion of video recordings for all concerts, allowing for a wide range of multimodal analyses. We also provide high-quality human pose estimation data of the musicians extracted from the video footage, and perform benchmarking experiments for the different modalities to validate the utility of the novel collection. Saraga Audiovisual, along with access tools and results of our experiments, is made available for research purposes.",
    "zenodo_id": 14877279,
    "dblp_key": null
  },
  {
    "title": "X-Cover: Better Music Version Identification System by Integrating Pretrained ASR Model",
    "author": [
      "Xingjian Du",
      "Mingyu Liu",
      "Pei Zou",
      "Xia Liang",
      "Zijie Wang",
      "Huidong Liang",
      "Bilei Zhu"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877280",
    "url": "https://doi.org/10.5281/zenodo.14877280",
    "ee": "https://zenodo.org/record/14877280/files/000003.pdf",
    "pages": "70-77",
    "abstract": "Methods based on deep learning have emerged as a dominant approach for cover song identification (CSI) literature over the past years, among which ByteCover systems have consistently delivered state-of-the-art performance across major CSI datasets in the field. Despite its steady improvements along previous generations from audio feature dimensionality reduction to short query identification, the system is found to be vulnerable to audios with noise and ambiguous melody when extracting musical information from constant-Q transformation (CQT) spectrograms. Although some recent studies suggest that incorporating lyric-related features can enhance the overall performance of CSI systems, this approach typically requires training a separate automatic lyric recognition (ALR) model to extract lyric-related features from music recordings. In this work, we introduce X-Cover, the latest CSI system that incorporates a pre-trained automatic speech recognition (ASR) module, Whisper, to extract and integrate lyrics-related features into modelling. Specifically, we jointly fine-tune the ASR block and the previous ByteCover3 system in a parameter-efficient fashion, which largely reduces the cost of using lyric information compared to training a new ALR model from scratch. In addition, a bag of tricks is further applied to the training of this new generation, assisting X-Cover to achieve strong performance across various datasets.",
    "zenodo_id": 14877280,
    "dblp_key": null
  },
  {
    "title": "Harmonic and Transposition Constraints Arising From the Use of the Roland TR-808 Bass Drum",
    "author": [
      "Emmanuel Deruty"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877286",
    "url": "https://doi.org/10.5281/zenodo.14877286",
    "ee": "https://zenodo.org/record/14877286/files/000004.pdf",
    "pages": "78-85",
    "abstract": "The study investigates hip-hop music producer Scott Storch\u2019s approach to tonality, where the song\u2019s key is transposed to fit the Roland TR-808 bass drum instead of tuning the drums to the song\u2019s key. This process, involving the adjustment of all tracks except the bass drum, suggests significant production motives. The primary constraint stems from the limited usable pitch range of the TR-808 bass drum if its characteristic sound is to be preserved. The research examines drum tuning practices, the role of the Roland TR-808 in music, and the sub-bass qualities of its bass drum. Analysis of TR-808 samples reveals their characteristics and their integration into modern genres like trap and hip-hop. The study also considers the impact of loudspeaker frequency response and human ear sensitivity on bass drum perception. The findings suggest that Storch\u2019s method prioritizes the spectral properties of the bass drum over traditional pitch values to enhance the bass response. The need to maintain the unique sound of the TR-808 bass drum underscores the importance of spectral formants and register in contemporary popular music production.",
    "zenodo_id": 14877286,
    "dblp_key": null
  },
  {
    "title": "FruitsMusic: A Real-World Corpus of Japanese Idol-Group Songs",
    "author": [
      "Hitoshi Suda",
      "Shunsuke Yoshida",
      "Tomohiko Nakamura",
      "Satoru Fukayama",
      "Jun Ogata"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877287",
    "url": "https://doi.org/10.5281/zenodo.14877287",
    "ee": "https://zenodo.org/record/14877287/files/000005.pdf",
    "pages": "86-94",
    "abstract": "This study presents FruitsMusic, a metadata corpus of Japanese idol-group songs in the real world, precisely annotated with who sings what and when. Japanese idol-group songs, vital to Japanese pop culture, feature a unique vocal arrangement style, where songs are divided into several segments, and a specific individual or multiple singers are assigned to each segment. To enhance singer diarization methods for recognizing such structures, we constructed FruitsMusic as a resource using 40 music videos of Japanese idol groups from YouTube. The corpus includes detailed annotations, covering songs across various genres, division and assignment styles, and groups ranging from 4 to 9 members. FruitsMusic also facilitates the development of various music information retrieval techniques, such as lyrics transcription and singer identification, benefiting not only Japanese idol-group songs but also a wide range of songs featuring single or multiple singers from various cultures. This paper offers a comprehensive overview of FruitsMusic, including its creation methodology and unique characteristics compared to conversational speech. Additionally, this paper evaluates the efficacy of current methods for singer embedding extraction and diarization in challenging real-world conditions using FruitsMusic. Furthermore, this paper examines potential improvements in automatic diarization performance through evaluating human performance.",
    "zenodo_id": 14877287,
    "dblp_key": null
  },
  {
    "title": "Classical Guitar Duet Separation Using GuitarDuets - A Dataset of Real and Synthesized Guitar Recordings",
    "author": [
      "Marios Glytsos",
      "Christos Garoufis",
      "Athanasia Zlatintsi",
      "Petros Maragos"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877285",
    "url": "https://doi.org/10.5281/zenodo.14877285",
    "ee": "https://zenodo.org/record/14877285/files/000006.pdf",
    "pages": "95-102",
    "abstract": "Recent advancements in music source separation (MSS) have focused in the multi-timbral case, with existing architectures tailored for the separation of distinct instruments, overlooking thus the challenge of separating instruments with similar timbral characteristics. Addressing this gap, our work focuses on monotimbral MSS, specifically within the context of classical guitar duets. To this end, we introduce the GuitarDuets dataset, featuring a combined total of approximately three hours of real and synthesized classical guitar duet recordings, as well as note-level annotations of the synthesized duets. We perform an extensive cross-dataset evaluation by adapting Demucs, a state-of-the-art MSS architecture, to monotimbral source separation. Furthermore, we develop a joint permutation-invariant transcription and separation framework, to exploit note event predictions as auxiliary information. Our results indicate that utilizing both the real and synthesized subsets of GuitarDuets leads to improved separation performance in an independently recorded test set compared to utilizing solely one subset. We also find that while the availability of ground-truth note labels greatly helps the performance of the separation network, the predicted note estimates result only in marginal improvement. Finally, we discuss the behavior of commonly utilized metrics, such as SDR and SI-SDR, in the context of monotimbral MSS.",
    "zenodo_id": 14877285,
    "dblp_key": null
  },
  {
    "title": "Can LLMs \"Reason\" in Music? an Evaluation of LLMs' Capability of Music Understanding and Generation",
    "author": [
      "Ziya Zhou",
      "Yuhang Wu",
      "Zhiyue Wu",
      "Xinyue Zhang",
      "Ruibin Yuan",
      "Yinghao Ma",
      "Lu Wang",
      "Emmanouil Benetos",
      "Wei Xue",
      "Yike Guo"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877281",
    "url": "https://doi.org/10.5281/zenodo.14877281",
    "ee": "https://zenodo.org/record/14877281/files/000007.pdf",
    "pages": "103-110",
    "abstract": "Symbolic Music, akin to language, can be encoded in discrete symbols. Recent research has extended the application of large language models (LLMs) such as GPT-4 and Llama2 to the symbolic music domain including understanding and generation. Yet scant research explores the details of how these LLMs perform on advanced music understanding and conditioned generation, especially from the multi-step reasoning perspective, which is a critical aspect in the conditioned, editable, and interactive human-computer co-creation process. This study conducts a thorough investigation of LLMs\u2019 capability and limitations in symbolic music processing. We identify that current LLMs exhibit poor performance in song-level multi-step music reasoning, and typically fail to leverage learned music knowledge when addressing complex musical tasks. An analysis of LLMs\u2019 responses highlights distinctly their pros and cons. Our findings suggest achieving advanced musical capability is not intrinsically obtained by LLMs, and future research should focus more on bridging the gap between music knowledge and reasoning, to improve the co-creation experience for musicians.",
    "zenodo_id": 14877281,
    "dblp_key": null
  },
  {
    "title": "Music2Latent: Consistency Autoencoders for Latent Audio Compression",
    "author": [
      "Marco Pasini",
      "Stefan Lattner",
      "George Fazekas"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877289",
    "url": "https://doi.org/10.5281/zenodo.14877289",
    "ee": "https://zenodo.org/record/14877289/files/000008.pdf",
    "pages": "111-119",
    "abstract": "Efficient audio representations in a compressed continuous latent space are critical for generative audio modeling and Music Information Retrieval (MIR) tasks. However, some existing audio autoencoders have limitations, such as multi-stage training procedures, slow iterative sampling, or low reconstruction quality. We introduce Music2Latent, an audio autoencoder that overcomes these limitations by leveraging consistency models. Music2Latent encodes samples into a compressed continuous latent space in a single end-to-end training process while enabling high-fidelity single-step reconstruction. Key innovations include conditioning the consistency model on upsampled encoder outputs at all levels through cross connections, using frequency-wise self-attention to capture long-range frequency dependencies, and employing frequency-wise learned scaling to handle varying value distributions across frequencies at different noise levels. We demonstrate that Music2Latent outperforms existing continuous audio autoencoders in sound quality and reconstruction accuracy while achieving competitive performance on downstream MIR tasks using its latent representations. To our knowledge, this represents the first successful attempt at training an end-to-end consistency autoencoder model.",
    "zenodo_id": 14877289,
    "dblp_key": null
  },
  {
    "title": "Robust and Accurate Audio Synchronization Using Raw Features From Transcription Models",
    "author": [
      "Johannes Zeitler",
      "Ben Maman",
      "Meinard M\u00fcller"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877291",
    "url": "https://doi.org/10.5281/zenodo.14877291",
    "ee": "https://zenodo.org/record/14877291/files/000009.pdf",
    "pages": "120-127",
    "abstract": "In music information retrieval (MIR), precise synchronization of musical events is crucial for tasks like aligning symbolic information with music recordings or transferring annotations between audio versions. To achieve high temporal accuracy, synchronization approaches integrate onset-related information extracted from music recordings using either traditional signal processing techniques or exploiting symbolic representations obtained by data-driven automated music transcription (AMT) approaches. In line with this research direction, our paper introduces a high-resolution synchronization approach that combines recent AMT techniques with traditional synchronization methods. Rather than relying on the final symbolic AMT results, we show how to exploit raw onset and frame predictions obtained as intermediate outcomes from a state-of-the-art AMT approach. Through extensive evaluations conducted on piano recordings under varied acoustic conditions across different transcription models, audio features, and dynamic time warping variants, we illustrate the advantages of our proposed method in both audio\u2013audio and audio\u2013score synchronization tasks. Specifically, we emphasize the effectiveness of our approach in aligning historical piano recordings with poor audio quality. We underscore how additional fine-tuning steps of the transcription model on the target dataset enhance alignment robustness, even in challenging acoustic environments.",
    "zenodo_id": 14877291,
    "dblp_key": null
  },
  {
    "title": "Harnessing the Power of Distributions: Probabilistic Representation Learning on Hypersphere for Multimodal Music Information Retrieval",
    "author": [
      "Takayuki Nakatsuka",
      "Masahiro Hamasaki",
      "Masataka Goto"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877293",
    "url": "https://doi.org/10.5281/zenodo.14877293",
    "ee": "https://zenodo.org/record/14877293/files/000010.pdf",
    "pages": "128-136",
    "abstract": "Probabilistic representation learning provides intricate and diverse representations of music content by characterizing the latent features of each content item as a probability distribution within a certain space. However, typical Music Information Retrieval (MIR) methods based on representation learning utilize a feature vector of each content item, thereby missing some details of their distributional properties. In this study, we propose a probabilistic representation learning method for multimodal MIR based on contrastive learning and optimal transport. Our method trains encoders that map each content item to a hypersphere so that the probability distributions of a positive pair of content items become close to each other, while those of an irrelevant pair are far apart. To achieve such training, we design novel loss functions that utilize both probabilistic contrastive learning and spherical sliced-Wasserstein distances. We demonstrate our method\u2019s effectiveness on benchmark datasets as well as its suitability for multimodal MIR through both a quantitative evaluation and a qualitative analysis.",
    "zenodo_id": 14877293,
    "dblp_key": null
  },
  {
    "title": "Towards Automated Personal Value Estimation in Song Lyrics",
    "author": [
      "Andrew M. Demetriou",
      "Jaehun Kim",
      "Sandy Manolios",
      "Cynthia Liem"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877295",
    "url": "https://doi.org/10.5281/zenodo.14877295",
    "ee": "https://zenodo.org/record/14877295/files/000011.pdf",
    "pages": "137-145",
    "abstract": "Most music widely consumed in Western Countries contains song lyrics, with U.S. samples reporting almost all of their song libraries contain lyrics. In parallel, social science theory suggests that personal values - the abstract goals that guide our decisions and behaviors - play an important role in communication: we share what is important to us to coordinate efforts, solve problems and meet challenges. Thus, the values communicated in song lyrics may be similar or different to those of the listener, and by extension affect the listener\u2019s reaction to the song. This suggests that working towards automated estimation of values in lyrics may assist in downstream MIR tasks, in particular, personalization. However, as highly subjective text, song lyrics present a challenge in terms of sampling songs to be annotated, annotation methods, and in choosing a method for aggregation. In this project, we take a perspectivist approach, guided by social science theory, to gathering annotations, estimating their quality, and aggregating them. We then compare aggregated ratings to estimates based on pre-trained sentence/word embedding models by employing a validated value dictionary. We discuss conceptually \u2019fuzzy\u2019 solutions to sampling and annotation challenges, promising initial results in annotation quality and in automated estimations, and future directions.",
    "zenodo_id": 14877295,
    "dblp_key": null
  },
  {
    "title": "Audio Conditioning for Music Generation via Discrete Bottleneck Features",
    "author": [
      "Simon Rouard",
      "Yossi Adi",
      "Jade Copet",
      "Axel Roebel",
      "Alexandre Defossez"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877297",
    "url": "https://doi.org/10.5281/zenodo.14877297",
    "ee": "https://zenodo.org/record/14877297/files/000012.pdf",
    "pages": "146-153",
    "abstract": "While most music generation models use textual or parametric conditioning (e.g. tempo, harmony, musical genre), we propose to condition a language model based music generation system with audio input. Our exploration involves two distinct strategies. The first strategy, termed textual inversion, leverages a pre-trained text-to-music model to map audio input to corresponding \"pseudowords\" in the textual embedding space. For the second model we train a music language model from scratch jointly with a text conditioner and a quantized audio feature extractor. At inference time, we can mix textual and audio conditioning and balance them thanks to a novel double classifier free guidance method. We conduct automatic and human studies that validates our approach. We will release the code and we provide music samples on musicgenstyle.github.io in order to show the quality of our model.",
    "zenodo_id": 14877297,
    "dblp_key": null
  },
  {
    "title": "Variation Transformer: New Datasets, Models, and Comparative Evaluation for Symbolic Music Variation Generation",
    "author": [
      "Chenyu Gao",
      "Federico Reuben",
      "Tom Collins"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877299",
    "url": "https://doi.org/10.5281/zenodo.14877299",
    "ee": "https://zenodo.org/record/14877299/files/000013.pdf",
    "pages": "154-163",
    "abstract": "Variation in music is defined as repetition of a theme, but with various modifications, playing an important role in many musical genres in developing core music ideas into longer passages. Existing research on variation in music is mostly confined to datasets consisting of classical theme-and-variation pieces, and generative models limited to melody-only representations. In this paper, to address the problem of the lack of datasets, we propose an algorithm to extract theme-and-variation pairs automatically, and use it to annotate two datasets called POP909-TVar (2,871 theme-and-variation pairs) and VGMIDI-TVar (7,830 theme-and-variation pairs). We propose both non-deep learning and deep learning based symbolic music variation generation models, and report the results of a listening study and feature-based evaluation for these models. One of our two newly proposed models, called Variation Transformer, outperforms all other models that listeners evaluated for \u201cvariation success\u201d, including non-deep learning and deep learning based approaches. An implication of this work for the wider field of music making is that we now have a model that can generate material with stronger and perceivably more successful relationships to some given prompt or theme.",
    "zenodo_id": 14877299,
    "dblp_key": null
  },
  {
    "title": "Automatic Detection of Moral Values in Music Lyrics",
    "author": [
      "Vjosa Preniqi",
      "Iacopo Ghinassi",
      "Julia Ive",
      "Kyriaki Kalimeri",
      "Charalampos Saitis"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877301",
    "url": "https://doi.org/10.5281/zenodo.14877301",
    "ee": "https://zenodo.org/record/14877301/files/000014.pdf",
    "pages": "164-172",
    "abstract": "Moral values play a fundamental role in how we evaluate information, make decisions, and form judgements around important social issues. The possibility to extract morality rapidly from lyrics enables a deeper understanding of our music-listening behaviours. Building on the Moral Foundations Theory (MFT), we tasked a set of transformer-based language models (BERT) fine-tuned on 2,721 synthetic lyrics generated by a large language model (GPT-4) to detect moral values in 200 real music lyrics annotated by two experts. We evaluate their predictive capabilities against a series of baselines including out-of-domain (BERT fine-tuned on MFT-annotated social media texts) and zero-shot (GPT-4) classification. The proposed models yielded the best accuracy across experiments, with an average F1 weighted score of 0.8. This performance is, on average, 5% higher than out-of-domain and zero-shot models. When examining precision in binary classification, the proposed models perform on average 12% higher than the baselines. Our approach contributes to annotation-free and effective lyrics morality learning, and provides useful insights into the knowledge distillation of LLMs regarding moral expression in music, and the potential impact of these technologies on the creative industries and musical culture.",
    "zenodo_id": 14877301,
    "dblp_key": null
  },
  {
    "title": "Semi-Supervised Piano Transcription Using Pseudo-Labeling Techniques",
    "author": [
      "Sebastian Strahl",
      "Meinard M\u00fcller"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877303",
    "url": "https://doi.org/10.5281/zenodo.14877303",
    "ee": "https://zenodo.org/record/14877303/files/000015.pdf",
    "pages": "173-181",
    "abstract": "Automatic piano transcription (APT) transforms piano recordings into symbolic note events. In recent years, APT has relied on supervised deep learning, which demands a large amount of labeled data that is often limited. This paper introduces a semi-supervised approach to APT, leveraging unlabeled data with techniques originally introduced in computer vision (CV): pseudo-labeling, consistency regularization, and distribution matching. The idea of pseudo-labeling is to use the current model for producing artificial labels for unlabeled data, and consistency regularization makes the model\u2019s predictions for unlabeled data robust to augmentations. Finally, distribution matching ensures that the pseudo-labels follow the same marginal distribution as the reference labels, adding an extra layer of robustness. Our method, tested on three piano datasets, shows improvements over purely supervised methods and performs comparably to existing semi-supervised approaches. Conceptually, this work illustrates that semi-supervised learning techniques from CV can be effectively transferred to the music domain, considerably reducing the dependence on large annotated datasets.",
    "zenodo_id": 14877303,
    "dblp_key": null
  },
  {
    "title": "Note-Level Transcription of Choral Music",
    "author": [
      "Huiran Yu",
      "Zhiyao Duan"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877305",
    "url": "https://doi.org/10.5281/zenodo.14877305",
    "ee": "https://zenodo.org/record/14877305/files/000016.pdf",
    "pages": "182-188",
    "abstract": "Choral music is a musical activity with one of the largest participant bases, yet it has drawn little attention from automatic music transcription research. The main reasons we argue are due to the lack of data and technical difficulties arise from diverse acoustic conditions and unique properties of choral singing. To address these challenges, in this paper we propose a Transformer-based framework for note-level transcription of choral music. This framework bypasses the frame-level processing and directly produces a sequence of notes with associated timestamps. We also introduce YouChorale, a novel choral music dataset in a cappella setting curated from the Internet. YouChorale contains 452 real-world recordings in diverse acoustic configurations of choral music from over 100 composers as well as their MIDI scores. Trained on YouChorale, our proposed model achieves state-of-the-art performance in choral music transcription, marking a significant advancement in the field.",
    "zenodo_id": 14877305,
    "dblp_key": null
  },
  {
    "title": "Learning Multifaceted Self-Similarity Over Time and Frequency for Music Structure Analysis",
    "author": [
      "Tsung-Ping Chen",
      "Kazuyoshi Yoshii"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877309",
    "url": "https://doi.org/10.5281/zenodo.14877309",
    "ee": "https://zenodo.org/record/14877309/files/000017.pdf",
    "pages": "189-197",
    "abstract": "This paper describes a deep learning method for music structure analysis (MSA) that aims to split a music signal into temporal segments and assign a function label (e.g., intro, verse, or chorus) to each segment. The computational base for MSA is a spectro-temporal representation of input audio such as the spectrogram, where the compositional relationships of the spectral components provide valuable clues (e.g., chords) to the identification of structural units. However, such implicit features might be vulnerable to local operations such as convolution and pooling operations. In this paper, we hypothesize that the self-attention over the spectral domain as well as the temporal domain plays a key role in tackling MSA. Based on this hypothesis, we propose a novel MSA model built on the Transformer-in-Transformer architecture that alternately stacks spectral and temporal self-attention layers. Experiments with the Beatles, RWC, and SALAMI datasets showed the superiority of the dual-aspect self-attention. In particular, the differentiation between spectral and temporal self-attentions can provide extra performance gain. By analyzing the attention maps, we also demonstrate that self-attention can unfold tonal relationships and the internal structure of music.",
    "zenodo_id": 14877309,
    "dblp_key": null
  },
  {
    "title": "A Contrastive Self-Supervised Learning Scheme for Beat Tracking Amenable to Few-Shot Learning",
    "author": [
      "Antonin Gagner\u00e9",
      "Slim Essid",
      "Geoffroy Peeters"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877308",
    "url": "https://doi.org/10.5281/zenodo.14877308",
    "ee": "https://zenodo.org/record/14877308/files/000018.pdf",
    "pages": "198-206",
    "abstract": "In this paper, we propose a novel Self-Supervised-Learning scheme to train rhythm analysis systems and instantiate it for few-shot beat tracking. Taking inspiration from the Contrastive Predictive Coding paradigm, we propose to train a Log-Mel-Spectrogram-Transformer-encoder to contrast observations at times separated by hypothesized beat intervals from those that are not. We do this without the knowledge of ground-truth tempo or beat positions, as we rely on the local maxima of a Predominant Local Pulse function, considered as a proxy for Tatum positions, to define candidate anchors, candidate positives (located at a distance of a power of two from the anchor) and negatives (remaining time positions). We show that a model pre-trained using this approach on the unlabeled FMA, MTT and MTG-Jamendo datasets can successfully be fine-tuned in the few-shot regime, i.e. with just a few annotated examples to get a competitive beat-tracking performance.",
    "zenodo_id": 14877308,
    "dblp_key": null
  },
  {
    "title": "Using Pairwise Link Prediction and Graph Attention Networks for Music Structure Analysis",
    "author": [
      "Morgan Buisson",
      "Brian McFee",
      "Slim Essid"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877311",
    "url": "https://doi.org/10.5281/zenodo.14877311",
    "ee": "https://zenodo.org/record/14877311/files/000019.pdf",
    "pages": "207-214",
    "abstract": "The task of music structure analysis has been mostly addressed as a sequential problem, by relying on the internal homogeneity of musical sections or their repetitions. In this work, we instead regard it as a pairwise link prediction task. If for any pair of time instants in a track, one can successfully predict whether they belong to the same structural entity or not, then the underlying structure can be easily recovered. Building upon this assumption, we propose a method that first learns to classify pairwise links between time frames as belonging to the same section (or segment) or not. The resulting link features, along with node-specific information, are combined through a graph attention network. The latter is regularized with a graph partitioning training objective and outputs boundary locations between musical segments and section labels. The overall system is lightweight and performs competitively with previous methods. The evaluation is done on two standard datasets for music structure analysis and an ablation study is conducted in order to gain insight on the role played by its different components.",
    "zenodo_id": 14877311,
    "dblp_key": null
  },
  {
    "title": "Six Dragons Fly Again: Reviving 15th-Century Korean Court Music With Transformers and Novel Encoding",
    "author": [
      "Danbinaerin Han",
      "Mark R. H. Gotham",
      "DongMin Kim",
      "Hannah Park",
      "Sihun Lee",
      "Dasaem Jeong"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877313",
    "url": "https://doi.org/10.5281/zenodo.14877313",
    "ee": "https://zenodo.org/record/14877313/files/000020.pdf",
    "pages": "217-224",
    "abstract": "We introduce a project that revives a piece of 15th-century Korean court music, Chihwapyeong and Chwipunghyeong, composed upon the poem Songs of the Dragon Flying to Heaven. One of the earliest examples of Jeongganbo, a Korean musical notation system, the remaining version only consists of a rudimentary melody. Our research team, commissioned by the National Gugak (Korean Traditional Music) Center, aimed to transform this old melody into a performable arrangement for a six-part ensemble. Using Jeongganbo data acquired through bespoke optical music recognition, we trained a BERT-like masked language model and an encoder-decoder transformer model. We also propose an encoding scheme that strictly follows the structure of Jeongganbo and denotes note durations as positions. The resulting machine-transformed version of Chihwapyeong and Chwipunghyeong were evaluated by experts and performed by the Court Music Orchestra of National Gugak Center. Our work demonstrates that generative models can successfully be applied to traditional music with limited training data if combined with careful design.",
    "zenodo_id": 14877313,
    "dblp_key": null
  },
  {
    "title": "Lessons Learned From a Project to Encode Mensural Music on a Large Scale With Optical Music Recognition",
    "author": [
      "David Rizo",
      "Jorge Calvo-Zaragoza",
      "Patricia Garc\u00eda-Iasci",
      "Teresa Delgado-S\u00e1nchez"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877315",
    "url": "https://doi.org/10.5281/zenodo.14877315",
    "ee": "https://zenodo.org/record/14877315/files/000021.pdf",
    "pages": "225-231",
    "abstract": "This paper discusses the transcription of a collection of musical works using Optical Music Recognition (OMR) technologies during the implementation of the Spanish PolifonIA project. The project employs a research-oriented OMR application that leverages modern Artificial Intelligence (AI) technology to encode musical works from images into structured formats. The paper outlines the transcription workflow in several phases: selection, preparation, action, and resolution, emphasizing the efficiency of using AI to reduce manual transcription efforts. The tool facilitated various tasks such as document analysis, management of parts, and automatic content recognition, although manual corrections were still indispensable for ensuring accuracy, especially for complex musical notations and layouts. Our study also highlights the iterative process of model training and corrections that gradually improved transcription speed and accuracy. Furthermore, the paper delves into challenges like managing non-musical elements and the limitations of current OMR technologies with early musical notations. Our findings suggest that while automated tools significantly accelerate the transcription process, they require continuous refinement and human oversight to handle diverse and complex musical documents effectively.",
    "zenodo_id": 14877315,
    "dblp_key": null
  },
  {
    "title": "The Changing Sound of Music: An Exploratory Corpus Study of Vocal Trends Over Time",
    "author": [
      "Elena Georgieva",
      "Pablo Ripoll\u00e9s",
      "Brian McFee"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877317",
    "url": "https://doi.org/10.5281/zenodo.14877317",
    "ee": "https://zenodo.org/record/14877317/files/000022.pdf",
    "pages": "232-239",
    "abstract": "Recent advancements in audio processing provide a new opportunity to study musical trends using quantitative methods. While past work has investigated trends in music over time, there has been no large-scale study on the evolution of vocal lines. In this work, we conduct an exploratory study of 145,912 vocal tracks of popular songs spanning 55 years, from 1955 to 2010. We use source separation to extract the vocal stem and fundamental frequency (f0) estimation to analyze pitch tracks. Additionally, we extract pitch characteristics including mean pitch, total variation, and pitch class entropy of each song. We conduct statistical analysis of vocal pitch across years and genres, and report significant trends in our metrics over time, as well as significant differences in trends between genres. Our study demonstrates the utility of this method for studying vocals, contributes to the understanding of vocal trends, and showcases the potential of quantitative approaches in musicology.",
    "zenodo_id": 14877317,
    "dblp_key": null
  },
  {
    "title": "Music Proofreading With RefinPaint: Where and How to Modify Compositions Given Context",
    "author": [
      "Pedro Ramoneda",
      "Mart\u00edn Rocamora",
      "Taketo Akama"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877319",
    "url": "https://doi.org/10.5281/zenodo.14877319",
    "ee": "https://zenodo.org/record/14877319/files/000023.pdf",
    "pages": "240-247",
    "abstract": "Autoregressive generative transformers are key in music generation, producing coherent compositions but facing challenges in human-machine collaboration. We propose RefinPaint, an iterative technique that improves the sampling process. It does this by identifying the weaker music elements using a feedback model, which then informs the choices for resampling by an inpainting model. This dualfocus methodology not only facilitates the machine\u2019s ability to improve its automatic inpainting generation through repeated cycles but also offers a valuable tool for humans seeking to refine their compositions with automatic proofreading. Experimental results suggest RefinPaint\u2019s effectiveness in inpainting and proofreading tasks, demonstrating its value for refining music created by both machines and humans. This approach not only facilitates creativity but also aids amateur composers in improving their work.",
    "zenodo_id": 14877319,
    "dblp_key": null
  },
  {
    "title": "Notewise Evaluation for Music Source Separation: A Case Study for Separated Piano Tracks",
    "author": [
      "Yigitcan \u00d6zer",
      "Hans-Ulrich Berendes",
      "Vlora Arifi-M\u00fcller",
      "Fabian-Robert St\u00f6ter",
      "Meinard M\u00fcller"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877321",
    "url": "https://doi.org/10.5281/zenodo.14877321",
    "ee": "https://zenodo.org/record/14877321/files/000024.pdf",
    "pages": "248-255",
    "abstract": "Deep learning has significantly advanced music source separation (MSS), aiming to decompose music recordings into individual tracks corresponding to singing or specific instruments. Typically, results are evaluated using quantitative measures like signal-to-distortion ratio (SDR) computed for entire excerpts or songs. As the main contribution of this article, we introduce a novel evaluation approach that decomposes an audio track into musically meaningful sound events and applies the evaluation metric based on these units. In a case study, we apply this strategy to the challenging task of separating piano concerto recordings into piano and orchestra tracks. To assess piano separation quality, we use a score-informed nonnegative matrix factorization approach to decompose the reference and separate piano tracks into notewise sound events. In our experiments assessing various MSS systems, we demonstrate that our notewise evaluation, which takes into account factors such as pitch range and musical complexity, enhances the comprehension of both the results of source separation and the intricacies within the underlying music.",
    "zenodo_id": 14877321,
    "dblp_key": null
  },
  {
    "title": "Automatic Estimation of Singing Voice Musical Dynamics",
    "author": [
      "Jyoti Narang",
      "Nazif Can Tamer",
      "Viviana De La Vega",
      "Xavier Serra"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877323",
    "url": "https://doi.org/10.5281/zenodo.14877323",
    "ee": "https://zenodo.org/record/14877323/files/000025.pdf",
    "pages": "256-263",
    "abstract": "Musical dynamics form a core part of expressive singing voice performances. However, automatic analysis of musical dynamics for singing voice has received limited attention partly due to the scarcity of suitable datasets and a lack of clear evaluation frameworks. To address this challenge, we propose a methodology for dataset curation. Employing the proposed methodology, we compile a dataset comprising 509 musical dynamics annotated singing voice performances, aligned with 163 score files, leveraging state-of-the-art source separation and alignment techniques. The scores are sourced from the OpenScore Lieder corpus of romantic-era compositions, widely known for its wealth of expressive annotations. Utilizing the curated dataset, we train a multi-head attention based CNN model with varying window sizes to evaluate the effectiveness of estimating musical dynamics. We explored two distinct perceptually motivated input representations for the model training: log-Mel spectrum and bark-scale based features. For testing, we manually curate another dataset of 25 musical dynamics annotated performances in collaboration with a professional vocalist. We conclude through our experiments that bark-scale based features outperform log-Mel-features for the task of singing voice dynamics prediction. The dataset along with the code is shared publicly for further research on the topic.",
    "zenodo_id": 14877323,
    "dblp_key": null
  },
  {
    "title": "Joint Audio and Symbolic Conditioning for Temporally Controlled Text-to-Music Generation",
    "author": [
      "Or Tal",
      "Alon Ziv",
      "Itai Gat",
      "Felix Kreuk",
      "Yossi Adi"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877325",
    "url": "https://doi.org/10.5281/zenodo.14877325",
    "ee": "https://zenodo.org/record/14877325/files/000026.pdf",
    "pages": "264-271",
    "abstract": "We present JASCO, a temporally controlled text-to-music generation model utilizing both symbolic and audio-based conditions. JASCO can generate high-quality music samples conditioned on global text descriptions along with fine-grained local controls. JASCO is based on the Flow Matching modeling paradigm together with a novel conditioning method that allows for both locally (e.g., chords) and globally (text description) controlled music generation. Specifically, we apply information bottleneck layers in conjunction with temporal blurring to extract relevant information with respect to specific controls. This allows the incorporation of both symbolic and audio-based conditions in the same text-to-music model. We experiment with various symbolic control signals (e.g., chords, melody), as well as with audio representations (e.g., separated drum tracks, full-mix). We evaluate JASCO considering both generation quality and condition adherence using objective metrics and human studies. Results suggest that JA S C O is comparable to the evaluated baselines considering generation quality while allowing significantly better and more versatile controls over the generated music. Samples are available on our demo page https://pages.cs.huji.ac.il/adiyoss-lab/JASCO",
    "zenodo_id": 14877325,
    "dblp_key": null
  },
  {
    "title": "Diff-a-Riff: Musical Accompaniment Co-Creation via Latent Diffusion Models",
    "author": [
      "Javier Nistal",
      "Marco Pasini",
      "Cyran Aouameur",
      "Maarten Grachten",
      "Stefan Lattner"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877327",
    "url": "https://doi.org/10.5281/zenodo.14877327",
    "ee": "https://zenodo.org/record/14877327/files/000027.pdf",
    "pages": "272-280",
    "abstract": "Recent advancements in deep generative models present new opportunities for music production but also pose challenges, such as high computational demands and limited audio quality. Moreover, current systems frequently rely solely on text input and typically focus on producing complete musical pieces, which is incompatible with existing workflows in music production. To address these issues, we introduce Diff-A-Riff, a Latent Diffusion Model designed to generate high-quality instrumental accompaniments adaptable to any musical context. This model offers control through either audio references, text prompts, or both, and produces 48kHz pseudo-stereo audio while significantly reducing inference time and memory usage. We demonstrate the model\u2019s capabilities through objective metrics and subjective listening tests, with extensive examples available on the accompanying website.",
    "zenodo_id": 14877327,
    "dblp_key": null
  },
  {
    "title": "Exploring Internet Radio Across the Globe With the MIRAGE Online Dashboard",
    "author": [
      "Ngan V.T. Nguyen",
      "Elizabeth Acosta",
      "Tommy Dang",
      "David Sears"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877329",
    "url": "https://doi.org/10.5281/zenodo.14877329",
    "ee": "https://zenodo.org/record/14877329/files/000028.pdf",
    "pages": "281-287",
    "abstract": "This study presents the Music Informatics for Radio Across the GlobE (MIRAGE) online dashboard, which allows users to access, interact with, and export metadata (e.g., artist name, track title) and musicological features (e.g., instrument list, voice type, key/mode) for 1 million events streaming on 10,000 internet radio stations across the globe. Users can search for stations or events according to several criteria, display, analyze, and listen to the selected station/event lists using interactive visualizations that include embedded links to streaming services, and finally export relevant metadata and visualizations for further study.",
    "zenodo_id": 14877329,
    "dblp_key": null
  },
  {
    "title": "MIDI-to-Tab: Guitar Tablature Inference via Masked Language Modeling",
    "author": [
      "Andrew C. Edwards",
      "Xavier Riley",
      "Pedro Pereira Sarmento",
      "Simon Dixon"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877333",
    "url": "https://doi.org/10.5281/zenodo.14877333",
    "ee": "https://zenodo.org/record/14877333/files/000029.pdf",
    "pages": "288-294",
    "abstract": "Guitar tablatures enrich the structure of traditional music notation by assigning each note to a string and fret of a guitar in a particular tuning, indicating precisely where to play the note on the instrument. The problem of generating tablature from a symbolic music representation involves inferring this string and fret assignment per note across an entire composition or performance. On the guitar, multiple string-fret assignments are possible for most pitches, which leads to a large combinatorial space that prevents exhaustive search approaches. Most modern methods use constraint-based dynamic programming to minimize some cost function (e.g. hand position movement). In this work, we introduce a novel deep learning solution to symbolic guitar tablature estimation. We train an encoder-decoder Transformer model in a masked language modeling paradigm to assign notes to strings. The model is first pre-trained on DadaGP, a dataset of over 25K tablatures, and then fine-tuned on a curated set of professionally transcribed guitar performances. Given the subjective nature of assessing tablature quality, we conduct a user study amongst guitarists, wherein we ask participants to rate the playability of multiple versions of tablature for the same four-bar excerpt. The results indicate our system significantly outperforms competing algorithms.",
    "zenodo_id": 14877333,
    "dblp_key": null
  },
  {
    "title": "Transcription-Based Lyrics Embeddings: Simple Extraction of Effective Lyrics Embeddings From Audio",
    "author": [
      "Jaehun Kim",
      "Florian Henkel",
      "Camilo Landau",
      "Samuel E. Sandberg",
      "Andreas F. Ehmann"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877331",
    "url": "https://doi.org/10.5281/zenodo.14877331",
    "ee": "https://zenodo.org/record/14877331/files/000030.pdf",
    "pages": "295-303",
    "abstract": "The majority of Western popular music contains lyrics. Previous studies have shown that lyrics are a rich source of information and are complementary to other information sources, such as audio. One factor that hinders the research and application of lyrics on a large scale is their availability. To mitigate this, we propose the use of transcriptionbased lyrics embeddings (TLE). These estimate \u2018groundtruth\u2019 lyrics embeddings given only audio as input. Central to this approach is the use of transcripts derived from an automatic lyrics transcription (ALT) system instead of human-transcribed, \u2018ground-truth\u2019 lyrics, making them substantially more accessible. We conduct an experiment to assess the effectiveness of TLEs across various music information retrieval (MIR) tasks. Our results indicate that TLEs can improve the performance of audio embeddings alone, especially when combined, closing the gap with cases where ground-truth lyrics information is available.",
    "zenodo_id": 14877331,
    "dblp_key": null
  },
  {
    "title": "A Method for MIDI Velocity Estimation for Piano Performance by a U-Net With Attention and FiLM",
    "author": [
      "Hyon Kim",
      "Xavier Serra"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877335",
    "url": "https://doi.org/10.5281/zenodo.14877335",
    "ee": "https://zenodo.org/record/14877335/files/000031.pdf",
    "pages": "304-310",
    "abstract": "It is a well known fact that the dynamics in piano performance gives significant effect in expressiveness. Taking the polyphonic nature of the instrument into account, analysing information to form dynamics for each performed note has significant meaning to understand piano performance in a quantitative way. It is also a key element in an education context for piano learners. In this study, we developed a model for estimating MIDI velocity for each note, as one of indicators to represent loudness, with a condition of score assuming educational use case, by a Deep Neural Network (DNN) utilizing a U-Net with Scaled Dot-Product Attention (Attention) and Feature-wise Linear Modulation (FiLM) conditioning. As a result, we prove that effectiveness of Attention and FiLM conditioning, improved estimation accuracy and achieved the best result among previous researches using DNNs and showed its robustness across the various domain of test data.",
    "zenodo_id": 14877335,
    "dblp_key": null
  },
  {
    "title": "MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music Generation",
    "author": [
      "Yun-Han Lan",
      "Wen-Yi Hsiao",
      "Hao-Chung Cheng",
      "Yi-Hsuan Yang"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877337",
    "url": "https://doi.org/10.5281/zenodo.14877337",
    "ee": "https://zenodo.org/record/14877337/files/000032.pdf",
    "pages": "311-318",
    "abstract": "Existing text-to-music models can produce high-quality audio with great diversity. However, textual prompts alone cannot precisely control temporal musical features such as chords and rhythm of the generated music. To address this challenge, we introduce MusiConGen, a temporallyconditioned Transformer-based text-to-music model that builds upon the pretrained MusicGen framework. Our innovation lies in an efficient finetuning mechanism, tailored for consumer-grade GPUs, that integrates automaticallyextracted rhythm and chords as the condition signal. During inference, the condition can either be musical features extracted from a reference audio signal, or be user-defined symbolic chord sequence, BPM, and textual prompts. Our performance evaluation on two datasets\u2014one derived from extracted features and the other from user-created inputs\u2014demonstrates that MusiConGen can generate realistic backing track music that aligns well with the specified conditions. We open-source the code and model checkpoints, and provide audio examples online, https://musicongen.github.io/musicongen_demo/.",
    "zenodo_id": 14877337,
    "dblp_key": null
  },
  {
    "title": "End-to-End Piano Performance-MIDI to Score Conversion With Transformers",
    "author": [
      "Tim Beyer",
      "Angela Dai"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877339",
    "url": "https://doi.org/10.5281/zenodo.14877339",
    "ee": "https://zenodo.org/record/14877339/files/000033.pdf",
    "pages": "319-326",
    "abstract": "The automated creation of accurate musical notation from an expressive human performance is a fundamental task in computational musicology. To this end, we present an end-to-end deep learning approach that constructs detailed musical scores directly from real-world piano performance-MIDI files. We introduce a modern transformer-based architecture with a novel tokenized representation for symbolic music data. Framing the task as sequence-to-sequence translation rather than note-wise classification reduces alignment requirements and annotation costs, while allowing the prediction of more concise and accurate notation. To serialize symbolic music data, we design a custom tokenization stage based on compound tokens that carefully quantizes continuous values. This technique preserves more score information while reducing sequence lengths by 3.5\u00d7 compared to prior approaches. Using the transformer backbone, our method demonstrates better understanding of note values, rhythmic structure, and details such as staff assignment. When evaluated end-to-end using transcription metrics such as MUSTER, we achieve significant improvements over previous deep learning approaches and complex HMM-based state-of-the-art pipelines. Our method is also the first to directly predict notational details like trill marks or stem direction from performance data. Code and models are available on GitHub.",
    "zenodo_id": 14877339,
    "dblp_key": null
  },
  {
    "title": "From Real to Cloned Singer Identification",
    "author": [
      "Dorian Desblancs",
      "Gabriel Meseguer-Brocal",
      "Romain Hennequin",
      "Manuel Moussallam"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877342",
    "url": "https://doi.org/10.5281/zenodo.14877342",
    "ee": "https://zenodo.org/record/14877342/files/000034.pdf",
    "pages": "327-334",
    "abstract": "Cloned voices of popular singers sound increasingly realistic and have gained popularity over the past few years. They however pose a threat to the industry due to personality rights concerns. As such, methods to identify the original singer in synthetic voices are needed. In this paper, we investigate how singer identification methods could be used for such a task. We present three embedding models that are trained using a singer-level contrastive learning scheme, where positive pairs consist of segments with vocals from the same singers. These segments can be mixtures for the first model, vocals for the second, and both for the third. We demonstrate that all three models are highly capable of identifying real singers. However, their performance deteriorates when classifying cloned versions of singers in our evaluation set. This is especially true for models that use mixtures as an input. These findings highlight the need to understand the biases that exist within singer identification systems, and how they can influence the identification of voice deepfakes in music.",
    "zenodo_id": 14877342,
    "dblp_key": null
  },
  {
    "title": "Emotion-Driven Piano Music Generation via Two-Stage Disentanglement and Functional Representation",
    "author": [
      "Jingyue Huang",
      "Ke Chen",
      "Yi-Hsuan Yang"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877343",
    "url": "https://doi.org/10.5281/zenodo.14877343",
    "ee": "https://zenodo.org/record/14877343/files/000035.pdf",
    "pages": "335-342",
    "abstract": "Managing the emotional aspect remains a challenge in automatic music generation. Prior works aim to learn various emotions at once, leading to inadequate modeling. This paper explores the disentanglement of emotions in piano performance generation through a two-stage framework. The first stage focuses on valence modeling of lead sheet, and the second stage addresses arousal modeling by introducing performance-level attributes. To further capture features that shape valence, an aspect less explored by previous approaches, we introduce a novel functional representation of symbolic music. This representation aims to capture the emotional impact of major-minor tonality, as well as the interactions among notes, chords, and key signatures. Objective and subjective experiments validate the effectiveness of our framework in both emotional valence and arousal modeling. We further leverage our framework in a novel application of emotional controls, showing a broad potential in emotion-driven music generation.",
    "zenodo_id": 14877343,
    "dblp_key": null
  },
  {
    "title": "Efficient Adapter Tuning for Joint Singing Voice Beat and Downbeat Tracking With Self-Supervised Learning Features",
    "author": [
      "Jiajun Deng",
      "Yaolong Ju",
      "Jing Yang",
      "Simon Lui",
      "Xunying Liu"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877345",
    "url": "https://doi.org/10.5281/zenodo.14877345",
    "ee": "https://zenodo.org/record/14877345/files/000036.pdf",
    "pages": "343-351",
    "abstract": "Singing voice beat tracking is a challenging task, due to the lack of musical accompaniment that often contains robust rhythmic and harmonic patterns, something most existing beat tracking systems utilize and can be essential for estimating beats. In this paper, a novel temporal convolutional network-based beat-tracking approach featuring self-supervised learning (SSL) representations and adapter tuning is proposed to track the beat and downbeat of singing voices jointly. The SSL DistilHuBERT representations are utilized to capture the semantic information of singing voices and are further fused with the generic spectral features to facilitate beat estimation. Sources of variabilities that are particularly prominent with the non-homogeneous singing voice data are reduced by the efficient adapter tuning. Extensive experiments show that feature fusion and adapter tuning improve the performance individually, and the combination of both leads to significantly better performances than the un-adapted baseline system, with up to 31.6% and 42.4% absolute F1-score improvements on beat and downbeat tracking, respectively.",
    "zenodo_id": 14877345,
    "dblp_key": null
  },
  {
    "title": "Which Audio Features Can Predict the Dynamic Musical Emotions of Both Composers and Listeners?",
    "author": [
      "Eun Ji Oh",
      "Hyunjae Kim",
      "Kyung Myun Lee"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877347",
    "url": "https://doi.org/10.5281/zenodo.14877347",
    "ee": "https://zenodo.org/record/14877347/files/000037.pdf",
    "pages": "352-359",
    "abstract": "Are composers\u2019 emotional intentions conveyed to listeners through audio features? In the field of Music Emotion Recognition (MER), recent efforts have been made to predict listeners\u2019 time-varying perceived emotions using machine-learning models. However, interpreting these models has been challenging due to their black-box nature. To increase the explainability of models for subjective emotional experiences, we focus on composers\u2019 emotional intentions. Our study aims to determine which audio features effectively predict both composers\u2019 time-varying emotions and listeners\u2019 perceived emotions. Seven composers performed 18 piano improvisations expressing three types of emotions (joy/happiness, sadness, and anger), which were then listened to by 36 participants in a laboratory setting. Both composers and listeners continuously assessed the emotional valence of the music clips on a 9-point scale (1: \u2018very negative\u2019 to 9: \u2018very positive\u2019). Linear mixed-effect models analysis revealed that listeners significantly perceived the composers\u2019 intended emotions. Regarding audio features, the RMS was found to modulate the degree to which the listener\u2019s perceived emotion resembled the composer\u2019s emotion across all emotions. Moreover, the significant audio features that influenced this relationship varied depending on the emotion type. We propose that audio features related to the emotional responses of composers-listeners can be considered key factors in predicting listeners\u2019 emotional responses.",
    "zenodo_id": 14877347,
    "dblp_key": null
  },
  {
    "title": "Exploring Musical Roots: Applying Audio Embeddings to Empower Influence Attribution for a Generative Music Model",
    "author": [
      "Julia Barnett",
      "Hugo Flores Garc\u00eda",
      "Bryan Pardo"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877350",
    "url": "https://doi.org/10.5281/zenodo.14877350",
    "ee": "https://zenodo.org/record/14877350/files/000038.pdf",
    "pages": "360-368",
    "abstract": "Every artist has a creative process that draws inspiration from previous artists and their works. Today, \u201cinspiration\u201d has been automated by generative music models. The black box nature of these models obscures the identity of the works that influence their creative output. As a result, users may inadvertently appropriate or copy existing artists\u2019 works. We establish a replicable methodology to systematically identify similar pieces of music audio in a manner that is useful for understanding training data attribution. We compare the effect of applying CLMR [1] and CLAP [2] embeddings to similarity measurement in a set of 5 million audio clips used to train VampNet [3], a recent open source generative music model. We validate this approach with a human listening study. We also explore the effect that modifications of an audio example (e.g., pitch shifting) have on similarity measurements. This work is foundational to incorporating automated influence attribution into generative modeling, which promises to let model creators and users move from ignorant appropriation to informed creation. Audio samples accompanying this paper are available at tinyurl.com/exploring-musical-roots.",
    "zenodo_id": 14877350,
    "dblp_key": null
  },
  {
    "title": "Green MIR? Investigating Computational Cost of Recent Music-Ai Research in ISMIR",
    "author": [
      "Andre Holzapfel",
      "Anna-Kaisa Kaila",
      "Petra J\u00e4\u00e4skel\u00e4inen"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877351",
    "url": "https://doi.org/10.5281/zenodo.14877351",
    "ee": "https://zenodo.org/record/14877351/files/000039.pdf",
    "pages": "371-380",
    "abstract": "The environmental footprint of Generative AI and other Deep Learning (DL) technologies is increasing. To understand the scale of the problem and to identify solutions for avoiding excessive energy use in DL research at communities such as ISMIR, more knowledge is needed of the current energy cost of the undertaken research. In this paper, we provide a scoping inquiry of how the ISMIR research concerning automatic music generation (AMG) and computing-heavy music analysis currently discloses information related to environmental impact. We present a study based on two corpora that document 1) ISMIR papers published in the years 2017\u20132023 that introduce an AMG model, and 2) ISMIR papers from the years 2022\u20132023 that propose music analysis models and include heavy computations with GPUs. Our study demonstrates a lack of transparency in model training documentation. It provides the first estimates of energy consumption related to model training at ISMIR, as a baseline for making more systematic estimates about the energy footprint of the ISMIR conference in relation to other machine learning events. Furthermore, we map the geographical distribution of generative model contributions and discuss the corporate role in the funding and model choices in this body of work.",
    "zenodo_id": 14877351,
    "dblp_key": null
  },
  {
    "title": "Field Study on Children's Home Piano Practice: Developing a Comprehensive System for Enhanced Student-Teacher Engagement",
    "author": [
      "Seikoh Fukuda",
      "Yuko Fukuda",
      "Masamichi Hosoda",
      "Ami Motomura",
      "Eri Sasao",
      "Masaki Matsubara",
      "Masahiro Niitsuma"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877353",
    "url": "https://doi.org/10.5281/zenodo.14877353",
    "ee": "https://zenodo.org/record/14877353/files/000040.pdf",
    "pages": "381-388",
    "abstract": "Regular weekly lessons and daily home practice are key for skill development. This paper focuses on identifying the challenges within such practice routines and developing a system to address these issues, thereby enhancing teacher support and elevating student performance in piano. Observations from real-world lessons and an analysis of practice videos spanning 177 days from 30 students reveal successful tactics, including the assignment of suitably challenging pieces and motivational rewards like stickers or stamps. Furthermore, the study underscores issues such as tension in parent-led practice and ineffective repetition. Insights from the field study suggest the potential of third-party feedback, practice segmentation, reporting practice records to teachers, and rewarding practice sessions. We developed a system incorporating these solutions and tested it with 80 children over 4 months. Results showed increased teacher engagement with students' home practice, improved student motivation and practice duration, and enhanced sight-reading skills, demonstrating the system's effectiveness in supporting piano education.",
    "zenodo_id": 14877353,
    "dblp_key": null
  },
  {
    "title": "Inner Metric Analysis as a Measure of Rhythmic Syncopation",
    "author": [
      "Brian Bemman",
      "Justin Christensen"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877355",
    "url": "https://doi.org/10.5281/zenodo.14877355",
    "ee": "https://zenodo.org/record/14877355/files/000041.pdf",
    "pages": "389-396",
    "abstract": "Inner Metric Analysis (IMA) is a method for symbolic music analysis that identifies strong and weak metrical positions according to coinciding periodicities within note onsets. These periodicities are visualized with bar graphs known as metric weight and spectral weight profiles. Analyzing these profiles for the presence of syncopation has thus far required manual inspection. In this paper, we propose a simple measure using chi-squared distance for quantifying the level of syncopation found in IMA weight profiles by considering each as a distribution to be compared against (1) a uniform distribution \u2018nominal\u2019 weight profile, and (2) a non-uniform distribution based on beat strength. We apply this measure to the task of predicting perceptual ratings of syncopation using the Song (2014) dataset of 111 single-bar rhythmic patterns and compare its performance to seven existing models of syncopation/complexity. Our results indicate that the proposed measure based on (1) achieves a moderately high Spearman rank correlation (rs = 0.80) to all ratings and is the only single measure that reportedly works across all categories. For so-called polyrhythms in 4/4, the measure based on (2) surpasses all other models and further outperforms five models for monorhythms in 6/8 and three models for monorhythms in 4/4.",
    "zenodo_id": 14877355,
    "dblp_key": null
  },
  {
    "title": "HAISP: A Dataset of Human-AI Songwriting Processes From the AI Song Contest",
    "author": [
      "Lidia J. Morris",
      "Rebecca Leger",
      "Michele Newman",
      "John Ashley Burgoyne",
      "Ryan Groves",
      "Natasha Mangal",
      "Jin Ha Lee"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877357",
    "url": "https://doi.org/10.5281/zenodo.14877357",
    "ee": "https://zenodo.org/record/14877357/files/000042.pdf",
    "pages": "397-404",
    "abstract": "The advent of accessible artificial intelligence (AI) tools and systems has begun a new era for creative expression, challenging us to gain a better understanding of human-AI collaboration and creativity. In this paper, we introduce Human\u2013AI Songwriting Processes Dataset (HAISP), consisting of 34 coded submissions from the 2023 AI Song Contest. This dataset offers a resource for exploring the complex dynamics of AI-supported songwriting processes, facilitating investigations into the possibilities and challenges posed by AI in creative endeavors. Overall, HAISP is anticipated to contribute to advancing understanding of human-AI co-creation from the users\u2019 perspective. We suggest potential use cases for the dataset, including examining AI tools used in songwriting and exploring users\u2019 ethical considerations and creative approaches. This could help inform academic research and practical applications in music composition and related fields.",
    "zenodo_id": 14877357,
    "dblp_key": null
  },
  {
    "title": "Cue Point Estimation Using Object Detection",
    "author": [
      "Giulia Arg\u00fcello",
      "Luca A. Lanzend\u00f6rfer",
      "Roger Wattenhofer"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877359",
    "url": "https://doi.org/10.5281/zenodo.14877359",
    "ee": "https://zenodo.org/record/14877359/files/000043.pdf",
    "pages": "405-412",
    "abstract": "Cue points indicate possible temporal boundaries in a transition between two pieces of music in DJ mixing and constitute a crucial element in autonomous DJ systems as well as for live mixing. In this work, we present a novel method for automatic cue point estimation, interpreted as a computer vision object detection task. Our proposed system is based on a pre-trained object detection transformer which we fine-tune on our novel cue point dataset. Our provided dataset contains 21k manually annotated cue points from human experts as well as metronome information for nearly 5k individual tracks, making this dataset 35x larger than the previously available cue point dataset. Unlike previous methods, our approach does not require low-level musical information analysis, while demonstrating increased precision in retrieving cue point positions. Moreover, our proposed method demonstrates high adherence to phrasing, a type of high-level music structure commonly emphasized in electronic dance music. The code, model checkpoints, and dataset are made publicly available.",
    "zenodo_id": 14877359,
    "dblp_key": null
  },
  {
    "title": "The ListenBrainz Listens Dataset",
    "author": [
      "Kartik Ohri",
      "Robert Kaye"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877361",
    "url": "https://doi.org/10.5281/zenodo.14877361",
    "ee": "https://zenodo.org/record/14877361/files/000044.pdf",
    "pages": "413-419",
    "abstract": "The ListenBrainz listens dataset is a continually evolving repository of music listening history events submitted by all ListenBrainz users. Currently totalling over 800 million entries, each datum within the dataset encapsulates a timestamp, a pseudonymous user identifier, track metadata, and optionally MusicBrainz identifiers facilitating seamless linkage to external resources and datasets. This paper discusses the process of raw data acquisition, the subsequent steps of data synthesis and cleaning, the comprehensive contents of the refined dataset, and the diverse potential applications of this invaluable resource. Although not the largest dataset in terms of music listening events (yet), its distinctiveness lies in its perpetual evolution, with users contributing data daily. This paper underscores the significance of the ListenBrainz listens dataset as a significant asset for researchers and practitioners alike, offering insights into music consumption patterns, user preferences, and avenues for further exploration in the fields of music information retrieval and recommendation systems.",
    "zenodo_id": 14877361,
    "dblp_key": null
  },
  {
    "title": "SpecMaskGIT: Masked Generative Modeling of Audio Spectrogram for Efficient Audio Synthesis and Beyond",
    "author": [
      "Marco Comunit\u00e0",
      "Zhi Zhong",
      "Akira Takahashi",
      "Shiqi Yang",
      "Mengjie Zhao",
      "Koichi Saito",
      "Yukara Ikemiya",
      "Takashi Shibuya",
      "Shusuke Takahashi",
      "Yuki Mitsufuji"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877363",
    "url": "https://doi.org/10.5281/zenodo.14877363",
    "ee": "https://zenodo.org/record/14877363/files/000045.pdf",
    "pages": "420-428",
    "abstract": "Recent advances in generative models that iteratively synthesize audio clips sparked great success in text-to-audio synthesis (TTA), but at the cost of slow synthesis speed and heavy computation. Although there have been attempts to accelerate the iterative procedure, high-quality TTA systems remain inefficient due to the hundreds of iterations required in the inference phase and large amount of model parameters. To address these challenges, we propose SpecMaskGIT, a light-weight, efficient yet effective TTA model based on the masked generative modeling of spectrograms. First, SpecMaskGIT synthesizes a realistic 10 s audio clip in less than 16 iterations, an order of magnitude less than previous iterative TTA methods. As a discrete model, SpecMaskGIT outperforms larger VQ-Diffusion and auto-regressive models in a TTA benchmark, while being real-time with only 4 CPU cores or even 30\u00d7 faster with a GPU. Next, built upon a latent space of Mel-spectrograms, SpecMaskGIT has a wider range of applications (e.g., zero-shot bandwidth extension) than similar methods built on latent wave domains. Moreover, we interpret SpecMaskGIT as a generative extension to previous discriminative audio masked Transformers, and shed light on its audio representation learning potential. We hope that our work will inspire the exploration of masked audio modeling toward further diverse scenarios.",
    "zenodo_id": 14877363,
    "dblp_key": null
  },
  {
    "title": "Long-Form Music Generation With Latent Diffusion",
    "author": [
      "Zach Evans",
      "Julian D. Parker",
      "CJ Carr",
      "Zachary Zukowski",
      "Josiah Taylor",
      "Jordi Pons"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877365",
    "url": "https://doi.org/10.5281/zenodo.14877365",
    "ee": "https://zenodo.org/record/14877365/files/000046.pdf",
    "pages": "429-437",
    "abstract": "Audio-based generative models for music have seen great strides recently, but so far have not managed to produce full-length music tracks with coherent musical structure from text prompts. We show that by training a generative model on long temporal contexts it is possible to produce long-form music of up to 4m 45s. Our model consists of a diffusion-transformer operating on a highly downsampled continuous latent representation (latent rate of 21.5Hz). It obtains state-of-the-art generations according to metrics on audio quality and prompt alignment, and subjective tests reveal that it produces full-length music with coherent structure.",
    "zenodo_id": 14877365,
    "dblp_key": null
  },
  {
    "title": "Composer's Assistant 2: Interactive Multi-Track MIDI Infilling With Fine-Grained User Control",
    "author": [
      "Martin E. Malandro"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877367",
    "url": "https://doi.org/10.5281/zenodo.14877367",
    "ee": "https://zenodo.org/record/14877367/files/000047.pdf",
    "pages": "438-445",
    "abstract": "We introduce Composer\u2019s Assistant 2, a system for interactive human-computer composition in the REAPER digital audio workstation. Our work upgrades the Composer\u2019s Assistant system (which performs multi-track infilling of symbolic music at the track-measure level) with a wide range of new controls to give users fine-grained control over the system\u2019s outputs. Controls introduced in this work include two types of rhythmic conditioning controls, horizontal and vertical note onset density controls, several types of pitch controls, and a rhythmic interest control. We train a T5-like transformer model to implement these controls and to serve as the backbone of our system. With these controls, we achieve a dramatic improvement in objective metrics over the original system. We also study how well our model understands the meaning of our controls, and we conduct a listening study that does not find a significant difference between real music and music composed in a co-creative fashion with our system. We release our complete system, consisting of source code, pretrained models, and REAPER scripts.",
    "zenodo_id": 14877367,
    "dblp_key": null
  },
  {
    "title": "Towards Zero-Shot Amplifier Modeling: One-to-Many Amplifier Modeling via Tone Embedding Control",
    "author": [
      "Yu-Hua Chen",
      "Yen-Tung Yeh",
      "Yuan-Chiao Cheng ",
      "Jui-Te Wu",
      "Yu-Hsiang Ho",
      "Jyh-Shing Roger Jang",
      "Yi-Hsuan Yang"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877373",
    "url": "https://doi.org/10.5281/zenodo.14877373",
    "ee": "https://zenodo.org/record/14877373/files/000048.pdf",
    "pages": "446-453",
    "abstract": "Replicating analog device circuits through neural audio effect modeling has garnered increasing interest in recent years. Existing work has predominantly focused on a one-to-one emulation strategy, modeling specific devices individually. In this paper, we tackle the less-explored scenario of one-to-many emulation, utilizing conditioning mechanisms to emulate multiple guitar amplifiers through a single neural model. For condition representation, we use contrastive learning to build a tone embedding encoder that extracts style-related features of various amplifiers, leveraging a dataset of comprehensive amplifier settings. Targeting zero-shot application scenarios, we also examine various strategies for tone embedding representation, evaluating referenced tone embedding against two retrievalbased embedding methods for amplifiers unseen in the training time. Our findings showcase the efficacy and potential of the proposed methods in achieving versatile one-to-many amplifier modeling, contributing a foundational step towards zero-shot audio modeling applications.",
    "zenodo_id": 14877373,
    "dblp_key": null
  },
  {
    "title": "Mel-RoFormer for Vocal Separation and Vocal Melody Transcription",
    "author": [
      "Ju-Chiang Wang",
      "Wei-Tsung Lu",
      "Jitong Chen"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877371",
    "url": "https://doi.org/10.5281/zenodo.14877371",
    "ee": "https://zenodo.org/record/14877371/files/000049.pdf",
    "pages": "454-461",
    "abstract": "Developing a versatile deep neural network to model music audio is crucial in MIR. This task is challenging due to the intricate spectral variations inherent in music signals, which convey melody, harmonics, and timbres of diverse instruments. In this paper, we introduce Mel-RoFormer, a spectrogram-based model featuring two key designs: a novel Mel-band Projection module at the front-end to enhance the model\u2019s capability to capture informative features across multiple frequency bands, and interleaved RoPE Transformers to explicitly model the frequency and time dimensions as two separate sequences. We apply Mel-RoFormer to tackle two essential MIR tasks: vocal separation and vocal melody transcription, aimed at isolating singing voices from audio mixtures and transcribing their lead melodies, respectively. Despite their shared focus on singing signals, these tasks possess distinct optimization objectives. Instead of training a unified model, we adopt a two-step approach. Initially, we train a vocal separation model, which subsequently serves as a foundation model for fine-tuning for vocal melody transcription. Through extensive experiments conducted on benchmark datasets, we showcase that our models achieve state-of-the-art performance in both vocal separation and melody transcription tasks, underscoring the efficacy and versatility of Mel-RoFormer in modeling complex music audio signals.",
    "zenodo_id": 14877371,
    "dblp_key": null
  },
  {
    "title": "Unsupervised Synthetic-to-Real Adaptation for Optical Music Recognition",
    "author": [
      "Noelia N. Luna-Barahona",
      "Adri\u00e1n Rosell\u00f3",
      "Mar\u00eda Alfaro-Contreras",
      "David Rizo",
      "Jorge Calvo-Zaragoza"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877375",
    "url": "https://doi.org/10.5281/zenodo.14877375",
    "ee": "https://zenodo.org/record/14877375/files/000050.pdf",
    "pages": "462-469",
    "abstract": "The field of Optical Music Recognition (OMR) focuses on models capable of reading music scores from document images. Despite its growing popularity, OMR is still confined to settings where the target scores are similar in both musical context and visual presentation to the data used for training the model. The common scenario, therefore, involves manually annotating data for each specific case, a process that is not only labor-intensive but also raises concerns regarding practicality. We present a methodology based on training a neural model with synthetic images, thus reducing the difficulty of obtaining labeled data. As sheet music renderings depict regular visual characteristics compared to scores from real collections, we propose an unsupervised neural adaptation approach consisting of loss functions that promote alignment between the features learned by the model and those of the target collection while preventing the model from converging to undesirable solutions. This unsupervised adaptation bypasses the need for extensive retraining, requiring only the unlabeled target images. Our experiments, focused on music written in Mensural notation, demonstrate that the methodology is successful and that synthetic-to-real adaptation is indeed a promising way to create practical OMR systems with little human effort.",
    "zenodo_id": 14877375,
    "dblp_key": null
  },
  {
    "title": "MMT-BERT: Chord-Aware Symbolic Music Generation Based on Multitrack Music Transformer and MusicBERT",
    "author": [
      "Jinlong Zhu",
      "Keigo Sakurai",
      "Ren Togo",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877377",
    "url": "https://doi.org/10.5281/zenodo.14877377",
    "ee": "https://zenodo.org/record/14877377/files/000051.pdf",
    "pages": "470-477",
    "abstract": "We propose a novel symbolic music representation and Generative Adversarial Network (GAN) framework specially designed for symbolic multitrack music generation. The main theme of symbolic music generation primarily encompasses the preprocessing of music data and the implementation of a deep learning framework. Current techniques dedicated to symbolic music generation generally encounter two significant challenges: training data\u2019s lack of information about chords and scales and the requirement of specially designed model architecture adapted to the unique format of symbolic music representation. In this paper, we solve the above problems by introducing new symbolic music representation with MusicLang chord analysis model. We propose our MMT-BERT architecture adapting to the representation. To build a robust multitrack music generator, we fine-tune a pre-trained MusicBERT model to serve as the discriminator, and incorporate relativistic standard loss. This approach, supported by the in-depth understanding of symbolic music encoded within MusicBERT, fortifies the consonance and humanity of music generated by our method. Experimental results demonstrate the effectiveness of our approach which strictly follows the state-of-the-art methods.",
    "zenodo_id": 14877377,
    "dblp_key": null
  },
  {
    "title": "Discogs-VI: A Musical Version Identification Dataset Based on Public Editorial Metadata",
    "author": [
      "Recep Oguz Araz",
      "Xavier Serra",
      "Dmitry Bogdanov"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877379",
    "url": "https://doi.org/10.5281/zenodo.14877379",
    "ee": "https://zenodo.org/record/14877379/files/000052.pdf",
    "pages": "478-485",
    "abstract": "Current version identification (VI) datasets often lack sufficient size and musical diversity to train robust neural networks (NNs). Additionally, their non-representative clique size distributions prevent realistic system evaluations. To address these challenges, we explore the untapped potential of the rich editorial metadata in the Discogs music database and create a large dataset of musical versions containing about 1,900,000 versions across 348,000 cliques. Utilizing a high-precision search algorithm, we map this dataset to official music uploads on YouTube, resulting in a dataset of approximately 493,000 versions across 98,000 cliques. This dataset offers over nine times the number of cliques and over four times the number of versions than existing datasets. We demonstrate the utility of our dataset by training a baseline NN without extensive model complexities or data augmentations, which achieves competitive results on the SHS100K and Da-TACOS datasets. Our dataset, along with the tools used for its creation, the extracted audio features, and a trained model, are all publicly available online.",
    "zenodo_id": 14877379,
    "dblp_key": null
  },
  {
    "title": "Who's Afraid of the `Artyfyshall Byrd'? Historical Notions and Current Challenges of Musical Artificiality",
    "author": [
      "Nicholas Cornia",
      "Bruno Forment"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877381",
    "url": "https://doi.org/10.5281/zenodo.14877381",
    "ee": "https://zenodo.org/record/14877381/files/000053.pdf",
    "pages": "486-492",
    "abstract": "The meteoric surge of AI-generated music has prompted significant concerns among artists and publishers alike. Some fear that the adoption of AI is poised to result in massive job destruction; others sense it will jeopardize and eventually upend all legal frameworks of intellectual property. AI, however, is not the first instance where humanity has confronted the prospect of machines emulating musical creativity. Already in the Baroque, various modes of musical artificiality were explored, ranging from automata and organ stops mimicking human performance and natural sounds, up to devices for mechanized composition (e.g., Athanasius Kircher, Johann Philip Kirnberger, C.P.E. Bach, Antonio Calegari and Diederich Nickolaus Winkel). Valuable insights emerge from the reconsideration\u2014and digital implementation\u2014of these curiosities through the lens of present-day generative models. It can be argued that the very notion of \u2018artificiality\u2019 has presented humanity with long-standing philosophical dilemmas, in addressing the debate on the role of art as a substitute of (divine) nature. By digitally implementing and formalizing some pioneering instances of algorithmically-generated music we wish to illustrate how mechanical devices have played a role in human art and entertainment prior to our digital era.",
    "zenodo_id": 14877381,
    "dblp_key": null
  },
  {
    "title": "End-to-End Automatic Singing Skill Evaluation Using Cross-Attention and Data Augmentation for Solo Singing and Singing With Accompaniment",
    "author": [
      "Yaolong Ju",
      "Chun Yat Wu",
      "Betty Corti\u00f1as Lorenzo",
      "Jing Yang",
      "Jiajun Deng",
      "Fan Fan",
      "Simon Lui"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877383",
    "url": "https://doi.org/10.5281/zenodo.14877383",
    "ee": "https://zenodo.org/record/14877383/files/000054.pdf",
    "pages": "493-500",
    "abstract": "Automatic singing skill evaluation (ASSE) systems are predominantly designed for solo singing, and the scenario of singing with accompaniment is largely unaddressed. In this paper, we propose an end-to-end ASSE system that effectively processes both solo singing and singing with accompaniment using data augmentation, where a comparative study is conducted on four different data augmentation approaches. Additionally, we incorporate bi-directional cross-attention (BiCA) for feature fusion which, compared to simple concatenation, can better exploit the inter-relationships between different features. Results on the 10KSinging dataset show that data augmentation and BiCA boost performance individually. When combined, they contribute to further improvements, with a Pearson correlation coefficient of 0.769 for solo singing and 0.709 for singing with accompaniment. This represents relative improvements of 36.8% and 26.2% compared to the baseline model score of 0.562, respectively.",
    "zenodo_id": 14877383,
    "dblp_key": null
  },
  {
    "title": "Cluster and Separate: A GNN Approach to Voice and Staff Prediction for Score Engraving",
    "author": [
      "Francesco Foscarin",
      "Emmanouil Karystinaios",
      "Eita Nakamura",
      "Gerhard Widmer"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877385",
    "url": "https://doi.org/10.5281/zenodo.14877385",
    "ee": "https://zenodo.org/record/14877385/files/000055.pdf",
    "pages": "503-510",
    "abstract": "This paper approaches the problem of separating the notes from a quantized symbolic music piece (e.g., a MIDI file) into multiple voices and staves. This is a fundamental part of the larger task of music score engraving (or score type-setting), which aims to produce readable musical scores for human performers. We focus on piano music and support homophonic voices, i.e., voices that can contain chords, and cross-staff voices, which are notably difficult tasks that have often been overlooked in previous research. We propose an end-to-end system based on graph neural networks that clusters notes that belong to the same chord and connects them with edges if they are part of a voice. Our results show clear and consistent improvements over a previous approach on two datasets of different styles. To aid the qualitative analysis of our results, we support the export in symbolic music formats and provide a direct visualization of our outputs graph over the musical score. All code and pre-trained models are available at https://github.com/CPJKU/piano_svsep.",
    "zenodo_id": 14877385,
    "dblp_key": null
  },
  {
    "title": "From Audio Encoders to Piano Judges: Benchmarking Performance Understanding for Solo Piano",
    "author": [
      "Huan Zhang",
      "Jinhua Liang",
      "Simon Dixon"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877387",
    "url": "https://doi.org/10.5281/zenodo.14877387",
    "ee": "https://zenodo.org/record/14877387/files/000056.pdf",
    "pages": "511-519",
    "abstract": "Our study investigates an approach for understanding musical performances through the lens of audio encoding models, focusing on the domain of solo Western classical piano music. Compared to composition-level attribute understanding such as key or genre, we identify a knowledge gap in performance-level music understanding, and address three critical tasks: expertise ranking, difficulty estimation, and piano technique detection, introducing a comprehensive Pianism-Labelling Dataset (PLD) for this purpose. We leverage pre-trained audio encoders, specifically Jukebox, Audio-MAE, MERT, and DAC, demonstrating varied capabilities in tackling downstream tasks, to explore whether domain-specific fine-tuning enhances capability in capturing performance nuances. Our best approach achieved 93.6% accuracy in expertise ranking, 33.7% in difficulty estimation, and 46.7% in technique detection, with Audio-MAE as the overall most effective encoder. Finally, we conducted a case study on Chopin Piano Competition data using trained models for expertise ranking, which highlights the challenge of accurately assessing top-tier performances.",
    "zenodo_id": 14877387,
    "dblp_key": null
  },
  {
    "title": "Towards Explainable and Interpretable Musical Difficulty Estimation: A Parameter-Efficient Approach",
    "author": [
      "Pedro Ramoneda",
      "Vsevolod E. Eremenko",
      "Alexandre D'Hooge",
      "Emilia Parada-Cabaleiro",
      "Xavier Serra"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877389",
    "url": "https://doi.org/10.5281/zenodo.14877389",
    "ee": "https://zenodo.org/record/14877389/files/000057.pdf",
    "pages": "520-528",
    "abstract": "Estimating music piece difficulty is important for organizing educational music collections. This process could be partially automatized to facilitate the educator\u2019s role. Nevertheless, the decisions performed by prevalent deep-learning models are hardly understandable, which may impair the acceptance of such a technology in music education curricula. Our work employs explainable descriptors for difficulty estimation in symbolic music representations. Furthermore, through a novel parameter-efficient white-box model, we outperform previous efforts while delivering interpretable results. These comprehensible outcomes emulate the functionality of a rubric, a tool widely used in music education. Our approach, evaluated in piano repertoire categorized in 9 classes, achieved 41.4% accuracy independently, with a mean squared error (MSE) of 1.7, showing precise difficulty estimation. Through our baseline, we illustrate how building on top of past research can offer alternatives for music difficulty assessment which are explainable and interpretable. With this, we aim to promote a more effective communication between the Music Information Retrieval (MIR) community and the music education one.",
    "zenodo_id": 14877389,
    "dblp_key": null
  },
  {
    "title": "Purposeful Play: Evaluation and Co-Design of Casual Music Creation Applications With Children",
    "author": [
      "Michele Newman",
      "Lidia J. Morris",
      "Jun Kato",
      "Masataka Goto",
      "Jason Yip",
      "Jin Ha Lee"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877391",
    "url": "https://doi.org/10.5281/zenodo.14877391",
    "ee": "https://zenodo.org/record/14877391/files/000058.pdf",
    "pages": "529-539",
    "abstract": "The rise of digital technologies has increased interest in democratizing music creation, but current creativity support tools often prioritize literacy and education over meeting children\u2019s needs for casual creation. To address this, we conducted Participatory Design sessions with children aged 6-13 to explore their perceptions of casual music creation activities and identify elements of creative applications that support different expressions. Our study aimed to answer two key questions: (1) How do children perceive casual music creation activities and which elements of creative applications facilitate expression? and (2) What insights can inform the design of future casual music creation tools? Our findings indicate that children view casual music creation as involving diverse activities, with visuals aiding in understanding sounds, and engaging in various playful interactions leading to creative experiences. We present design implications based on our findings and introduce casual creation as \"purposeful play\". Furthermore, we discuss its implications for creative MIR.",
    "zenodo_id": 14877391,
    "dblp_key": null
  },
  {
    "title": "El Bongosero: A Crowd-Sourced Symbolic Dataset of Improvised Hand Percussion Rhythms Paired With Drum Patterns",
    "author": [
      "Nicholas Evans",
      "Behzad Haki",
      "Daniel G\u00f3mez-Mar\u00edn",
      "Sergi Jord\u00e0"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877393",
    "url": "https://doi.org/10.5281/zenodo.14877393",
    "ee": "https://zenodo.org/record/14877393/files/000059.pdf",
    "pages": "540-546",
    "abstract": "We present El Bongosero, a large-scale, open-source symbolic dataset comprising expressive, improvised drum performances crowd-sourced from a pool of individuals with varying levels of musical expertise. Originating from an interactive installation hosted at Centre de Cultura Contempor\u00e0nia de Barcelona, our dataset consists of 6,035 unique tapped sequences performed by 3,184 participants. To our knowledge, this is the only symbolic dataset of its size and type that includes expressive timing and dynamics information as well as each participant\u2019s level of expertise. These unique characteristics could prove to be valuable to future research, particularly in the areas of music generation and music education. Preliminary analysis, including a step-wise Jaccard similarity analysis on a subset of the data, demonstrate that this dataset is a diverse, non-random, and musically meaningful collection. To facilitate prompt exploration and understanding of the data, we have also prepared a dedicated website and an open-source API in order to interact with the data.",
    "zenodo_id": 14877393,
    "dblp_key": null
  },
  {
    "title": "Utilizing Listener-Provided Tags for Music Emotion Recognition: A Data-Driven Approach",
    "author": [
      "Joanne Affolter",
      "Martin A. Rohrmeier"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877395",
    "url": "https://doi.org/10.5281/zenodo.14877395",
    "ee": "https://zenodo.org/record/14877395/files/000060.pdf",
    "pages": "547-554",
    "abstract": "This work introduces a data-driven approach for assigning emotions to music tracks. Consisting of two distinct phases, our framework enables the creation of synthetic emotion-labeled datasets that can serve both Music Emotion Recognition and Auto-Tagging tasks. The first phase presents a versatile method for collecting listener-generated verbal data, such as tags and playlist names, from multiple online sources on a large scale. We compiled a dataset of 5, 892 tracks, each associated with textual data from four distinct sources. The second phase leverages Natural Language Processing for representing music-evoked emotions, relying solely on the data acquired during the first phase. By semantically matching user-generated text to a well-known corpus of emotion-labelled English words, we are ultimately able to represent each music track as an 8-dimensional vector that captures the emotions perceived by listeners. Our method departs from conventional labeling techniques: instead of defining emotions as generic \u201cmood tags\u201d found on social platforms, we leverage a refined psychological model drawn from Plutchik\u2019s theory [1], which appears more intuitive than the extensively used Valence-Arousal model.",
    "zenodo_id": 14877395,
    "dblp_key": null
  },
  {
    "title": "PiCoGen2: Piano Cover Generation With Transfer Learning Approach and Weakly Aligned Data",
    "author": [
      "Chih-Pin Tan",
      "Hsin Ai",
      "Yi-Hsin Chang",
      "Shuen-Huei Guan",
      "Yi-Hsuan Yang"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877397",
    "url": "https://doi.org/10.5281/zenodo.14877397",
    "ee": "https://zenodo.org/record/14877397/files/000061.pdf",
    "pages": "555-562",
    "abstract": "Piano cover generation aims to create a piano cover from a pop song. Existing approaches mainly employ supervised learning and the training demands strongly-aligned and paired song-to-piano data, which is built by remapping piano notes to song audio. This would, however, result in the loss of piano information and accordingly cause inconsistencies between the original and remapped piano versions. To overcome this limitation, we propose a transfer learning approach that pre-trains our model on piano-only data and fine-tunes it on weakly-aligned paired data constructed without note remapping. During pre-training, to guide the model to learn piano composition concepts instead of merely transcribing audio, we use an existing lead sheet transcription model as the encoder to extract high-level features from the piano recordings. The pre-trained model is then fine-tuned on the paired song-piano data to transfer the learned composition knowledge to the pop song domain. Our evaluation shows that this training strategy enables our model, named PiCoGen2, to attain high-quality results, outperforming baselines on both objective and subjective metrics across five pop genres.",
    "zenodo_id": 14877397,
    "dblp_key": null
  },
  {
    "title": "Diff-MST: Differentiable Mixing Style Transfer",
    "author": [
      "Soumya Sai Vanka",
      "Christian J. Steinmetz",
      "Jean-Baptiste Rolland",
      "Joshua D. Reiss",
      "George Fazekas"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877399",
    "url": "https://doi.org/10.5281/zenodo.14877399",
    "ee": "https://zenodo.org/record/14877399/files/000062.pdf",
    "pages": "563-570",
    "abstract": "Mixing style transfer automates the generation of a multi-track mix for a given set of tracks by inferring production attributes from a reference song. However, existing systems for mixing style transfer are limited in that they often operate only on a fixed number of tracks, introduce artifacts, and produce mixes in an end-to-end fashion, without grounding in traditional audio effects, prohibiting interpretability and controllability. To overcome these challenges, we introduce Diff-MST, a framework comprising a differentiable mixing console, a transformer controller, and an audio production style loss function. By inputting raw tracks and a reference song, our model estimates control parameters for audio effects within a differentiable mixing console, producing high-quality mixes and enabling post-hoc adjustments. Moreover, our architecture supports an arbitrary number of input tracks without source labelling, enabling real-world applications. We evaluate our model\u2019s performance against robust baselines and showcase the effectiveness of our approach, architectural design, tailored audio production style loss, and innovative training methodology for the given task. We provide code and listening examples online.",
    "zenodo_id": 14877399,
    "dblp_key": null
  },
  {
    "title": "Semi-Supervised Contrastive Learning of Musical Representations",
    "author": [
      "Julien PM Guinot",
      "Elio Quinton",
      "George Fazekas"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877401",
    "url": "https://doi.org/10.5281/zenodo.14877401",
    "ee": "https://zenodo.org/record/14877401/files/000063.pdf",
    "pages": "571-579",
    "abstract": "Despite the success of contrastive learning in Music Information Retrieval, the inherent ambiguity of contrastive self-supervision presents a challenge. Relying solely on augmentation chains and self-supervised positive sampling strategies can lead to a pretraining objective that does not capture key musical information for downstream tasks. We introduce semi-supervised contrastive learning (SemiSupCon), a simple method for leveraging musically informed labeled data (supervision signals) in the contrastive learning of musical representations. Our approach introduces musically relevant supervision signals into self-supervised contrastive learning by combining supervised and self-supervised contrastive objectives in a simpler framework than previous approaches. This framework improves downstream performance and robustness to audio corruptions on a range of downstream MIR tasks with moderate amounts of labeled data. Our approach enables shaping the learned similarity metric through the choice of labeled data that (1) infuses the representations with musical domain knowledge and (2) improves out-of-domain performance with minimal general downstream performance loss. We show strong transfer learning performance on musically related yet not trivially similar tasks - such as pitch and key estimation. Additionally, our approach shows performance improvement on automatic tagging over self-supervised approaches with only 5% of available labels included in pretraining.",
    "zenodo_id": 14877401,
    "dblp_key": null
  },
  {
    "title": "Improved Symbolic Drum Style Classification With Grammar-Based Hierarchical Representations",
    "author": [
      "L\u00e9o G\u00e9r\u00e9",
      "Nicolas Audebert",
      "Philippe Rigaux"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877403",
    "url": "https://doi.org/10.5281/zenodo.14877403",
    "ee": "https://zenodo.org/record/14877403/files/000064.pdf",
    "pages": "580-587",
    "abstract": "Deep learning models have become a critical tool for analysis and classification of musical data. These models operate either on the audio signal, e.g. waveform or spectrogram, or on a symbolic representation, such as MIDI. In the latter, musical information is often reduced to basic features, i.e. durations, pitches and velocities. Most existing works then rely on generic tokenization strategies from classical natural language processing, or matrix representations, e.g. piano roll. In this work, we evaluate how enriched representations of symbolic data can impact deep models, i.e. Transformers and RNN, for music style classification. In particular, we examine representations that explicitly incorporate musical information implicitly present in MIDI-like encodings, such as rhythmic organization, and show that they outperform generic tokenization strategies. We introduce a new tree-based representation of MIDI data built upon a context-free musical grammar. We show that this grammar representation accurately encodes high-level rhythmic information and outperforms existing encodings on the GrooveMIDI Dataset for drumming style classification, while being more compact and parameter-efficient.",
    "zenodo_id": 14877403,
    "dblp_key": null
  },
  {
    "title": "Nested Music Transformer: Sequentially Decoding Compound Tokens in Symbolic Music and Audio Generation",
    "author": [
      "Jiwoo Ryu",
      "Hao-Wen Dong",
      "Jongmin Jung",
      "Dasaem Jeong"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877405",
    "url": "https://doi.org/10.5281/zenodo.14877405",
    "ee": "https://zenodo.org/record/14877405/files/000065.pdf",
    "pages": "588-595",
    "abstract": "Representing symbolic music with compound tokens, where each token consists of several different sub-tokens representing a distinct musical feature or attribute, offers the advantage of reducing sequence length. While previous research has validated the efficacy of compound tokens in music sequence modeling, predicting all sub-tokens simultaneously can lead to suboptimal results as it may not fully capture the interdependencies between them. We introduce the Nested Music Transformer (NMT), an architecture tailored for decoding compound tokens autoregressively, similar to processing flattened tokens, but with low memory usage. The NMT consists of two transformers: the main decoder that models a sequence of compound tokens and the sub-decoder for modeling sub-tokens of each compound token. The experiment results showed that applying the NMT to compound tokens can enhance the performance in terms of better perplexity in processing various symbolic music datasets and discrete audio tokens from the MAESTRO dataset.",
    "zenodo_id": 14877405,
    "dblp_key": null
  },
  {
    "title": "Continual Learning for Music Classification",
    "author": [
      "Pedro Gonz\u00e1lez-Barrachina",
      "Mar\u00eda Alfaro-Contreras",
      "Jorge Calvo-Zaragoza"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877407",
    "url": "https://doi.org/10.5281/zenodo.14877407",
    "ee": "https://zenodo.org/record/14877407/files/000066.pdf",
    "pages": "596-602",
    "abstract": "Music classification is a prominent research area within Music Information Retrieval. While Deep Learning methods can adequately perform this task, their classification space remains fixed once trained, which conflicts with the dynamic nature of the ever-evolving music landscape. This work explores, for the first time, the application of Continual Learning (CL) in the context of music classification. Specifically, we thoroughly evaluate five state-of-the-art CL approaches across four different music classification tasks. Additionally, we showcase that a foundation model might be the key to CL in music classification. To that end, we study a new approach called Pre-trained Class Centers, which leverages pre-trained features to create fixed class-center spaces. Our results reveal that existing CL methods struggle when applied to music classification tasks, whereas this simple method consistently out-performs them. This highlights the need for CL methods tailored specifically for music classification.",
    "zenodo_id": 14877407,
    "dblp_key": null
  },
  {
    "title": "TheGlueNote: Learned Representations for Robust and Flexible Note Alignment",
    "author": [
      "Silvan Peter",
      "Gerhard Widmer"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877409",
    "url": "https://doi.org/10.5281/zenodo.14877409",
    "ee": "https://zenodo.org/record/14877409/files/000067.pdf",
    "pages": "603-610",
    "abstract": "Note alignment refers to the task of matching individual notes of two versions of the same symbolically encoded piece. Methods addressing this task commonly rely on sequence alignment algorithms such as Hidden Markov Models or Dynamic Time Warping (DTW) applied directly to note or onset sequences. While successful in many cases, such methods struggle with large mismatches between the versions. In this work, we learn note-wise representations from data augmented with various complex mismatch cases, e.g. repeats, skips, block insertions, and long trills. At the heart of our approach lies a transformer encoder network \u2014 TheGlueNote 1 \u2014 which predicts pairwise note similarities for two 512 note subsequences. We postprocess the predicted similarities using flavors of weightedDTW and pitch-separated onsetDTW to retrieve note matches for two sequences of arbitrary length. Our approach performs on par with the state of the art in terms of note alignment accuracy, is considerably more robust to version mismatches, and works directly on any pair of MIDI files.",
    "zenodo_id": 14877409,
    "dblp_key": null
  },
  {
    "title": "GAPS: A Large and Diverse Classical Guitar Dataset and Benchmark Transcription Model",
    "author": [
      "Xavier Riley",
      "Zixun Guo",
      "Andrew C. Edwards",
      "Simon Dixon"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877413",
    "url": "https://doi.org/10.5281/zenodo.14877413",
    "ee": "https://zenodo.org/record/14877413/files/000068.pdf",
    "pages": "611-617",
    "abstract": "We introduce GAPS (Guitar-Aligned Performance Scores), a new dataset of classical guitar performances, and a benchmark guitar transcription model that achieves state-of-the-art performance on GuitarSet in both supervised and zero-shot settings. GAPS is the largest dataset of real guitar audio, containing 14 hours of freely available audio-score aligned pairs, recorded in diverse conditions by over 200 performers, together with high-resolution note-level MIDI alignments and performance videos. These enable us to train a state-of-the-art model for automatic transcription of solo guitar recordings which can generalise well to real world audio that is unseen during training. For each track in the dataset, we provide metadata of the composer and performer, giving dates, nationality, gender and links to IMSLP or Wikipedia. We also analyse guitar-specific features of the dataset, such as the distribution of fret-string combinations and alternate tunings. This dataset has applications to various MIR tasks, including automatic music transcription, score following, performance analysis, generative music modelling and the study of expressive performance timing.",
    "zenodo_id": 14877413,
    "dblp_key": null
  },
  {
    "title": "A Kalman Filter Model for Synchronization in Musical Ensembles",
    "author": [
      "Hugo T. Carvalho",
      "Min Susan Li",
      "Massimiliano Di Luca",
      "Alan M. Wing"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877411",
    "url": "https://doi.org/10.5281/zenodo.14877411",
    "ee": "https://zenodo.org/record/14877411/files/000069.pdf",
    "pages": "618-624",
    "abstract": "The synchronization of motor responses to rhythmic auditory cues is a fundamental biological phenomenon observed across various species. While the importance of temporal alignment varies across different contexts, achieving precise temporal synchronization is a prominent goal in musical performances. Musicians often incorporate expressive timing variations, which require precise control over timing and synchronization, particularly in ensemble performance. This is crucial because both deliberate expressive nuances and accidental timing deviations can affect the overall timing of a performance. This discussion prompts the question of how musicians adjust their temporal dynamics to achieve synchronization within an ensemble. This paper introduces a novel feedback correction model based on the Kalman Filter, aimed at improving the understanding of interpersonal timing in ensemble music performances. The proposed model performs similarly to other linear correction models in the literature, with the advantage of low computational cost and good performance even in scenarios where the underlying tempo varies.",
    "zenodo_id": 14877411,
    "dblp_key": null
  },
  {
    "title": "Stem-JEPA: A Joint-Embedding Predictive Architecture for Musical Stem Compatibility Estimation",
    "author": [
      "Alain Riou",
      "Stefan Lattner",
      "Ga\u00ebtan Hadjeres",
      "Michael Anslow",
      "Geoffroy Peeters"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877415",
    "url": "https://doi.org/10.5281/zenodo.14877415",
    "ee": "https://zenodo.org/record/14877415/files/000070.pdf",
    "pages": "625-633",
    "abstract": "This paper explores the automated process of determining stem compatibility by identifying audio recordings of single instruments that blend well with a given musical context. To tackle this challenge, we present Stem-JEPA, a novel Joint-Embedding Predictive Architecture (JEPA) trained on a multi-track dataset using a self-supervised learning approach. Our model comprises two networks: an encoder and a predictor, which are jointly trained to predict the embeddings of compatible stems from the embeddings of a given context, typically a mix of several instruments. Training a model in this manner allows its use in estimating stem compatibility\u2014retrieving, aligning, or generating a stem to match a given mix\u2014or for downstream tasks such as genre or key estimation, as the training paradigm requires the model to learn information related to timbre, harmony, and rhythm. We evaluate our model\u2019s performance on a retrieval task on the MUSDB18 dataset, testing its ability to find the missing stem from a mix and through a subjective user study. We also show that the learned embeddings capture temporal alignment information and, finally, evaluate the representations learned by our model on several downstream tasks, highlighting that they effectively capture meaningful musical features.",
    "zenodo_id": 14877415,
    "dblp_key": null
  },
  {
    "title": "Audio Prompt Adapter: Unleashing Music Editing Abilities for Text-to-Music With Lightweight Finetuning",
    "author": [
      "Fang Duo Tsai",
      "Shih-Lun Wu",
      "Haven Kim",
      "Bo-Yu Chen",
      "Hao-Chung Cheng",
      "Yi-Hsuan Yang"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877417",
    "url": "https://doi.org/10.5281/zenodo.14877417",
    "ee": "https://zenodo.org/record/14877417/files/000071.pdf",
    "pages": "634-641",
    "abstract": "Text-to-music models allow users to generate nearly realistic musical audio with textual commands. However, editing music audios remains challenging due to the conflicting desiderata of performing fine-grained alterations on the audio while maintaining a simple user interface. To address this challenge, we propose Audio Prompt Adapter (or AP-Adapter), a lightweight addition to pretrained text-to-music models. We utilize AudioMAE to extract features from the input audio, and construct attention-based adapters to feed these features into the internal layers of AudioLDM2, a diffusion-based text-to-music model. With 22M trainable parameters, AP-Adapter empowers users to harness both global (e.g., genre and timbre) and local (e.g., melody) aspects of music, using the original audio and a short text as inputs. Through objective and subjective studies, we evaluate AP-Adapter on three tasks: timbre transfer, genre transfer, and accompaniment generation. Additionally, we demonstrate its effectiveness on out-of-domain audios containing unseen instruments during training.",
    "zenodo_id": 14877417,
    "dblp_key": null
  },
  {
    "title": "MelodyT5: A Unified Score-to-Score Transformer for Symbolic Music Processing",
    "author": [
      "Shangda Wu",
      "Yashan Wang",
      "Xiaobing Li",
      "Feng Yu",
      "Maosong Sun"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877419",
    "url": "https://doi.org/10.5281/zenodo.14877419",
    "ee": "https://zenodo.org/record/14877419/files/000072.pdf",
    "pages": "642-650",
    "abstract": "In the domain of symbolic music research, the progress of developing scalable systems has been notably hindered by the scarcity of available training data and the demand for models tailored to specific tasks. To address these issues, we propose MelodyT5, a novel unified framework that leverages an encoder-decoder architecture tailored for symbolic music processing in ABC notation. This framework challenges the conventional task-specific approach, considering various symbolic music tasks as score-to-score transformations. Consequently, it integrates seven melody-centric tasks, from generation to harmonization and segmentation, within a single model. Pre-trained on Melody-Hub, a newly curated collection featuring over 261K unique melodies encoded in ABC notation and encompassing more than one million task instances, MelodyT5 demonstrates superior performance in symbolic music processing via multi-task transfer learning. Our findings highlight the efficacy of multi-task transfer learning in symbolic music processing, particularly for data-scarce tasks, challenging the prevailing task-specific paradigms and offering a comprehensive dataset and framework for future explorations in this domain.",
    "zenodo_id": 14877419,
    "dblp_key": null
  },
  {
    "title": "GraphMuse: A Library for Symbolic Music Graph Processing",
    "author": [
      "Emmanouil Karystinaios",
      "Gerhard Widmer"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877421",
    "url": "https://doi.org/10.5281/zenodo.14877421",
    "ee": "https://zenodo.org/record/14877421/files/000073.pdf",
    "pages": "651-658",
    "abstract": "Graph Neural Networks (GNNs) have recently gained traction in symbolic music tasks, yet a lack of a unified framework impedes progress. Addressing this gap, we present GraphMuse, a graph processing framework and library that facilitates efficient music graph processing and GNN training for symbolic music tasks. Central to our contribution is a new neighbor sampling technique specifically targeted toward meaningful behavior in musical scores. Additionally, GraphMuse integrates hierarchical modeling elements that augment the expressivity and capabilities of graph networks for musical tasks. Experiments with two specific musical prediction tasks \u2013 pitch spelling and cadence detection \u2013 demonstrate significant performance improvement over previous methods. Our hope is that GraphMuse will lead to a boost in, and standardization of, symbolic music processing based on graph representations. The library is available at https: //github.com/manoskary/graphmuse",
    "zenodo_id": 14877421,
    "dblp_key": null
  },
  {
    "title": "ST-ITO: Controlling Audio Effects for Style Transfer With Inference-Time Optimization",
    "author": [
      "Christian J. Steinmetz",
      "Shubhr Singh",
      "Marco Comunit\u00e0",
      "Ilias Ibnyahya",
      "Shanxin Yuan",
      "Emmanouil Benetos",
      "Joshua D. Reiss"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877423",
    "url": "https://doi.org/10.5281/zenodo.14877423",
    "ee": "https://zenodo.org/record/14877423/files/000074.pdf",
    "pages": "661-668",
    "abstract": "Audio production style transfer is the task of processing an input to impart stylistic elements from a reference recording. Existing approaches often train a neural network to estimate control parameters for a set of audio effects. However, these approaches are limited in that they can only control a fixed set of effects, where the effects must be differentiable or otherwise employ specialized training techniques. In this work, we introduce ST-ITO, Style Transfer with Inference-Time Optimization, an approach that instead searches the parameter space of an audio effect chain at inference. This method enables control of arbitrary audio effect chains, including unseen and non-differentiable effects. Our approach employs a learned metric of audio production style, which we train through a simple and scalable self-supervised pretraining strategy, along with a gradient-free optimizer. Due to the limited existing evaluation methods for audio production style transfer, we introduce a multi-part benchmark to evaluate audio production style metrics and style transfer systems. This evaluation demonstrates that our audio representation better captures attributes related to audio production and enables expressive style transfer via control of arbitrary audio effects.",
    "zenodo_id": 14877423,
    "dblp_key": null
  },
  {
    "title": "ComposerX: Multi-Agent Symbolic Music Composition With LLMs",
    "author": [
      "Qixin Deng",
      "Qikai Yang",
      "Ruibin Yuan",
      "Yipeng Huang ",
      "Yi Wang",
      "Xubo Liu",
      "Zeyue Tian",
      "Jiahao Pan",
      "Ge Zhang",
      "Hanfeng Lin",
      "Yizhi Li",
      "Yinghao Ma",
      "Jie Fu",
      "Chenghua Lin",
      "Emmanouil Benetos",
      "Wenwu Wang",
      "Guangyu Xia",
      "Wei Xue",
      "Yike Guo"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877425",
    "url": "https://doi.org/10.5281/zenodo.14877425",
    "ee": "https://zenodo.org/record/14877425/files/000075.pdf",
    "pages": "669-679",
    "abstract": "Music composition represents the creative side of humanity, and itself is a complex task that requires abilities to understand and generate information with long dependency and harmony constraints. Current LLMs often struggle with this task, sometimes generating poorly written music even when equipped with modern techniques like In-Context-Learning and Chain-of-Thoughts. To further explore and enhance LLMs\u2019 potential in music composition by leveraging their reasoning ability and the large knowledge base in music history and theory, we propose ComposerX, an agent-based symbolic music generation framework. We find that applying a multi-agent approach significantly improves the music composition quality of GPT-4. The results demonstrate that ComposerX is capable of producing coherent polyphonic music compositions with captivating melodies, while adhering to user instructions.",
    "zenodo_id": 14877425,
    "dblp_key": null
  },
  {
    "title": "Do Music Generation Models Encode Music Theory?",
    "author": [
      "Megan Wei",
      "Michael Freeman",
      "Chris Donahue",
      "Chen Sun"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877427",
    "url": "https://doi.org/10.5281/zenodo.14877427",
    "ee": "https://zenodo.org/record/14877427/files/000076.pdf",
    "pages": "680-687",
    "abstract": "Music foundation models possess impressive music generation capabilities. When people compose music, they may infuse their understanding of music into their work, by using notes and intervals to craft melodies, chords to build progressions, and tempo to create a rhythmic feel. To what extent is this true of music generation models? More specifically, are fundamental Western music theory concepts observable within the \u201cinner workings\u201d of these models? Recent work proposed leveraging latent audio representations from music generation models towards music information retrieval tasks (e.g. genre classification, emotion recognition), which suggests that high-level musical characteristics are encoded within these models. However, probing individual music theory concepts (e.g. tempo, pitch class, chord quality) remains under-explored. Thus, we introduce SynTheory, a synthetic MIDI and audio music theory dataset, consisting of tempos, time signatures, notes, intervals, scales, chords, and chord progressions concepts. We then propose a framework to probe for these music theory concepts in music foundation models (Jukebox and MusicGen) and assess how strongly they encode these concepts within their internal representations. Our findings suggest that music theory concepts are discernible within foundation models and that the degree to which they are detectable varies by model size and layer.",
    "zenodo_id": 14877427,
    "dblp_key": null
  },
  {
    "title": "PolySinger: Singing-Voice to Singing-Voice Translation From English to Japanese",
    "author": [
      "Silas Antonisen",
      "Iv\u00e1n L\u00f3pez-Espejo"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877429",
    "url": "https://doi.org/10.5281/zenodo.14877429",
    "ee": "https://zenodo.org/record/14877429/files/000077.pdf",
    "pages": "688-696",
    "abstract": "The speech domain prevails in the spotlight for several natural language processing (NLP) tasks while the singing domain remains less explored. The culmination of NLP is the speech-to-speech translation (S2ST) task, referring to translation and synthesis of human speech. A disparity between S2ST and the possible adaptation to the singing domain, which we describe as singing-voice to singing-voice translation (SV2SVT), is becoming prominent as the former is progressing ever faster, while the latter is at a standstill. Singing-voice synthesis systems are overcoming the barrier of multi-lingual synthesis, despite limited attention has been paid to multi-lingual songwriting and song translation. This paper endeavors to determine what is required for successful SV2SVT and proposes PolySinger (Polyglot Singer): the first system for SV2SVT, performing lyrics translation from English to Japanese. A cascaded approach is proposed to establish a framework with a high degree of control which can potentially diminish the disparity between SV2SVT and S2ST. The performance of PolySinger is evaluated by a mean opinion score test with native Japanese speakers. Results and in-depth discussions with test subjects suggest a solid foundation for SV2SVT, but several shortcomings must be overcome, which are discussed for the future of SV2SVT.",
    "zenodo_id": 14877429,
    "dblp_key": null
  },
  {
    "title": "On the Validity of Employing ChatGPT for Distant Reading of Music Similarity",
    "author": [
      "Arthur Flexer"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877431",
    "url": "https://doi.org/10.5281/zenodo.14877431",
    "ee": "https://zenodo.org/record/14877431/files/000078.pdf",
    "pages": "697-704",
    "abstract": "In this work we explore whether large language models (LLM) can be a useful and valid tool for music knowledge discovery. LLMs offer an interface to enormous quantities of text and hence can be seen as a new tool for \u2019distant reading\u2019, i.e. the computational analysis of text including sources about music. More specifically we investigated whether ratings of music similarity, as measured via human listening tests, can be recovered from textual data by using ChatGPT. We examined the inferences that can be drawn from these experiments through the formal lens of validity. We showed that correlation of ChatGPT with human raters is of moderate positive size but also lower than the average human inter-rater agreement. By evaluating a number of threats to validity and conducting additional experiments with ChatGPT, we were able to show that especially construct validity of such an approach is seriously compromised. The opaque black box nature of ChatGPT makes it close to impossible to judge the experiment\u2019s construct validity, i.e. the relationship between what is meant to be inferred from the experiment, which are estimates of music similarity, and what is actually being measured. As a consequence the use of LLMs for music knowledge discovery cannot be recommended.",
    "zenodo_id": 14877431,
    "dblp_key": null
  },
  {
    "title": "Sanidha: A Studio Quality Multi-Modal Dataset for Carnatic Music",
    "author": [
      "Venkatakrishnan Vaidyanathapuram Krishnan",
      "Noel Alben",
      "Anish A. Nair",
      "Nathaniel Condit-Schultz"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877433",
    "url": "https://doi.org/10.5281/zenodo.14877433",
    "ee": "https://zenodo.org/record/14877433/files/000079.pdf",
    "pages": "705-712",
    "abstract": "Music source separation demixes a piece of music into its individual sound sources (vocals, percussion, melodic instruments, etc.), a task with no simple mathematical solution. It requires deep learning methods involving training on large datasets of isolated music stems. The most commonly available datasets are made from commercial Western music, limiting the models\u2019 applications to non-Western genres like Carnatic music. Carnatic music is a live tradition, with the available multi-track recordings containing overlapping sounds and bleeds between the sources. This poses a challenge to commercially available source separation models like Spleeter and Hybrid Demucs. In this work, we introduce Sanidha, the first open-source novel dataset1 for Carnatic music, offering studio-quality, multi-track recordings with minimal to no overlap or bleed. Along with the audio files, we provide high-definition videos of the artists\u2019 performances. Additionally, we fine-tuned Spleeter, one of the most commonly used source separation models, on our dataset and observed improved SDR performance compared to finetuning on a pre-existing Carnatic multi-track dataset. The outputs of the fine-tuned model with Sanidha are evaluated through a listening study.",
    "zenodo_id": 14877433,
    "dblp_key": null
  },
  {
    "title": "Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music",
    "author": [
      "Pedro Pereira Sarmento",
      "Jackson J. Loth",
      "Mathieu Barthet"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877435",
    "url": "https://doi.org/10.5281/zenodo.14877435",
    "ee": "https://zenodo.org/record/14877435/files/000080.pdf",
    "pages": "713-720",
    "abstract": "Generative AI models have recently blossomed, significantly impacting artistic and musical traditions. Research investigating how humans interact with and deem these models is therefore crucial. Through a listening and reflection study, we explore participants\u2019 perspectives on AI-vs human-generated progressive metal, in symbolic format, using rock music as a control group. AI-generated examples were produced by ProgGP [1], a Transformer-based model. We propose a mixed methods approach to assess the effects of generation type (human vs. AI), genre (progressive metal vs. rock), and curation process (random vs. cherry-picked). This combines quantitative feedback on genre congruence, preference, creativity, consistency, playability, humanness, and repeatability, and qualitative feedback to provide insights into listeners\u2019 experiences. A total of 32 progressive metal fans completed the study. Our findings validate the use of fine-tuning to achieve genre-specific specialization in AI music generation, as listeners could distinguish between AI-generated rock and progressive metal. Despite some AI-generated excerpts receiving similar ratings to human music, listeners exhibited a preference for human compositions. Thematic analysis identified key features for genre and AI vs. human distinctions. Finally, we consider the ethical implications of our work in promoting musical data diversity within MIR research by focusing on an under-explored genre.",
    "zenodo_id": 14877435,
    "dblp_key": null
  },
  {
    "title": "Combining Audio Control and Style Transfer Using Latent Diffusion",
    "author": [
      "Nils Demerl\u00e9",
      "Philippe Esling",
      "Guillaume Doras",
      "David Genova"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877437",
    "url": "https://doi.org/10.5281/zenodo.14877437",
    "ee": "https://zenodo.org/record/14877437/files/000081.pdf",
    "pages": "721-728",
    "abstract": "Deep generative models are now able to synthesize high-quality audio signals, shifting the critical aspect in their development from audio quality to control capabilities. Although text-to-music generation is getting largely adopted by the general public, explicit control and example-based style transfer are more adequate modalities to capture the intents of artists and musicians. In this paper, we aim to unify explicit control and style transfer within a single model by separating local and global information to capture musical structure and timbre respectively. To do so, we leverage the capabilities of diffusion autoencoders to extract semantic features, in order to build two representation spaces. We enforce disentanglement between those spaces using an adversarial criterion and a two-stage training strategy. Our resulting model can generate audio matching a timbre target, while specifying structure either with explicit controls or through another audio example. We evaluate our model on one-shot timbre transfer and MIDI-to-audio tasks on instrumental recordings and show that we outperform existing baselines in terms of audio quality and target fidelity. Furthermore, we show that our method can generate cover versions of complete musical pieces by transferring rhythmic and melodic content to the style of a target audio in a different genre.",
    "zenodo_id": 14877437,
    "dblp_key": null
  },
  {
    "title": "Computational Analysis of Yaredawi YeZema Silt in Ethiopian Orthodox Tewahedo Church Chants",
    "author": [
      "Mequanent Argaw Muluneh",
      "Yan-Tsung Peng",
      "Li Su"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877441",
    "url": "https://doi.org/10.5281/zenodo.14877441",
    "ee": "https://zenodo.org/record/14877441/files/000082.pdf",
    "pages": "729-736",
    "abstract": "Despite its musicological, cultural, and religious significance, the Ethiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented in music research. Historical records, including manuscripts, research papers, and oral traditions, confirm Saint Yared\u2019s establishment of three canonical EOTC chanting modes during the 6th century. This paper attempts to investigate the EOTC chants using music information retrieval (MIR) techniques. Among the research questions regarding the analysis and understanding of EOTC chants, Yaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared\u2019s standards, is of primary importance. Therefore, we consider the task of Yaredawi YeZema Silt classification in EOTC chants by introducing a new dataset and showcasing a series of classification experiments for this task. Results show that using the distribution of stabilized pitch contours as the feature representation on a simple neural-network-based classifier becomes an effective solution. The musicological implications and insights of such results are further discussed through a comparative study with the previous ethnomusicology literature on EOTC chants. By making this dataset publicly accessible, our aim is to promote future exploration and analysis of EOTC chants and highlight potential directions for further research, thereby fostering a deeper understanding and preservation of this unique spiritual and cultural heritage.",
    "zenodo_id": 14877441,
    "dblp_key": null
  },
  {
    "title": "Lyrics Transcription for Humans: A Readability-Aware Benchmark",
    "author": [
      "Ond\u0159ej C\u00edfka",
      "Hendrik Schreiber",
      "Luke Miner",
      "Fabian-Robert St\u00f6ter"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877443",
    "url": "https://doi.org/10.5281/zenodo.14877443",
    "ee": "https://zenodo.org/record/14877443/files/000083.pdf",
    "pages": "737-744",
    "abstract": "Writing down lyrics for human consumption involves not only accurately capturing word sequences, but also incorporating punctuation and formatting for clarity and to convey contextual information. This includes song structure, emotional emphasis, and contrast between lead and background vocals. While automatic lyrics transcription (ALT) systems have advanced beyond producing unstructured strings of words and are able to draw on wider context, ALT benchmarks have not kept pace and continue to focus exclusively on words. To address this gap, we introduce Jam-ALT, a comprehensive lyrics transcription benchmark. The benchmark features a complete revision of the JamendoLyrics dataset, in adherence to industry standards for lyrics transcription and formatting, along with evaluation metrics designed to capture and assess the lyric-specific nuances, laying the foundation for improving the readability of lyrics. We apply the benchmark to recent transcription systems and present additional error analysis, as well as an experimental comparison with a classical music dataset.",
    "zenodo_id": 14877443,
    "dblp_key": null
  },
  {
    "title": "A Critical Survey of Research in Music Genre Recognition",
    "author": [
      "Owen Green",
      "Bob L. T. Sturm",
      "Georgina Born",
      "Melanie Wald-Fuhrmann"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877445",
    "url": "https://doi.org/10.5281/zenodo.14877445",
    "ee": "https://zenodo.org/record/14877445/files/000084.pdf",
    "pages": "745-782",
    "abstract": "This paper surveys 560 publications about music genre recognition (MGR) published between 2013\u20132022, complementing the comprehensive survey of [474], which covered the time frame 1995\u20132012 (467 publications). For each publication we determine its main functions: a review of research, a contribution to evaluation methodology, or an experimental work. For each experimental work we note the data, experimental approach, and figure of merit it applies. We also note the extents to which any publication engages with work critical of MGR as a research problem, as well as genre theory. Our bibliographic analysis shows for MGR research: 1) it typically does not meaningfully engage with any critique of itself; and 2) it typically does not meaningfully engage with work in genre theory.",
    "zenodo_id": 14877445,
    "dblp_key": null
  },
  {
    "title": "Content-Based Controls for Music Large Language Modeling",
    "author": [
      "Liwei Lin",
      "Gus Xia",
      "Junyan Jiang",
      "Yixiao Zhang"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877447",
    "url": "https://doi.org/10.5281/zenodo.14877447",
    "ee": "https://zenodo.org/record/14877447/files/000085.pdf",
    "pages": "783-790",
    "abstract": "Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music indirectly through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). We aim to further equip the models with direct and content-based controls on innate music languages such as pitch, chords and drum track. To this end, we contribute Coco-Mulla, a content-based control method for music large language modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieves high-quality music generation with low-resource semi-supervised learning. We fine-tune the model with less than 4% of the orignal parameters on a small dataset with fewer than 300 songs. Moreover, our approach enables effective content-based controls. We illustrate its controllability via chord and rhythm conditions, two of the most salient features of pop music. Furthermore, we show that by combining content-based controls and text descriptions, our system achieves flexible music variation generation and arrangement. Our source codes and demos are available online.",
    "zenodo_id": 14877447,
    "dblp_key": null
  },
  {
    "title": "Exploring the Inner Mechanisms of Large Generative Music Models",
    "author": [
      "Marcel A. V\u00e9lez V\u00e1squez",
      "Charlotte Pouw",
      "John Ashley Burgoyne",
      "Willem Zuidema"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877449",
    "url": "https://doi.org/10.5281/zenodo.14877449",
    "ee": "https://zenodo.org/record/14877449/files/000086.pdf",
    "pages": "791-798",
    "abstract": "GGenerative models are starting to become very good at generating realistic text, images, and even music. Identifying how exactly these models conceptualize data has become crucial. To date, however, interpretability research has mainly focused on the text and image domain, leaving a gap in the music domain. In this paper, we investigate the transferability of straightforward text-oriented interpretability techniques to the music domain. Specifically, we examine the usability of these techniques for analyzing how the generative music model MusicGen constructs representations of human-interpretable musicological concepts. Using the DecoderLens, we gain insight into how the model gradually composes these concepts, and using interchange interventions, we observe the contributions of individual model components in generating the sound of specific instruments and genres. We also encounter several shortcomings of the interpretability techniques for the music domain, which underscore the complexity of music and need for proper audio-oriented adaptation. Our research marks an initial step toward understanding generative music models, fundamentally, paving the way for future advancements in controlling music generation.",
    "zenodo_id": 14877449,
    "dblp_key": null
  },
  {
    "title": "Quantitative Analysis of Melodic Similarity in Music Copyright Infringement Cases",
    "author": [
      "Saebyul Park",
      "Halla Kim",
      "Jiye Jung",
      "Juyong Park",
      "Jeounghoon Kim",
      "Juhan Nam"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877451",
    "url": "https://doi.org/10.5281/zenodo.14877451",
    "ee": "https://zenodo.org/record/14877451/files/000087.pdf",
    "pages": "799-806",
    "abstract": "This study aims to measure the similarity of melodies objectively using natural language processing (NLP) techniques. We utilize Mel2word which is a melody tokenization method based on byte-pair encoding to facilitate the semantic analysis of melodies. In addition, we apply two word weighting methods: the modified Tversky measure for word salience and the TF-IDF method for word importance and uniqueness, to better understand the characteristics of each melodic element. We validate our approach by comparing song vectors calculated from an average of Mel2Word vectors to the ground truth in 108 cases of music copyright infringement, sourced from an extensive review of legal documents from law archives. The results demonstrate that the proposed approach is more in accordance with court rulings and perceptual similarity.",
    "zenodo_id": 14877451,
    "dblp_key": null
  },
  {
    "title": "Robust Lossy Audio Compression Identification",
    "author": [
      "Hendrik Vincent Koops",
      "Gianluca Micchi",
      "Elio Quinton"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877453",
    "url": "https://doi.org/10.5281/zenodo.14877453",
    "ee": "https://zenodo.org/record/14877453/files/000088.pdf",
    "pages": "807-813",
    "abstract": "Previous research contributions on blind lossy compression identification report near perfect performance metrics on their test set, across a variety of codecs and bit rates. However, we show that such results can be deceptive and may not accurately represent true ability of the system to tackle the task at hand. In this article, we present an investigation into the robustness and generalisation capability of a lossy audio identification model. Our contributions are as follows. (1) We show the lack of robustness to codec parameter variations of a model equivalent to prior art. In particular, when naively training a lossy compression detection model on a dataset of music recordings processed with a range of codecs and their lossless counterparts, we obtain near perfect performance metrics on the held-out test set, but severely degraded performance on lossy tracks produced with codec parameters not seen in training. (2) We propose and show the effectiveness of an improved training strategy to significantly increase the robustness and generalisation capability of the model beyond codec configurations seen during training. Namely we apply a random mask to the input spectrogram to encourage the model not to rely solely on the training set\u2019s codec cutoff frequency.",
    "zenodo_id": 14877453,
    "dblp_key": null
  },
  {
    "title": "RNBert: Fine-Tuning a Masked Language Model for Roman Numeral Analysis",
    "author": [
      "Malcolm Sailor"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877455",
    "url": "https://doi.org/10.5281/zenodo.14877455",
    "ee": "https://zenodo.org/record/14877455/files/000089.pdf",
    "pages": "814-821",
    "abstract": "Music is plentiful, but labeled data for music theory tasks like roman numeral analysis is scarce. Self-supervised pre-training is therefore a promising avenue for improving performance on these tasks, especially because, in learning a task like predicting masked notes, a model may acquire latent representations of music theory concepts like keys and chords. However, existing models for roman numeral analysis have not used pretraining, instead training from scratch on labeled data, while conversely, pretrained models for music understanding have generally been applied to sequence-level tasks requiring little explicit music theory, such as composer classification. In contrast, this paper applies pretraining methods to a music theory task by fine-tuning a masked language model, MusicBERT, for roman numeral analysis. We apply token classification to get a chord label for each note and then aggregate the predictions of simultaneous notes to achieve a single label at each time step. The resulting model substantially outperforms previous roman numeral analysis models. Our approach can readily be extended to other note- and/or chord- level music theory tasks (e.g., nonharmonic tone analysis, melody harmonization).",
    "zenodo_id": 14877455,
    "dblp_key": null
  },
  {
    "title": "MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models",
    "author": [
      "Benno Weck",
      "Ilaria Manco",
      "Emmanouil Benetos",
      "Elio Quinton",
      "George Fazekas",
      "Dmitry Bogdanov"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877459",
    "url": "https://doi.org/10.5281/zenodo.14877459",
    "ee": "https://zenodo.org/record/14877459/files/000090.pdf",
    "pages": "825-833",
    "abstract": "Multimodal models that jointly process audio and language hold great promise in audio understanding and are increasingly being adopted in the music domain. By allowing users to query via text and obtain information about a given audio input, these models have the potential to enable a variety of music understanding tasks via language-based interfaces. However, their evaluation poses considerable challenges, and it remains unclear how to effectively assess their ability to correctly interpret music-related inputs with current methods. Motivated by this, we introduce MuChoMusic, a benchmark for evaluating music understanding in multimodal language models focused on audio. MuChoMusic comprises 1,187 multiple-choice questions, all validated by human annotators, on 644 music tracks sourced from two publicly available music datasets, and covering a wide variety of genres. Questions in the benchmark are crafted to assess knowledge and reasoning abilities across several dimensions that cover fundamental musical concepts and their relation to cultural and functional contexts. Through the holistic analysis afforded by the benchmark, we evaluate five open-source models and identify several pitfalls, including an over-reliance on the language modality, pointing to a need for better multimodal integration. Data and code are open-sourced.",
    "zenodo_id": 14877459,
    "dblp_key": null
  },
  {
    "title": "Human Pose Estimation for Expressive Movement Descriptors in Vocal Musical Performances",
    "author": [
      "Sujoy Roychowdhury",
      "Preeti Rao",
      "Sharat Chandran"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877458",
    "url": "https://doi.org/10.5281/zenodo.14877458",
    "ee": "https://zenodo.org/record/14877458/files/000091.pdf",
    "pages": "834-841",
    "abstract": "Vocal concerts in Indian music are invariably associated with the performers\u2019 hand gesticulations that are believed to convey emotion, music semantics as well as the individual style of the performers. Video recordings, with one or more cameras, along with markerless human pose estimation algorithms can be employed to capture such movements, and thus potentially solve music information retrieval (MIR) queries. Nevertheless, off-the-shelf algorithms are built for the most part for upright human configurations contrasting with seated positions in Indian vocal concerts and the upper body movements in the context of performing music. Current state-of-the-art algorithms are black box neural network based and this calls for an investigation of the components of such algorithms. Key decisions involve the choice of one or more cameras, the choice of 2D or 3D features, and relevant parameters such as confidence thresholds in common machine learning methods. In this paper, we quantify the increase in the performance with three cameras on two music information retrieval tasks. We offer insights for single and multi-view processing of videos.",
    "zenodo_id": 14877458,
    "dblp_key": null
  },
  {
    "title": "Enhancing Predictive Models of Music Familiarity With EEG: Insights From Fans and Non-Fans of K-Pop Group NCT127",
    "author": [
      "Seokbeom Park",
      "Hyunjae Kim",
      "Kyung Myun Lee"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877461",
    "url": "https://doi.org/10.5281/zenodo.14877461",
    "ee": "https://zenodo.org/record/14877461/files/000092.pdf",
    "pages": "842-849",
    "abstract": "Predicting a listener\u2019s experience of music based solely on audio features has its limitations due to the individual variability in responses to the same music. This study examines the effectiveness of electroencephalogram (EEG) in predicting the subjective experiences while listening to music, including arousal, valence, familiarity, and preference. We collected EEG data alongside subjective ratings of arousal, valence, familiarity, and preference from both fans (N=20) and non-fans (N=34) of the K-pop idol group, NCT127 to investigate response variability to the same NCT127 music. Our analysis focused on determining whether the inclusion of EEG alongside audio features could enhance the predictive power of linear mixed-effect models for these subjective ratings. Specifically, we employed stimulus-response correlation (SRC), a recent approach in neuroscience correlating stimulus features with EEG responses to the ecologically valid stimuli. The results showed that familiarity and preference was significantly higher in the fan group. Furthermore, the inclusion of SRC significantly enhanced the prediction of familiarity compared to models based solely on audio features. However, the impact of SRC on predictions of arousal and valence exhibited variation depending on the correlated audio features, with certain SRCs improving predictions while others diminished them. For preference, only a few SRCs negatively affected model performance. These results suggest that correlations of EEG responses and audio features can provide information of individual listeners\u2019 subjective responses, particularly in predicting familiarity.",
    "zenodo_id": 14877461,
    "dblp_key": null
  },
  {
    "title": "Mosaikbox: Improving Fully Automatic DJ Mixing Through Rule-Based Stem Modification and Precise Beat-Grid Estimation",
    "author": [
      "Robert Sowula",
      "Peter Knees"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877463",
    "url": "https://doi.org/10.5281/zenodo.14877463",
    "ee": "https://zenodo.org/record/14877463/files/000093.pdf",
    "pages": "850-857",
    "abstract": "We present a novel system for automatic music mixing combining diverse music information retrieval (MIR) techniques and sources for song selection and transitioning. Specifically, we explore how music source separation and stem analysis can contribute to the task of music similarity calculation by modifying incompatible stems using a rule-based approach and investigate how audio-based similarity measures can be supplemented by lyrics as contextual information to capture more aspects of music. Additionally, we propose a novel approach for tempo detection, outperforming state-of-the-art techniques in low error-tolerance windows. We evaluate our approaches using a listening experiment and compare them to a state-of-the-art model as a baseline. The results show that our approach to automatic song selection and automated music mixing significantly outperforms the baseline and that our rule-based stem removal approach significantly enhances the perceived quality of a mix. No improvement can be observed for the inclusion of contextual information, i.e., mood information derived from lyrics, into the music similarity measure.",
    "zenodo_id": 14877463,
    "dblp_key": null
  },
  {
    "title": "MidiCaps: A Large-Scale MIDI Dataset With Text Captions",
    "author": [
      "Jan Melechovsky",
      "Abhinaba Roy",
      "Dorien Herremans"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877465",
    "url": "https://doi.org/10.5281/zenodo.14877465",
    "ee": "https://zenodo.org/record/14877465/files/000094.pdf",
    "pages": "858-865",
    "abstract": "Generative models guided by text prompts are increasingly becoming more popular. However, no text-to-MIDI models currently exist due to the lack of a captioned MIDI dataset. This work aims to enable research that combines LLMs with symbolic music by presenting MidiCaps, the first openly available large-scale MIDI dataset with text captions. MIDI (Musical Instrument Digital Interface) files are widely used for encoding musical information and can capture the nuances of musical composition. They are widely used by music producers, composers, musicologists, and performers alike. Inspired by recent advancements in captioning techniques, we present a curated dataset of over 168k MIDI files with textual descriptions. Each MIDI caption describes the musical content, including tempo, chord progression, time signature, instruments, genre, and mood, thus facilitating multi-modal exploration and analysis. The dataset encompasses various genres, styles, and complexities, offering a rich data source for training and evaluating models for tasks such as music information retrieval, music understanding, and cross-modal translation. We provide detailed statistics about the dataset and have assessed the quality of the captions in an extensive listening study. We anticipate that this resource will stimulate further research at the intersection of music and natural language processing, fostering advancements in both fields.",
    "zenodo_id": 14877465,
    "dblp_key": null
  },
  {
    "title": "A New Dataset, Notation Software, and Representation for Computational Schenkerian Analysis",
    "author": [
      "Stephen Ni-Hahn",
      "Weihan Xu",
      "Zirui Yin",
      "Rico Zhu",
      "Simon Mak",
      "Yue Jiang",
      "Cynthia Rudin"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877467",
    "url": "https://doi.org/10.5281/zenodo.14877467",
    "ee": "https://zenodo.org/record/14877467/files/000095.pdf",
    "pages": "866-873",
    "abstract": "Schenkerian Analysis (SchA) is a uniquely expressive method of music analysis, combining elements of melody, harmony, counterpoint, and form to describe the hierarchical structure supporting a work of music. However, despite its powerful analytical utility and potential to improve music understanding and generation, SchA has rarely been utilized by the computer music community. This is in large part due to the paucity of available high-quality data in a computer-readable format. With a larger corpus of Schenkerian data, it may be possible to infuse machine learning models with a deeper understanding of musical structure, thus leading to more \u201chuman\u201d results. To encourage further research in Schenkerian analysis and its potential benefits for music informatics and generation, this paper presents three main contributions: 1) a new and growing dataset of SchAs, the largest in human- and computer-readable formats to date (>140 excerpts), 2) a novel software for visualization and collection of SchA data, and 3) a novel, flexible representation of SchA as a heterogeneous-edge graph data structure.",
    "zenodo_id": 14877467,
    "dblp_key": null
  },
  {
    "title": "DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music Generation",
    "author": [
      "Zachary Novack",
      "Julian McAuley",
      "Taylor Berg-Kirkpatrick",
      "Nicholas J. Bryan"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877469",
    "url": "https://doi.org/10.5281/zenodo.14877469",
    "ee": "https://zenodo.org/record/14877469/files/000096.pdf",
    "pages": "874-881",
    "abstract": "Controllable music generation methods are critical for human-centered AI-based music creation, but are currently limited by speed, quality, and control design trade-offs. Diffusion inference-time T-optimization (DITTO), in particular, offers state-of-the-art results, but is over 10x slower than real-time, limiting practical use. We propose Distilled Diffusion Inference-Time T -Optimization (or DITTO-2), a new method to speed up inference-time optimization-based control and unlock faster-than-real-time generation for a wide-variety of applications such as music inpainting, outpainting, intensity, melody, and musical structure control. Our method works by (1) distilling a pre-trained diffusion model for fast sampling via an efficient, modified consistency or consistency trajectory distillation process (2) performing inference-time optimization using our distilled model with one-step sampling as an efficient surrogate optimization task and (3) running a final multi-step sampling generation (decoding) using our estimated noise latents for best-quality, fast, controllable generation. Through thorough evaluation, we find our method not only speeds up generation over 10-20x, but simultaneously improves control adherence and generation quality all at once. Furthermore, we apply our approach to a new application of maximizing text adherence (CLAP score) and show we can convert an unconditional diffusion model without text inputs into a model that yields state-of-the-art text control. Sound examples can be found at https://ditto-music.github.io/ditto2/.",
    "zenodo_id": 14877469,
    "dblp_key": null
  },
  {
    "title": "The Concatenator: A Bayesian Approach to Real Time Concatenative Musaicing",
    "author": [
      "Christopher J. Tralie",
      "Ben Cantil"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877471",
    "url": "https://doi.org/10.5281/zenodo.14877471",
    "ee": "https://zenodo.org/record/14877471/files/000097.pdf",
    "pages": "882-889",
    "abstract": "We present \u201cThe Concatenator,\u201d a real time system for audio-guided concatenative synthesis. Similarly to Driedger et al.\u2019s \u201cmusaicing\u201d (or \u201caudio mosaicing\u201d) technique, we concatenate a set number of windows within a corpus of audio to re-create the harmonic and percussive aspects of a target audio stream. Unlike Driedger\u2019s NMF-based technique, however, we instead use an explicitly Bayesian point of view, where corpus window indices are hidden states and the target audio stream is an observation. We use a particle filter to infer the best hidden corpus states in real-time. Our transition model includes a tunable parameter to control the time-continuity of corpus grains, and our observation model allows users to prioritize how quickly windows change to match the target. Because the computational complexity of the system is independent of the corpus size, our system scales to corpora that are hours long, which is an important feature in the age of vast audio data collections. Within The Concatenator module itself, composers can vary grain length, fit to target, and pitch shift in real time while reacting to the sounds they hear, enabling them to rapidly iterate ideas. To conclude our work, we evaluate our system with extensive quantitative tests of the effects of parameters, as well as a qualitative evaluation with artistic insights. Based on the quality of the results, we believe the real-time capability unlocks new avenues for musical expression and control, suitable for live performance and modular synthesis integration, which furthermore represents an essential breakthrough in concatenative synthesis technology.",
    "zenodo_id": 14877471,
    "dblp_key": null
  },
  {
    "title": "Deep Recombinant Transformer: Enhancing Loop Compatibility in Digital Music Production",
    "author": [
      "Muhammad Taimoor Haseeb",
      "Ahmad Hammoudeh",
      "Gus Xia"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877473",
    "url": "https://doi.org/10.5281/zenodo.14877473",
    "ee": "https://zenodo.org/record/14877473/files/000098.pdf",
    "pages": "890-896",
    "abstract": "The widespread availability of music loops has revolutionized music production. However, combining loops requires a nuanced understanding of musical compatibility that can be difficult to learn and time-consuming. This study concentrates on the \u2019vertical problem\u2019 of music loop compatibility, which pertains to layering different loops to create a harmonious blend. The main limitation to applying deep learning in this domain is the absence of a large, high-quality, labeled dataset containing both positive and negative pairs. To address this, we synthesize high-quality audio from multi-track MIDI datasets containing independent instrument stems, and then extract loops to serve as positive pairs. This provides models with instrument-level information when learning compatibility. Moreover, we improve the generation of negative examples by matching the key and tempo of candidate loops, and then employing AutoMashUpper [1] to identify incompatible loops. Creating a large dataset allows us to introduce and examine the application of Transformer architectures for addressing vertical loop compatibility. Experimental results show that our method outperforms the previous state-of-the-art, achieving an 18.6% higher accuracy across multiple genres. Subjective assessments rate our model higher in seamlessly and creatively combining loops, underscoring our method\u2019s effectiveness. We name our approach the Deep Recombinant Transformer and provide audio samples.",
    "zenodo_id": 14877473,
    "dblp_key": null
  },
  {
    "title": "I Can Listen but Cannot Read: An Evaluation of Two-Tower Multimodal Systems for Instrument Recognition",
    "author": [
      "Yannis Vasilakis",
      "Rachel Bittner",
      "Johan Pauwels"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877475",
    "url": "https://doi.org/10.5281/zenodo.14877475",
    "ee": "https://zenodo.org/record/14877475/files/000099.pdf",
    "pages": "897-905",
    "abstract": "Music two-tower multimodal systems integrate audio and text modalities into a joint audio-text space, enabling direct comparison between songs and their corresponding labels. These systems enable new approaches for classification and retrieval, leveraging both modalities. Despite the promising results they have shown for zero-shot classification and retrieval tasks, closer inspection of the embeddings is needed. This paper evaluates the inherent zero-shot properties of joint audio-text spaces for the case-study of instrument recognition. We present an evaluation and analysis of two-tower systems for zero-shot instrument recognition and a detailed analysis of the properties of the pre-joint and joint embedding spaces. Our findings suggest that audio encoders alone demonstrate good quality, while challenges remain within the text encoder or joint space projection. Specifically, two-tower systems exhibit sensitivity towards specific words, favoring generic prompts over musically informed ones. Despite the large size of textual encoders, they do not yet leverage additional textual context or infer instruments accurately from their descriptions. Lastly, a novel approach for quantifying the semantic meaningfulness of the textual space leveraging an instrument ontology is proposed. This method reveals deficiencies in the systems\u2019 understanding of instruments and provides evidence of the need for fine-tuning text encoders on musical data.",
    "zenodo_id": 14877475,
    "dblp_key": null
  },
  {
    "title": "Streaming Piano Transcription Based on Consistent Onset and Offset Decoding With Sustain Pedal Detection",
    "author": [
      "Weixing Wei",
      "Jiahao Zhao",
      "Yulun Wu",
      "Kazuyoshi Yoshii"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877477",
    "url": "https://doi.org/10.5281/zenodo.14877477",
    "ee": "https://zenodo.org/record/14877477/files/000100.pdf",
    "pages": "906-913",
    "abstract": "This paper describes a streaming audio-to-MIDI transcription method that can sequentially translate a piano recording into a sequence of note-on and note-off events. The sequence-to-sequence learning nature of this task may call for using a Transformer model, which has been used for offline transcription and could be extended for streaming transcription with a causal restriction of the attention mechanism. We assume that the decoder of this model suffers from the performance limitation. Although time-frequency features useful for onset detection are considerably different from those for offset detection, the single decoder is trained to output a mixed sequence of onset and offset events without guarantee of the correspondence between the onset and offset events of the same note. To overcome this limitation, we propose a streaming encoder-decoder model that uses a convolutional encoder aggregating local acoustic features, followed by an autoregressive transformer decoder detecting a variable number of onset events and another decoder detecting the offset events of the active pitches with validation of the sustain pedal at each time frame. Experiments using the MAESTRO dataset showed that the proposed streaming method performed comparably with or even better than the state-of-the-art offline methods while significantly reducing the computational cost.",
    "zenodo_id": 14877477,
    "dblp_key": null
  },
  {
    "title": "Towards Universal Optical Music Recognition: A Case Study on Notation Types",
    "author": [
      "Juan Carlos Martinez-Sevilla",
      "David Rizo",
      "Jorge Calvo-Zaragoza"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877479",
    "url": "https://doi.org/10.5281/zenodo.14877479",
    "ee": "https://zenodo.org/record/14877479/files/000101.pdf",
    "pages": "914-921",
    "abstract": "Recent advances in Deep Learning have propelled the development of fields such as Optical Music Recognition (OMR), which is responsible for extracting the content from music score images. Despite progress in the field, existing literature scarcely addresses core issues like performance in real-world scenarios, user experience, maintainability of multiple pipelines, reusability of architectures and data, among others. These factors result in high costs for both users and developers of such systems. Furthermore, research has often been conducted under certain constraints, such as using a single musical texture or type of notation, which may not align with the end-user requirements of OMR systems. For the first time, our study involves a comprehensive and extensive experimental setup to explore new ideas towards the development of a universal OMR system\u2014capable of transcribing all textures and notation types. Our investigation provides valuable insights into several aspects, such as the ability of a model to leverage knowledge from different domains despite significant differences in music notation types.",
    "zenodo_id": 14877479,
    "dblp_key": null
  },
  {
    "title": "Controlling Surprisal in Music Generation via Information Content Curve Matching",
    "author": [
      "Mathias Rose Bjare",
      "Stefan Lattner",
      "Gerhard Widmer"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877481",
    "url": "https://doi.org/10.5281/zenodo.14877481",
    "ee": "https://zenodo.org/record/14877481/files/000102.pdf",
    "pages": "922-929",
    "abstract": "In recent years, the quality and public interest in music generation systems have grown, encouraging research into various ways to control these systems. We propose a novel method for controlling surprisal in music generation using sequence models. To achieve this goal, we define a metric called Instantaneous Information Content (IIC). The IIC serves as a proxy function for the perceived musical surprisal (as estimated from a probabilistic model) and can be calculated at any point within a music piece. This enables the comparison of surprisal across different musical content even if the musical events occur in irregular time intervals. We use beam search to generate musical material whose IIC curve closely approximates a given target IIC. We experimentally show that the IIC correlates with harmonic and rhythmic complexity and note density. The correlation decreases with the length of the musical context used for estimating the IIC. Finally, we conduct a qualitative user study to test if human listeners can identify the IIC curves that have been used as targets when generating the respective musical material. We provide code for creating IIC interpolations and IIC visualizations on https://github.com/muthissar/iic.",
    "zenodo_id": 14877481,
    "dblp_key": null
  },
  {
    "title": "Toward a More Complete OMR Solution",
    "author": [
      "Guang Yang",
      "Muru Zhang",
      "Lin Qiu",
      "Yanming Wan",
      "Noah A. Smith"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877483",
    "url": "https://doi.org/10.5281/zenodo.14877483",
    "ee": "https://zenodo.org/record/14877483/files/000103.pdf",
    "pages": "930-937",
    "abstract": "Optical music recognition (OMR) aims to convert music notation into digital formats. One approach to tackle OMR is through a multi-stage pipeline, where the system first detects visual music notation elements in the image (object detection) and then assembles them into a music notation (notation assembly). Most previous work on notation assembly unrealistically assumes perfect object detection. In this study, we focus on the MUSCIMA++ v2.0 dataset, which represents musical notation as a graph with pairwise relationships among detected music objects, and we consider both stages together. First, we introduce a music object detector based on YOLOv8, which improves detection performance. Second, we introduce a supervised training pipeline that completes the notation assembly stage based on detection output. We find that this model is able to outperform existing models trained on perfect detection output, showing the benefit of considering the detection and assembly stages in a more holistic way. These findings, together with our novel evaluation metric, are important steps toward a more complete OMR solution.",
    "zenodo_id": 14877483,
    "dblp_key": null
  },
  {
    "title": "Augment, Drop & Swap: Improving Diversity in LLM Captions for Efficient Music-Text Representation Learning",
    "author": [
      "Ilaria Manco",
      "Justin Salamon",
      "Oriol Nieto"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877485",
    "url": "https://doi.org/10.5281/zenodo.14877485",
    "ee": "https://zenodo.org/record/14877485/files/000104.pdf",
    "pages": "938-945",
    "abstract": "Audio-text contrastive models have become a powerful approach in music representation learning. Despite their empirical success, however, little is known about the influence of key design choices on the quality of music-text representations learnt through this framework. In this work, we expose these design choices within the constraints of limited data and computation budgets, and establish a more solid understanding of their impact grounded in empirical observations along three axes: the choice of base encoders, the level of curation in training data, and the use of text augmentation. We find that data curation is the single most important factor for music-text contrastive training in resource-constrained scenarios. Motivated by this insight, we introduce two novel techniques, Augmented View Dropout and TextSwap, which increase the diversity and descriptiveness of text inputs seen in training. Through our experiments we demonstrate that these are effective at boosting performance across different pre-training regimes, model architectures, and downstream data distributions, without incurring higher computational costs or requiring additional training data.",
    "zenodo_id": 14877485,
    "dblp_key": null
  },
  {
    "title": "Music Discovery Dialogue Generation Using Human Intent Analysis and Large Language Models",
    "author": [
      "Seungheon Doh",
      "Keunwoo Choi",
      "Daeyong Kwon",
      "Taesoo Kim",
      "Juhan Nam"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877487",
    "url": "https://doi.org/10.5281/zenodo.14877487",
    "ee": "https://zenodo.org/record/14877487/files/000105.pdf",
    "pages": "946-953",
    "abstract": "A conversational music retrieval system can help users discover music that matches their preferences through dialogue. To achieve this, a conversational music retrieval system should seamlessly engage in multi-turn conversation by 1) understanding user queries and 2) responding with natural language and retrieved music. A straightforward solution would be a data-driven approach utilizing such conversation logs. However, few datasets are available for the research and are limited in terms of volume and quality. In this paper, we present a data generation framework for rich music discovery dialogue using a large language model (LLM) and user intents, system actions, and musical attributes. This is done by i) dialogue intent analysis using grounded theory, ii) generating attribute sequences via cascading database filtering, and iii) generating utterances using large language models. By applying this framework to the Million Song dataset, we create \u2013 LP-MusicDialog, a Large Language Model based Pseudo Music Dialogue dataset, containing over 288k music conversations using more than 319k music items. Our evaluation shows that the synthetic dataset is competitive with an existing, small human dialogue dataset in terms of dialogue consistency, item relevance, and naturalness. Furthermore, using the dataset, we train a conversational music retrieval model and show promising results.",
    "zenodo_id": 14877487,
    "dblp_key": null
  },
  {
    "title": "STONE: Self-Supervised Tonality Estimator",
    "author": [
      "Yuexuan Kong",
      "Vincent Lostanlen",
      "Gabriel Meseguer-Brocal",
      "Stella Wong",
      "Mathieu Lagrange",
      "Romain Hennequin"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877489",
    "url": "https://doi.org/10.5281/zenodo.14877489",
    "ee": "https://zenodo.org/record/14877489/files/000106.pdf",
    "pages": "954-961",
    "abstract": "Although deep neural networks can estimate the key of a musical piece, their supervision incurs a massive annotation effort. Against this shortcoming, we present STONE, the first self-supervised tonality estimator. The architecture behind STONE, named ChromaNet, is a convnet with octave equivalence which outputs a \u201ckey signature profile\u201d (KSP) of 12 structured logits. First, we train ChromaNet to regress artificial pitch transpositions between any two unlabeled musical excerpts from the same audio track, as measured as cross-power spectral density (CPSD) within the circle of fifths (CoF). We observe that this self-supervised pretext task leads KSP to correlate with tonal key signature. Based on this observation, we extend STONE to output a structured KSP of 24 logits, and introduce supervision so as to disambiguate major versus minor keys sharing the same key signature. Applying different amounts of supervision yields semi-supervised and fully supervised tonality estimators: i.e., Semi-TONEs and Sup-TONEs. We evaluate these estimators on FMAK, a new dataset of 5489 real-world musical recordings with expert annotation of 24 major and minor keys. We find that Semi-TONE matches the classification accuracy of Sup-TONE with reduced supervision and outperforms it with equal supervision.",
    "zenodo_id": 14877489,
    "dblp_key": null
  },
  {
    "title": "Beat This! Accurate Beat Tracking Without DBN Postprocessing",
    "author": [
      "Francesco Foscarin",
      "Jan Schl\u00fcter",
      "Gerhard Widmer"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877491",
    "url": "https://doi.org/10.5281/zenodo.14877491",
    "ee": "https://zenodo.org/record/14877491/files/000107.pdf",
    "pages": "962-969",
    "abstract": "We propose a system for tracking beats and downbeats with two objectives: generality across a diverse music range, and high accuracy. We achieve generality by training on multiple datasets \u2013 including solo instrument recordings, pieces with time signature changes, and classical music with high tempo variations \u2013 and by removing the commonly used Dynamic Bayesian Network (DBN) postprocessing, which introduces constraints on the meter and tempo. For high accuracy, among other improvements, we develop a loss function tolerant to small time shifts of annotations, and an architecture alternating convolutions with transformers either over frequency or time. Our system surpasses the current state of the art in F1 score despite using no DBN. However, it can still fail, especially for difficult and underrepresented genres, and performs worse on continuity metrics, so we publish our model, code, and preprocessed datasets, and invite others to beat this.",
    "zenodo_id": 14877491,
    "dblp_key": null
  },
  {
    "title": "Scoring Time Intervals Using Non-Hierarchical Transformer for Automatic Piano Transcription",
    "author": [
      "Yujia Yan",
      "Zhiyao Duan"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877493",
    "url": "https://doi.org/10.5281/zenodo.14877493",
    "ee": "https://zenodo.org/record/14877493/files/000108.pdf",
    "pages": "973-980",
    "abstract": "The neural semi-Markov Conditional Random Field (semi-CRF) framework has demonstrated promise for event-based piano transcription. In this framework, all events (notes or pedals) are represented as closed time intervals tied to specific event types. The neural semi-CRF approach requires an interval scoring matrix that assigns a score for every candidate interval. However, designing an efficient and expressive architecture for scoring intervals is not trivial. This paper introduces a simple method for scoring intervals using scaled inner product operations that resemble how attention scoring is done in transformers. We show theoretically that, due to the special structure from encoding the non-overlapping intervals, under a mild condition, the inner product operations are expressive enough to represent an ideal scoring matrix that can yield the correct transcription result. We then demonstrate that an encoder-only non-hierarchical transformer backbone, operating only on a low-time-resolution feature map, is capable of transcribing piano notes and pedals with high accuracy and time precision. The experiment shows that our approach achieves the new state-of-the-art performance across all subtasks in terms of the F1 measure on the Maestro dataset. See appendix for post-camera-ready updates.",
    "zenodo_id": 14877493,
    "dblp_key": null
  },
  {
    "title": "PerTok: Expressive Encoding and Modeling of Symbolic Musical Ideas and Variations",
    "author": [
      "Julian Lenz",
      "Anirudh Mani"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877495",
    "url": "https://doi.org/10.5281/zenodo.14877495",
    "ee": "https://zenodo.org/record/14877495/files/000109.pdf",
    "pages": "981-988",
    "abstract": "We introduce Cadenza, a new multi-stage generative framework for predicting expressive variations of symbolic musical ideas as well as unconditional generations. To accomplish this we propose a novel MIDI encoding method, PerTok (Performance Tokenizer) that captures minute expressive details whilst reducing sequence length up to 59% and vocabulary size up to 95% for polyphonic, monophonic and rhythmic tasks. The proposed framework comprises of two sequential stages: 1) Composer and 2) Performer. The Composer model is a transformer-based Variational Autoencoder (VAE), with Rotary Positional Embeddings (RoPE) [1] and an autoregressive decoder modified to more effectively integrate the latent codes of the input musical idea. The Performer model is a bidirectional transformer encoder that is separately trained to predict velocities and microtimings on MIDI sequences. Objective and human evaluations demonstrate Cadenza\u2019s versatile capability in 1) matching other unconditional state-of-the-art symbolic models in musical quality whilst sounding more expressive, and 2) composing new, expressive ideas that are both stylistically related to the input whilst providing novel ideas to the user. Our framework is designed, researched and implemented with the objective of ethically providing inspiration for musicians.",
    "zenodo_id": 14877495,
    "dblp_key": null
  },
  {
    "title": "Looking for Tactus in All the Wrong Places: Statistical Inference of Metric Alignment in Rap Flow",
    "author": [
      "Nathaniel Condit-Schultz"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877497",
    "url": "https://doi.org/10.5281/zenodo.14877497",
    "ee": "https://zenodo.org/record/14877497/files/000110.pdf",
    "pages": "989-995",
    "abstract": "Musical rhythm and meter are characterized by simple proportional relationships between event durations within pieces, making comparison of rhythms between different musical pieces a nebulous practice, especially at different tempos. Though the \u201cmain tempo,\u201d or tactus, of a piece serves as an important cognitive reference point, it is difficult to identify objectively. In this paper, I investigate how statistical regularities in rhythmic patterns can be used to determine how to compare pieces at different tempos, speculating that these regularities could relate to the perception of tactus. Using a Bayesian statistical approach, I model first-order (two-gram) rhythmic event transitions in a symbolic dataset of rap transcriptions (MCFlow), allowing the model to renotate the rhythmic values of each transcription as needed to optimize fit. The resulting model predicts makes \u201crenotations\u201d which match a priori predictions from the original dataset\u2019s transcriber. I then demonstrate that the model can be used to rhythmically align new data, giving an objective basis for rhythmic annotation decisions.",
    "zenodo_id": 14877497,
    "dblp_key": null
  },
  {
    "title": "Exploring GPT's Ability as a Judge in Music Understanding",
    "author": [
      "Kun Fang",
      "Ziyu Wang",
      "Gus Xia",
      "Ichiro Fujinaga"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877499",
    "url": "https://doi.org/10.5281/zenodo.14877499",
    "ee": "https://zenodo.org/record/14877499/files/000111.pdf",
    "pages": "996-1003",
    "abstract": "Recent progress in text-based Large Language Models (LLMs) and their extended ability to process multi-modal sensory data have led us to explore their applicability in addressing music information retrieval (MIR) challenges. In this paper, we use a systematic prompt engineering approach for LLMs to solve MIR problems. We convert the music data to symbolic inputs and evaluate LLMs\u2019 ability in detecting annotation errors in three key MIR tasks: beat tracking, chord extraction, and key estimation. A concept augmentation method is proposed to evaluate LLMs\u2019 music reasoning consistency with the provided music concepts in the prompts. Our experiments tested the MIR capabilities of Generative Pre-trained Transformers (GPT). Results show that GPT has an error detection accuracy of 65.20%, 64.80%, and 59.72% in beat tracking, chord extraction, and key estimation tasks, respectively, all exceeding the random baseline. Moreover, we observe a positive correlation between GPT\u2019s error finding accuracy and the amount of concept information provided. The current findings based on symbolic music input provide a solid ground for future LLM-based MIR research.",
    "zenodo_id": 14877499,
    "dblp_key": null
  },
  {
    "title": "Towards Assessing Data Replication in Music Generation With Music Similarity Metrics on Raw Audio",
    "author": [
      "Roser Batlle-Roca",
      "Wei-Hsiang Liao",
      "Xavier Serra",
      "Yuki Mitsufuji",
      "Emilia G\u00f3mez"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877501",
    "url": "https://doi.org/10.5281/zenodo.14877501",
    "ee": "https://zenodo.org/record/14877501/files/000112.pdf",
    "pages": "1004-1011",
    "abstract": "Recent advancements in music generation are raising multiple concerns about the implications of AI in creative music processes, current business models and impacts related to intellectual property management. A relevant discussion and related technical challenge is the potential replication and plagiarism of the training set in AI-generated music, which could lead to misuse of data and intellectual property rights violations. To tackle this issue, we present the Music Replication Assessment (MiRA) tool: a model-independent open evaluation method based on diverse audio music similarity metrics to assess data replication. We evaluate the ability of five metrics to identify exact replication by conducting a controlled replication experiment in different music genres using synthetic samples. Our results show that the proposed methodology can estimate exact data replication with a proportion higher than 10%. By introducing the MiRA tool, we intend to encourage the open evaluation of music-generative models by researchers, developers, and users concerning data replication, highlighting the importance of the ethical, social, legal, and economic consequences. Code and examples are available for reproducibility purposes.",
    "zenodo_id": 14877501,
    "dblp_key": null
  },
  {
    "title": "Generating Sample-Based Musical Instruments Using Neural Audio Codec Language Models",
    "author": [
      "Shahan Nercessian",
      "Johannes Imort",
      "Ninon Devis",
      "Frederik Blang"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877503",
    "url": "https://doi.org/10.5281/zenodo.14877503",
    "ee": "https://zenodo.org/record/14877503/files/000113.pdf",
    "pages": "1012-1019",
    "abstract": "In this paper, we propose and investigate the use of neural audio codec language models for the automatic generation of sample-based musical instruments based on text or reference audio prompts. Our approach extends a generative audio framework to condition on pitch across an 88-key spectrum, velocity, and a combined text/audio embedding. We identify maintaining timbral consistency within the generated instruments as a major challenge. To tackle this issue, we introduce three distinct conditioning schemes. We analyze our methods through objective metrics and human listening tests, demonstrating that our approach can produce compelling musical instruments. Specifically, we introduce a new objective metric to evaluate the timbral consistency of the generated instruments and adapt the average Contrastive Language-Audio Pretraining (CLAP) score for the text-to-instrument case, noting that its naive application is unsuitable for assessing this task. Our findings reveal a complex interplay between timbral consistency, the quality of generated samples, and their correspondence to the input prompt.",
    "zenodo_id": 14877503,
    "dblp_key": null
  },
  {
    "title": "Hierarchical Generative Modeling of Melodic Vocal Contours in Hindustani Classical Music",
    "author": [
      "Nithya Nadig Shikarpur",
      "Krishna Maneesha Dendukuri",
      "Yusong Wu",
      "Antoine Caillon",
      "Cheng-Zhi Anna Huang"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877505",
    "url": "https://doi.org/10.5281/zenodo.14877505",
    "ee": "https://zenodo.org/record/14877505/files/000114.pdf",
    "pages": "1020-1028",
    "abstract": "Hindustani music is a performance-driven oral tradition that exhibits the rendition of rich melodic patterns. In this paper, we focus on generative modeling of singers\u2019 vocal melodies extracted from audio recordings, as the voice is musically prominent within the tradition. Prior generative work in Hindustani music models melodies as coarse discrete symbols which fails to capture the rich expressive melodic intricacies of singing. Thus, we propose to use a finely quantized pitch contour, as an intermediate representation for hierarchical audio modeling. We propose GaMaDHaNi, a modular two-level hierarchy, consisting of a generative model on pitch contours, and a pitch contour to audio synthesis model. We compare our approach to non-hierarchical audio models and hierarchical models that use a self-supervised intermediate representation, through a listening test and qualitative analysis. We also evaluate audio model\u2019s ability to faithfully represent the pitch contour input using Pearson correlation coefficient. By using pitch contours as an intermediate representation, we show that our model may be better equipped to listen and respond to musicians in a human-AI collaborative setting by highlighting two potential interaction use cases (1) primed generation, and (2) coarse pitch conditioning.",
    "zenodo_id": 14877505,
    "dblp_key": null
  },
  {
    "title": "SymPAC: Scalable Symbolic Music Generation With Prompts and Constraints",
    "author": [
      "Haonan Chen",
      "Jordan B. L. Smith",
      "Janne Spijkervet",
      "Ju-Chiang Wang",
      "Pei Zou",
      "Bochen Li",
      "Qiuqiang Kong",
      "Xingjian Du"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877507",
    "url": "https://doi.org/10.5281/zenodo.14877507",
    "ee": "https://zenodo.org/record/14877507/files/000115.pdf",
    "pages": "1029-1036",
    "abstract": "Progress in the task of symbolic music generation may be lagging behind other tasks like audio and text generation, in part because of the scarcity of symbolic training data. In this paper, we leverage the greater scale of audio music data by applying pre-trained MIR models (for transcription, beat tracking, structure analysis, etc.) to extract symbolic events and encode them into token sequences. To the best of our knowledge, this work is the first to demonstrate the feasibility of training symbolic generation models solely from auto-transcribed audio data. Furthermore, to enhance the controllability of the trained model, we introduce SymPAC (Symbolic Music Language Model with Prompting and Constrained Generation), which is distinguished by using (a) prompt bars in encoding and (b) a technique called Constrained Generation via Finite State Machines (FSMs) during inference time. We show the flexibility and controllability of this approach, which may be critical in making music AI useful to creators and users.",
    "zenodo_id": 14877507,
    "dblp_key": null
  },
  {
    "title": "Unsupervised Composable Representations for Audio",
    "author": [
      "Giovanni Bindi",
      "Philippe Esling"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877509",
    "url": "https://doi.org/10.5281/zenodo.14877509",
    "ee": "https://zenodo.org/record/14877509/files/000116.pdf",
    "pages": "1037-1045",
    "abstract": "Current generative models are able to generate high-quality artefacts but have been shown to struggle with compositional reasoning, which can be defined as the ability to generate complex structures from simpler elements. In this paper, we focus on the problem of compositional representation learning for music data, specifically targeting the fully-unsupervised setting. We propose a simple and extensible framework that leverages an explicit compositional inductive bias, defined by a flexible auto-encoding objective that can leverage any of the current state-of-art generative models. We demonstrate that our framework, used with diffusion models, naturally addresses the task of unsupervised audio source separation, showing that our model is able to perform high-quality separation. Our findings reveal that our proposal achieves comparable or superior performance with respect to other blind source separation methods and, furthermore, it even surpasses current state-of-art supervised baselines on signal-to-interference ratio metrics. Additionally, by learning an a-posteriori masking diffusion model in the space of composable representations, we achieve a system capable of seamlessly performing unsupervised source separation, unconditional generation, and variation generation. Finally, as our proposal works in the latent space of pre-trained neural audio codecs, it also provides a lower computational cost with respect to other neural baselines.",
    "zenodo_id": 14877509,
    "dblp_key": null
  },
  {
    "title": "Lyrically Speaking: Exploring the Link Between Lyrical Emotions, Themes and Depression Risk",
    "author": [
      "Pavani B. Chowdary",
      "Bhavyajeet Singh",
      "Rajat Agarwal",
      "Vinoo Alluri"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877511",
    "url": "https://doi.org/10.5281/zenodo.14877511",
    "ee": "https://zenodo.org/record/14877511/files/000117.pdf",
    "pages": "1046-1050",
    "abstract": "Lyrics play a crucial role in affecting and reinforcing emotional states by providing meaning and emotional connotations that interact with the acoustic properties of the music. Specific lyrical themes and emotions may intensify existing negative states in listeners and may lead to undesirable outcomes, especially in listeners with mood disorders such as depression. Hence, it is important for such individuals to be mindful of their listening strategies. In this study, we examine online music consumption of individuals at risk of depression in light of lyrical themes and emotions. Lyrics obtained from the listening histories of 541 Last.fm users, divided into At-Risk and No-Risk based on their mental well-being scores, were analyzed using natural language processing techniques. Statistical analyses of the results revealed that individuals at risk for depression prefer songs with lyrics associated with low valence and low arousal. Additionally, lyrics associated with themes of denial, self-reference and blame were preferred. This study opens up the possibility of an approach to assessing depression risk from the digital footprint of individuals and potentially developing personalized recommendation systems.",
    "zenodo_id": 14877511,
    "dblp_key": null
  },
  {
    "title": "A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond Four Stems",
    "author": [
      "Karn N. Watcharasupat",
      "Alexander Lerch"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877513",
    "url": "https://doi.org/10.5281/zenodo.14877513",
    "ee": "https://zenodo.org/record/14877513/files/000118.pdf",
    "pages": "1051-1059",
    "abstract": "Despite significant recent progress across multiple sub-tasks of audio source separation, few music source separation systems support separation beyond the four-stem vocals, drums, bass, and other (VDBO) setup. Of the very few current systems that support source separation beyond this setup, most continue to rely on an inflexible decoder setup that can only support a fixed pre-defined set of stems. Increasing stem support in these inflexible systems correspondingly requires increasing computational complexity, rendering extensions of these systems computationally infeasible for long-tail instruments. We propose Banquet, a system that allows source separation of multiple stems using just one decoder. A bandsplit source separation model is extended to work in a query-based setup in tandem with a music instrument recognition PaSST model. On the MoisesDB dataset, Banquet \u2014 at only 24.9 M trainable parameters \u2014 performed on par with or better than the significantly more complex 6-stem Hybrid Transformer Demucs. The query-based setup allows for the separation of narrow instrument classes such as clean acoustic guitars, and can be successfully applied to the extraction of less common stems such as reeds and organs.",
    "zenodo_id": 14877513,
    "dblp_key": null
  },
  {
    "title": "In-Depth Performance Analysis of the ADTOF-Based Algorithm for Automatic Drum Transcription",
    "author": [
      "Mickael Zehren",
      "Marco Alunno",
      "Paolo Bientinesi"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877515",
    "url": "https://doi.org/10.5281/zenodo.14877515",
    "ee": "https://zenodo.org/record/14877515/files/000119.pdf",
    "pages": "1060-1067",
    "abstract": "The importance of automatic drum transcription lies in the potential to extract useful information from a musical track; however, the low reliability of the models for this task represents a limiting factor. Indeed, even though in the recent literature the quality of the generated transcription has improved thanks to the curation of large training datasets via crowdsourcing, there is still a large margin of improvement for this task to be considered solved. Aiming to steer the development of future models, we identify the most common errors from training and testing on the aforementioned crowdsourced datasets. We perform this study in three steps: First, we detail the quality of the transcription for each class of interest; second, we employ a new metric and a pseudo confusion matrix to quantify different mistakes in the estimations; last, we compute the agreement between different annotators of the same track to estimate the accuracy of the ground-truth. Our findings are twofold: On the one hand, we observe that the previously reported issue that less represented instruments (e.g., toms) are less reliably transcribed is mostly solved now. On the other hand, cymbal instruments have unprecedented relative low performance. We provide intuitive explanations as to why cymbal instruments are difficult to transcribe and we identify that they represent the main source of disagreement among annotators.",
    "zenodo_id": 14877515,
    "dblp_key": null
  },
  {
    "title": "Towards Musically Informed Evaluation of Piano Transcription Models",
    "author": [
      "Patricia Hu",
      "Luk\u00e1\u0161 Samuel Mart\u00e1k",
      "Carlos Eduardo Cancino-Chac\u00f3n",
      "Gerhard Widmer"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877517",
    "url": "https://doi.org/10.5281/zenodo.14877517",
    "ee": "https://zenodo.org/record/14877517/files/000120.pdf",
    "pages": "1068-1075",
    "abstract": "Automatic piano transcription models are typically evaluated using simple frame- or note-wise information retrieval (IR) metrics. Such benchmark metrics do not provide insights into the transcription quality of specific musical aspects such as articulation, dynamics, or rhythmic precision of the output, which are essential in the context of expressive performance analysis. Furthermore, in recent years, MAESTRO has become the de-facto training and evaluation dataset for such models. However, inference performance has been observed to deteriorate substantially when applied on out-of-distribution data, thereby questioning the suitability and reliability of transcribed outputs from such models for specific MIR tasks. In this work, we investigate the performance of three state-of-the-art piano transcription models in two experiments. In the first one, we propose a variety of musically informed evaluation metrics which, in contrast to the IR metrics, offer more detailed insight into the musical quality of the transcriptions. In the second experiment, we compare inference performance on real-world and perturbed audio recordings, and highlight musical dimensions which our metrics can help explain. Our experimental results highlight the weaknesses of existing piano transcription metrics and contribute to a more musically sound error analysis of transcription outputs.",
    "zenodo_id": 14877517,
    "dblp_key": null
  },
  {
    "title": "Using Item Response Theory to Aggregate Music Annotation Results of Multiple Annotators",
    "author": [
      "Tomoyasu Nakano",
      "Masataka Goto"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877519",
    "url": "https://doi.org/10.5281/zenodo.14877519",
    "ee": "https://zenodo.org/record/14877519/files/000121.pdf",
    "pages": "1076-1084",
    "abstract": "Human music annotation is one of the most important tasks in music information retrieval (MIR) research. Results of labeling, tagging, assessment, and evaluation can be used as training data for machine learning models that estimate them automatically. For such machine learning purposes, a single target (e.g., song) is usually annotated by multiple human annotators, and the results are aggregated by majority voting or averaging. Majority voting, however, requires the number of annotators to be an odd number, which is not always possible. And averaging is sensitive to differences in the judgmental characteristics of each annotator and cannot be used for ordinal scales. This paper therefore proposes that the item response theory (IRT) be used to aggregate the music annotation results of multiple annotators. IRT-based models can jointly estimate annotators\u2019 characteristics and latent scores (i.e., aggregations of annotation results) of the targets, and they are also applicable to ordinal scales. We evaluated the IRT-based models in two actual cases of music annotation \u2014 semantic tagging of music and Likert scale-based evaluation of singing skill \u2014 and compared those models with their simplified models that do not consider the characteristics of each annotator.",
    "zenodo_id": 14877519,
    "dblp_key": null
  },
  {
    "title": "Just Label the Repeats for In-the-Wild Audio-to-Score Alignment",
    "author": [
      "Irmak Bukey",
      "Michael Feffer",
      "Chris Donahue"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877521",
    "url": "https://doi.org/10.5281/zenodo.14877521",
    "ee": "https://zenodo.org/record/14877521/files/000122.pdf",
    "pages": "1085-1092",
    "abstract": "We propose an efficient workflow for high-quality offline alignment of in-the-wild performance audio and corresponding sheet music scans (images).1 Recent work on audio-to-score alignment extends dynamic time warping (DTW) to be theoretically able to handle jumps in sheet music induced by repeat signs\u2014this method requires no human annotations, but we show that it often yields low-quality alignments. As an alternative, we propose a workflow and interface that allows users to quickly annotate jumps (by clicking on repeat signs), requiring a small amount of human supervision but yielding much higher quality alignments on average. Additionally, we refine audio and score feature representations to improve alignment quality by: (1) integrating measure detection into the score feature representation, and (2) using raw onset prediction probabilities from a music transcription model instead of piano roll. We propose an evaluation protocol for audio-to-score alignment that computes the distance between the estimated and ground truth alignment in units of measures. Under this evaluation, we find that our proposed jump annotation workflow and improved feature representations together improve alignment accuracy by 150% relative to prior work (33% \u2192 82%).",
    "zenodo_id": 14877521,
    "dblp_key": null
  },
  {
    "title": "Investigating Time-Line-Based Music Traditions With Field Recordings: A Case Study of Candombl\u00e9 Bell Patterns",
    "author": [
      "Lucas S. Maia",
      "Richa Namballa",
      "Mart\u00edn Rocamora",
      "Magdalena Fuentes",
      "Carlos Guedes"
    ],
    "year": "2024",
    "doi": "10.5281/zenodo.14877523",
    "url": "https://doi.org/10.5281/zenodo.14877523",
    "ee": "https://zenodo.org/record/14877523/files/000123.pdf",
    "pages": "1093-1100",
    "abstract": "We introduce a series of transdisciplinary corpus studies aimed at investigating cross-cultural trends in time-line-based music traditions. Our analyses concentrate on a compilation of field recordings from the Centre de Recherche en Ethnomusicologie (CREM) sound archive. To demonstrate the value of an interdisciplinary approach combining ethnomusicology and music information research to rhythmic analysis, we propose a case study on the bell patterns used in the musical practices of Candombl\u00e9, an Afro-Brazilian religion. After removing vocals from the recordings with a deep learning source separation technique, we further process the instrumental segments using non-negative matrix factorization and select the bell components. Then, we compute a tempo-agnostic rhythmic feature from the bell track and use it to cluster the data. Finally, we use synthesized patterns from the musicological literature about Candombl\u00e9 as references to propagate labels to the rhythmic clusters in our data. This semi-supervised approach to pattern analysis precludes the need for downbeat and cycle annotations, making it particularly suited for extensive archive investigations. Lastly, by comparing bell patterns in Candombl\u00e9 and a West African music tradition, we lay the foundation for our future crosscultural research and observe the potential application of this methodology to other time-line-based music.",
    "zenodo_id": 14877523,
    "dblp_key": null
  }
]