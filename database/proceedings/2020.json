[
  {
    "title": "Music Structure Analysis Based on an LSTM-HSMM Hybrid Model",
    "author": [
      " Go Shibata",
      " Ryo  Nishikimi",
      " Kazuyoshi  Yoshii"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245362",
    "url": "https://doi.org/10.5281/zenodo.4245362",
    "ee": "http://archives.ismir.net/ismir2020/paper/000005.pdf",
    "pages": "23-29",
    "zenodo_id": 4245362,
    "dblp_key": null,
    "Abstract": "This paper describes a statistical music structure analysis method that splits an audio signal of popular music into musically meaningful sections at the beat level and classifies them into predefined categories such as intro, verse, and chorus, where beat times are assumed to be estimated in advance. A basic approach to this task is to train a recurrent neural network (e.g., long short-term memory (LSTM) network) that directly predicts section labels from acoustic features. This approach, however, suffers from frequent musically unnatural label switching because the homogeneity, repetitiveness, and duration regularity of musical sections are hard to represent explicitly in the network architecture. To solve this problem, we formulate a unified hidden semi-Markov model (HSMM) that represents the generative process of homogeneous mel-frequency cepstrum coefficients, repetitive chroma features, and mel spectra from section labels, where the emission probabilities of mel spectra are computed from the posterior probabilities of section labels predicted by an LSTM. Given these acoustic features, the most likely label sequence can be estimated with Viterbi decoding. The experimental results show that the proposed LSTM-HSMM hybrid model outperformed a conventional HSMM.",
    "extra": {
      "takeaway": "This paper proposes a statistical approach to music structure analysis based on an LSTM-HSMM hybrid model."
    }
  },
  {
    "title": "Multi-Instrument Music Transcription Based on Deep Spherical Clustering of Spectrograms and Pitchgrams",
    "author": [
      " Keitaro Tanaka",
      " Takayuki  Nakatsuka",
      " Ryo  Nishikimi",
      " Kazuyoshi  Yoshii",
      " Shigeo  Morishima"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245436",
    "url": "https://doi.org/10.5281/zenodo.4245436",
    "ee": "http://archives.ismir.net/ismir2020/paper/000006.pdf",
    "pages": "327-334",
    "zenodo_id": 4245436,
    "dblp_key": null,
    "Abstract": "This paper describes a clustering-based music transcription method that estimates the piano rolls of arbitrary musical instrument parts from multi-instrument polyphonic music signals. If target musical pieces are always played by particular kinds of musical instruments, a way to obtain piano rolls is to compute the pitchgram (pitch saliency spectrogram) of each musical instrument by using a deep neural network (DNN). However, this approach has a critical limitation that it has no way to deal with musical pieces including undefined musical instruments. To overcome this limitation, we estimate a condensed pitchgram with an existing instrument-independent neural multi-pitch estimator and then separate the pitchgram into a specified number of musical instrument parts with a deep spherical clustering technique. To improve the performance of transcription, we propose a joint spectrogram and pitchgram clustering method based on the timbral and pitch characteristics of musical instruments. The experimental results show that the proposed method can transcribe musical pieces including unknown musical instruments as well as those containing only predefined instruments, at the state-of-the-art transcription accuracy.",
    "extra": {
      "takeaway": "We propose a clustering-based music transcription method to estimate the piano rolls of arbitrary musical instrument parts."
    }
  },
  {
    "title": "Improving Polyphonic Music Models with Feature-rich Encoding",
    "author": [
      " Omar A Peracha"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245396",
    "url": "https://doi.org/10.5281/zenodo.4245396",
    "ee": "http://archives.ismir.net/ismir2020/paper/000012.pdf",
    "pages": "169-175",
    "zenodo_id": 4245396,
    "dblp_key": null,
    "Abstract": "This paper explores sequential modelling of polyphonic music with deep neural networks. While recent breakthroughs have focussed on network architecture, we demonstrate that the representation of the sequence can make an equally significant contribution to the performance of the model as measured by validation set loss. By extracting salient features inherent to the training dataset, the model can either be conditioned on these features or trained to predict said features as extra components of the sequences being modelled. We show that training a neural network to predict a seemingly more complex sequence, with extra features included in the series being modelled, can improve overall model performance significantly. We first introduce TonicNet, a GRU-based model trained to initially predict the chord at a given time-step before then predicting the notes of each voice at that time-step, in contrast with the typical approach of predicting only the notes. We then evaluate TonicNet on the canonical JSB Chorales dataset and obtain state-of-the-art results.",
    "extra": {
      "takeaway": "Better feature representations allow for state-of-the-art results in modelling symbolic music, with smaller neural networks"
    }
  },
  {
    "title": "Practical Evaluation of Repeated Recommendations in Personalized Music Discovery",
    "author": [
      " Brian Manolovitz",
      " Mitsunori  Ogihara"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245516",
    "url": "https://doi.org/10.5281/zenodo.4245516",
    "ee": "http://archives.ismir.net/ismir2020/paper/000013.pdf",
    "pages": "633-639",
    "zenodo_id": 4245516,
    "dblp_key": null,
    "Abstract": "Studies have shown that repeated exposures to novel songs cause an increase in a persons memory and liking. These studies are commonly verified through self-reporting emotion-based surveys. This paper proposes the \"retention rate\" as an additional parameter for evaluation. The \"retention rate\" is one at which the listener revisits the novel items. The author hypothesize that when a person listens to novel (i.e., both unfamiliar and interesting) pieces of music, the retention rate will be proportional to the number of times the discovery engine suggests the pieces to her, as long as they remain novel. The author have tested the hypothesis through a six-week human-subject experiment that simulates a real-world listening environment and a follow-up survey. During the experiment period, each subject received, through Discover Weekly in Spotify, suggestions for novel songs up to three times and provided evaluation. One month after the evaluation experiment, the human-subjects answered whether they had revisited the novel songs. Through the analysis of the response and survey data, the researchers conclude that the more times a listener is exposed to a song during the discovery process, the more likely she is to return to the song.",
    "extra": {
      "takeaway": "Repeated presentations of novel songs increases the likelihood for listener to revisit"
    }
  },
  {
    "title": "Modeling and Estimating Local Tempo: A Case Study on Chopin's Mazurkas",
    "author": [
      " Hendrik Schreiber",
      " Frank  Zalkow",
      " Meinard  M\u00FCller"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245546",
    "url": "https://doi.org/10.5281/zenodo.4245546",
    "ee": "http://archives.ismir.net/ismir2020/paper/000014.pdf",
    "pages": "773-779",
    "zenodo_id": 4245546,
    "dblp_key": null,
    "Abstract": "Even though local tempo estimation promises musicological insights into expressive musical performances, it has never received as much attention in the music information retrieval (MIR) research community as either beat tracking or global tempo estimation. One reason for this may be the lack of a generally accepted definition. In this paper, we discuss how to model and measure local tempo in a musically meaningful way using a cross-version dataset of Fr\u00E9d\u00E9ric Chopin's Mazurkas as a use case. In particular, we explore how tempo stability can be measured and taken into account during evaluation. Comparing existing and newly trained systems, we find that CNN-based approaches can accurately measure local tempo even for expressive classical music, if trained on the target genre. Furthermore, we show that different training-test splits have a considerable impact on accuracy for difficult segments.",
    "extra": {
      "takeaway": "How to model and measure local tempo for highly expressive music"
    }
  },
  {
    "title": "Classifying Leitmotifs in Recordings of Operas by Richard Wagner",
    "author": [
      " Michael Krause",
      " Frank  Zalkow",
      " Christof  Weiss",
      " Meinard  M\u00FCller"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245472",
    "url": "https://doi.org/10.5281/zenodo.4245472",
    "ee": "http://archives.ismir.net/ismir2020/paper/000019.pdf",
    "pages": "473-480",
    "zenodo_id": 4245472,
    "dblp_key": null,
    "Abstract": "From the 19th century on, several composers of Western opera made use of leitmotifs (short musical ideas referring to semantic entities such as characters, places, items, or feelings) for guiding the audience through the plot and illustrating the events on stage. A prime example of this compositional technique is Richard Wagner's four-opera cycle Der Ring des Nibelungen. Across its different occurrences in the score, a leitmotif may undergo considerable musical variations. The concrete leitmotif instances in an audio recording are subject to acoustic variability. Our paper approaches the task of classifying such leitmotif instances in audio recordings. As our main contribution, we conduct a case study on a dataset covering 16 recorded performances of the Ring with annotations of ten central leitmotifs, leading to 2403 occurrences and 38448 instances in total. We build a neural network classification model and evaluate its ability to generalize across different performances and leitmotif occurrences. Our findings demonstrate the possibilities and limitations of leitmotif classification in audio recordings and pave the way towards the fully automated detection of leitmotifs in music recordings.",
    "extra": {
      "takeaway": "We introduce the problem of classifying leitmotifs in audio recordings by means of a case study on R. Wagner's Ring cycle."
    }
  },
  {
    "title": "Composer Style Classification of Piano Sheet Music Images Using Language Model Pretraining",
    "author": [
      " Timothy Tsai",
      " Kevin  Ji"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245398",
    "url": "https://doi.org/10.5281/zenodo.4245398",
    "ee": "http://archives.ismir.net/ismir2020/paper/000021.pdf",
    "pages": "176-183",
    "zenodo_id": 4245398,
    "dblp_key": null,
    "Abstract": "This paper studies composer style classification of piano sheet music images.  Previous approaches to the composer classification task have been limited by a scarcity of data.  We address this issue in two ways: (1) we recast the problem to be based on raw sheet music images rather than a symbolic music format, and (2) we propose an approach that can be trained on unlabeled data.  Our approach first converts the sheet music image into a sequence of musical ``words\" based on the bootleg feature representation, and then feeds the sequence into a text classifier.  We show that it is possible to significantly improve classifier performance by first training a language model on a set of unlabeled data, initializing the classifier with the pretrained language model weights, and then finetuning the classifier on a small amount of labeled data.  We train AWD-LSTM, GPT-2, and RoBERTa language models on all piano sheet music images in IMSLP.  We find that transformer-based architectures outperform CNN and LSTM models, and pretraining boosts classification accuracy for the GPT-2 model from 46% to 70% on a 9-way classification task.  The trained model can also be used as a feature extractor that projects piano sheet music into a feature space that characterizes compositional style. ",
    "extra": {
      "takeaway": "We classify the composer of a piano sheet music image using a language model pretraining approach."
    }
  },
  {
    "title": "Using Weakly Aligned Score-Audio Pairs to Train Deep Chroma Models for Cross-Modal Music Retrieval",
    "author": [
      " Frank Zalkow",
      " Meinard  M\u00FCller"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245400",
    "url": "https://doi.org/10.5281/zenodo.4245400",
    "ee": "http://archives.ismir.net/ismir2020/paper/000023.pdf",
    "pages": "184-191",
    "zenodo_id": 4245400,
    "dblp_key": null,
    "Abstract": "Many music information retrieval tasks involve the comparison of a symbolic score representation with an audio recording. A typical strategy is to compare score-audio pairs based on a common mid-level representation, such as chroma features. Several recent studies demonstrated the effectiveness of deep learning models that learn task-specific mid-level representations from temporally aligned training pairs. However, in practice, there is often a lack of strongly aligned training data, in particular for real-world scenarios. In our study, we use weakly aligned score-audio pairs for training, where only the beginning and end of a score excerpt is annotated in an audio recording, without aligned correspondences in between. To exploit such weakly aligned data, we employ the Connectionist Temporal Classification (CTC) loss to train a deep learning model for computing an enhanced chroma representation. We then apply this model to a cross-modal retrieval task, where we aim at finding relevant audio recordings of Western classical music, given a short monophonic musical theme in symbolic notation as a query. We present systematic experiments that show the effectiveness of the CTC-based model for this theme-based retrieval task.",
    "extra": {
      "takeaway": "We train a deep salience model with weakly aligned data to compute enhanced chroma features for cross-modal retrieval."
    }
  },
  {
    "title": "Camera-Based Piano Sheet Music Identification",
    "author": [
      " Daniel Yang",
      " Timothy  Tsai"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245476",
    "url": "https://doi.org/10.5281/zenodo.4245476",
    "ee": "http://archives.ismir.net/ismir2020/paper/000024.pdf",
    "pages": "481-488",
    "zenodo_id": 4245476,
    "dblp_key": null,
    "Abstract": "This paper presents a method for large-scale retrieval of piano sheet music images.  Our work differs from previous studies on sheet music retrieval in two ways.  First, we investigate the problem at a much larger scale than previous studies, using all solo piano sheet music images in the entire IMSLP dataset as a searchable database.  Second, we use cell phone images of sheet music as our input queries, which lends itself to a practical, user-facing application.  We show that a previously proposed fingerprinting method for sheet music retrieval is far too slow for a real-time application, and we diagnose its shortcomings.  We propose a novel hashing scheme called dynamic n-gram fingerprinting that significantly reduces runtime while simultaneously boosting retrieval accuracy.  In experiments on IMSLP data, our proposed method achieves a mean reciprocal rank of 0.85 and an average runtime of 0.98 seconds per query. ",
    "extra": {
      "takeaway": "Given a cell phone picture of piano sheet music, we present a way to identify the piece by searching all IMSLP piano scores."
    }
  },
  {
    "title": "User Perceptions Underlying  Social Music Behavior",
    "author": [
      " Louis Spinelli",
      " Josephine  Lau",
      " Jin Ha  Lee"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245474",
    "url": "https://doi.org/10.5281/zenodo.4245474",
    "ee": "http://archives.ismir.net/ismir2020/paper/000025.pdf",
    "pages": "489-496",
    "zenodo_id": 4245474,
    "dblp_key": null,
    "Abstract": "While prior studies investigating the social aspects of music provide a landscape of users' various social behaviors around commercial music services (CMS), there remains a lack in understanding of users' perceptions and value judgments underlying these behaviors. Specifically, there is more to learn about what influences and behaviors individual music users perceive as meaningful in social contexts. We used the Q methodology to explore which behaviors and influences are important to CMS users and why. We extracted two factors that explain the two different viewpoints shared by groups of music users, focusing on how they perceive the meaning and value of different social music behavior and interactions. From these findings, we then revise an existing social music coding dictionary and interaction model and offer new CMS design insights.",
    "extra": {
      "takeaway": "We used the Q methodology to explore which behaviors and influences are important to CMS users and why."
    }
  },
  {
    "title": "Perceptual vs. automated judgements of music copyright infringement",
    "author": [
      " Yuchen Yuan",
      " Sho  Oishi",
      " Charles  Cronin",
      " Daniel  M\u00FCllensiefen",
      " Quentin  Atkinson",
      " Shinya  Fujii",
      " Patrick E.  Savage"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245364",
    "url": "https://doi.org/10.5281/zenodo.4245364",
    "ee": "http://archives.ismir.net/ismir2020/paper/000028.pdf",
    "pages": "23-29",
    "zenodo_id": 4245364,
    "dblp_key": null,
    "Abstract": "Music copyright lawsuits often result in multimillion dollar damage awards or settlements, yet there are few objective guidelines for applying copyright law in in-fringement claims involving musical works. Recent re-search has attempted to develop objective methods based on automated similarity algorithms, but there remains almost no data on the role of perceived similarity in mu-sic copyright decisions despite its crucial role in copy-right law. We collected perceptual data from 20 partici-pants for 17 adjudicated copyright cases from the USA and Japan after editing the disputed sections to contain either full audio, melody only, or lyrics only. Due to the historical emphasis in legal opinions on melody as the key criterion for deciding infringement, we predicted that listening to melody-only versions would result in percep-tual judgements that more closely matched actual past legal decisions. Surprisingly, however, we found no sig-nificant differences between the three conditions, with participants matching past decisions in between 50-60% of cases in all three conditions. Automated algorithms designed to calculate melodic and audio similarity pro-duced comparable results: both algorithms were able to match past decisions with identical accuracy of 71% (12/17 cases). Analysis of cases that were difficult to classify suggests that melody, lyrics, and other factors sometimes interact in complex ways difficult to capture using quantitative metrics. We propose directions for fur-ther investigation of the role of similarity in music copy-right law using larger and more diverse samples of cases and enhanced methods, and adapting our perceptual ex-periment method to avoid relying for ground truth data only on court decisions (which may be subject to selec-tion bias). Our results contribute to important practical debates, such as whether jury members should be allowed to listen to full audio recordings during copyright cases. ",
    "extra": {
      "takeaway": "Melody-based and audio-based perceptual and automated methods achieved similar accuracies for music copyright infringement."
    }
  },
  {
    "title": "SuPP & MaPP: Adaptable Structure-Based Representations for MIR Tasks",
    "author": [
      " Claire Savard",
      " Erin H.  Bugbee",
      " Melissa R.  McGuirl",
      " Katherine M.  Kinnaird"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245438",
    "url": "https://doi.org/10.5281/zenodo.4245438",
    "ee": "http://archives.ismir.net/ismir2020/paper/000033.pdf",
    "pages": "335-342",
    "zenodo_id": 4245438,
    "dblp_key": null,
    "Abstract": "Accurate and flexible representations of music data are paramount to addressing MIR tasks, yet many of the existing approaches are difficult to interpret or rigid in nature. This work introduces two new song representations for structure-based retrieval methods: Surface Pattern Preservation (SuPP), a continuous song representation, and Matrix Pattern Preservation (MaPP), SuPP's discrete counterpart. These representations come equipped with several user-defined parameters so that they are adaptable for a range of MIR tasks. Experimental results show MaPP as successful in addressing the cover song task on a set of Mazurka scores, with a mean precision of 0.965 and recall of 0.776. SuPP and MaPP also show promise in other MIR applications, such as novel-segment detection and genre classification, the latter of which demonstrates their suitability as inputs for machine learning problems.",
    "extra": {
      "takeaway": "SuPP and MaPP are new structure-based musical representations applicable to many MIR tasks."
    }
  },
  {
    "title": "Automatic Figured Bass Annotation Using the New Bach Chorales Figured Bass Dataset",
    "author": [
      " Yaolong Ju",
      " Sylvain  Margot",
      " Cory  McKay",
      " Luke  Dahn",
      " Ichiro  Fujinaga"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245512",
    "url": "https://doi.org/10.5281/zenodo.4245512",
    "ee": "http://archives.ismir.net/ismir2020/paper/000040.pdf",
    "pages": "640-646",
    "zenodo_id": 4245512,
    "dblp_key": null,
    "Abstract": "This paper focuses on the computational study of figured bass, which remains an under-researched topic in MIR, likely due to a lack of machine-readable datasets. First, we introduce the Bach Chorales Figured Bass dataset (BCFB), a collection of 139 chorales composed by Johann Sebastian Bach that includes both the original music and figured bass annotations encoded in MusicXML, **kern, and MEI formats. We also present a comparative study on automatic figured bass annotation using both rule-based and machine learning approaches, which respectively achieved classification accuracies of 85.3% and 85.9% on BCFB. Finally, we discuss promising areas for MIR research involving figured bass, including automatic harmonic analysis. ",
    "extra": {
      "takeaway": "We facilitate figured bass research by presenting a new figured bass dataset and conducting automatic figured bass annotation"
    }
  },
  {
    "title": "A corpus-based analysis of syncopated patterns in ragtime",
    "author": [
      " Phillip B. Kirlin"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245514",
    "url": "https://doi.org/10.5281/zenodo.4245514",
    "ee": "http://archives.ismir.net/ismir2020/paper/000041.pdf",
    "pages": "647-653",
    "zenodo_id": 4245514,
    "dblp_key": null,
    "Abstract": "In this paper, we build on and extend a number of previous studies of rhythmic patterns that occur in ragtime music.  All of these studies have used the RAG-C dataset of approximately 11,000 symbolically-encoded ragtime pieces to identify salient rhythmic patterns in the corpus and  qualify how they are used.  Ragtime music is distinguished from other musical genres by frequent use of syncopation, and previous computational studies have confirmed a number of musicological hypotheses regarding the use of syncopated patterns in ragtime compositions.  In this work, we extend these studies to investigate further questions involving the use of syncopation.  Specifically, we introduce a new methodological framework for processing the RAG-C dataset and confirm that experiments from previous studies obtain similar results using the new methodology.  We investigate the use of the common ``short-long-short'' syncopated pattern in different time periods and present new results detailing its use by three well-known ragtime composers.  We describe how the use of other syncopated patterns has evolved over time and the different distributions of patterns that result from those changes.  Lastly, we present novel results identifying statistically significant patterns in the way composers varied the amount of syncopation in consecutive measures in compositions.",
    "extra": {
      "takeaway": "We confirm, quantify, and present novel findings regarding the use of syncopation in ragtime music."
    }
  },
  {
    "title": "A Method for Analysis of Shared Structure in Large Music Collections using Techniques from Genetic Sequencing and Graph Theory",
    "author": [
      " Florian Thalmann",
      " Kazuyoshi  Yoshii",
      " Thomas  Wilmering",
      " Wiggins  Geraint",
      " Mark B.  Sandler"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245440",
    "url": "https://doi.org/10.5281/zenodo.4245440",
    "ee": "http://archives.ismir.net/ismir2020/paper/000044.pdf",
    "pages": "343-350",
    "zenodo_id": 4245440,
    "dblp_key": null,
    "Abstract": "While common approaches to automatic structural analysis of music typically focus on individual audio files, our approach collates audio features of large sets of related files in order to find a shared musical temporal structure. The content of each individual file and the differences between them can then be described in relation to this shared structure. We first construct a large similarity graph of temporal segments, such as beats or bars, based on self-alignments and selected pair-wise alignments between the given input files. Part of this graph is then partitioned into groups of corresponding segments using multiple sequence alignment. This partitioned graph is searched for recurring sections which can be organized hierarchically based on their co-occurrence. We apply our approach to discover shared harmonic structure in a dataset containing a large number of different live performances of a number of songs. Our evaluation shows that using the joint information from a number of files has the advantage of evening out the noisiness or inaccuracy of the underlying feature data and leads to a robust estimate of shared musical material.",
    "extra": {
      "takeaway": "We propose a method for identifying both shared and individual temporal structure of files in large audio collections."
    }
  },
  {
    "title": "Investigating U-Nets with various Intermediate Blocks for Spectrogram-based Singing Voice Separation",
    "author": [
      " Woosung Choi",
      " Minseok   Kim",
      " Jaehwa  Chung",
      " Daewon  Lee",
      " Soonyoung  Jung"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245404 \t",
    "url": "https://doi.org/10.5281/zenodo.4245404 \t",
    "ee": "http://archives.ismir.net/ismir2020/paper/000046.pdf",
    "pages": "192-198",
    "zenodo_id": 4245404,
    "dblp_key": null,
    "Abstract": "Singing Voice Separation (SVS) tries to separate singing voice from a given mixed musical signal. Recently, many U-Net-based models have been proposed for the SVS task, but there were no existing works that evaluate and compare various types of intermediate blocks that can be used in the U-Net architecture. In this paper, we introduce a variety of intermediate spectrogram transformation blocks. We implement U-nets based on these blocks and train them on complex-valued spectrograms to consider both magnitude and phase. These networks are then compared on the SDR metric. When using a particular block composed of convolutional and fully-connected layers, it achieves state-of-the-art SDR on the MUSDB singing voice separation task by a large margin of 0.9 dB. Our code and models are available online.",
    "extra": {
      "takeaway": "We compare various types of intermediate blocks that can be used in the U-Net architecture for singing voice separation."
    }
  },
  {
    "title": "The Rhythmic Dictator: Does Gamification of Rhythm Dictation Exercises Help?",
    "author": [
      " Matev\u017D Pesek",
      " Lovro  Suhadolnik",
      " Peter  \u0160avli",
      " Matija  Marolt"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245478",
    "url": "https://doi.org/10.5281/zenodo.4245478",
    "ee": "http://archives.ismir.net/ismir2020/paper/000048.pdf",
    "pages": "497-503",
    "zenodo_id": 4245478,
    "dblp_key": null,
    "Abstract": "We present the development and evaluation of a gamified rhythmic dictation application for music theory learning. The application's focus is on mobile accessibility and user experience, so it includes intuitive controls for input of rhythmic exercises, a responsive user interface, several gamification elements and a flexible exercise generator. We evaluated the rhythmic dictation application with conservatory-level music theory students through A/B testing, to assess their engagement and performance. The results show a significant impact of the application on the students' exam scores. ",
    "extra": {
      "takeaway": "An open-source ear training application, which increases students' performance in conventional exams."
    }
  },
  {
    "title": "Learning to Read and Follow Music in Complete Score Sheet Images",
    "author": [
      " Florian Henkel",
      " Rainer  Kelz",
      " Gerhard  Widmer"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245550",
    "url": "https://doi.org/10.5281/zenodo.4245550",
    "ee": "http://archives.ismir.net/ismir2020/paper/000049.pdf",
    "pages": "780-787",
    "zenodo_id": 4245550,
    "dblp_key": null,
    "Abstract": "This paper addresses the task of score following in sheet music given as unprocessed images. While existing work either relies on OMR software to obtain a computer-readable score representation, or crucially relies on prepared sheet image excerpts, we propose the first system that directly performs score following in full-page, completely unprocessed sheet images.  Based on incoming audio and a given image of the score, our system directly predicts the most likely position within the page that matches the audio, outperforming current state-of-the-art image-based score followers in terms of alignment precision. We also compare our method to an OMR-based approach and empirically show that it can be a viable alternative to such a system.",
    "extra": {
      "takeaway": "Score following can be directly applied to complete sheet images in an end-to-end fashion using machine learning techniques."
    }
  },
  {
    "title": "Learning to Denoise Historical Music",
    "author": [
      " Yunpeng Li",
      " Marco  Tagliasacchi",
      " Beat  Gfeller",
      " Dominik  Roblek"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245480",
    "url": "https://doi.org/10.5281/zenodo.4245480",
    "ee": "http://archives.ismir.net/ismir2020/paper/000057.pdf",
    "pages": "504-511",
    "zenodo_id": 4245480,
    "dblp_key": null,
    "Abstract": "We propose an audio-to-audio generative model that learns to denoise old music recordings. Our model internally converts its input into a time-frequency representation by means of a short-time Fourier transform (STFT), and processes the resulting complex spectrogram using a convolutional neural network. The network is trained with both reconstruction and adversarial objectives on a synthetic noisy music dataset, which is created by mixing clean music with real noise samples extracted from quiet segments of old recordings. We evaluate our method quantitatively on held-out test examples of the synthetic dataset, and qualitatively by human rating on samples of actual historical recordings. Our results show that the proposed method is effective in removing noise, while preserving the quality and details of the original music.",
    "extra": {
      "takeaway": "We present a learning-based approach to historical music restoration, which removes noise while preserving musical details. "
    }
  },
  {
    "title": "Discourse not Dualism: An Interdisciplinary Dialogue on Sonata Form in Beethoven's Early Piano Sonatas",
    "author": [
      " Christof Weiss",
      " Stephanie  Klauk",
      " Mark R H  Gotham",
      " Meinard  M\u00FCller",
      " Rainer  Kleinertz"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245402",
    "url": "https://doi.org/10.5281/zenodo.4245402",
    "ee": "http://archives.ismir.net/ismir2020/paper/000058.pdf",
    "pages": "199-206",
    "zenodo_id": 4245402,
    "dblp_key": null,
    "Abstract": "The computational analysis of music has traditionally seen a sharp divide between the \"audio approach\" relying on signal processing and the \"symbolic approach\" based on scores. Likewise, there has also been an unfortunate gap between any such computational endeavour and more traditional approaches as used in historical musicology. In this paper, we take a step towards ameliorating this situation through the application of a computational method for visualizing local key characteristics in audio recordings. We exploit these visualizations of diatonic scale content by discussing their musicological implications, being aware of methodological limitations as for the case of minor keys. As a proof of concept, we use this method for investigating differences between the traditional sonata-form model and selected Beethoven piano sonatas in the context of sonata theory from the end of the 18th century. We consider this scenario as an example for a rewarding dialogue between computer science and historical musicology.",
    "extra": {
      "takeaway": "We report on the application of an audio-based method to address a long-standing debate over formal structure in music."
    }
  },
  {
    "title": "Play Music: User Motivations and Expectations for Non-Specific Voice Queries",
    "author": [
      " Jennifer Thom",
      " Angela  Nazarian",
      " Ruth  Brillman",
      " Henriette  Cramer",
      " Sarah  Mennicken"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245524",
    "url": "https://doi.org/10.5281/zenodo.4245524",
    "ee": "http://archives.ismir.net/ismir2020/paper/000060.pdf",
    "pages": "654-661",
    "zenodo_id": 4245524,
    "dblp_key": null,
    "Abstract": "The growing market of voice-enabled devices introduces new types of music search requests that can be more ambiguous than in typed search interfaces as voice assistants can potentially support conversational requests. However, these systems may not be able to fulfill ambiguous requests in a manner that matches the user need.  In this work, we study an example of ambiguous requests which we term as non-specific queries (NSQs), such as \"play music,\" where users ask to stream content using a single utterance that does not specify what content they want to hear.  To better understand user motivations for making NSQs, we conducted semi-structured qualitative interviews with voice users. We observed four themes that structure user perceptions of the benefits and shortcomings of making NSQs: the tradeoff between control and convenience, varying expectations for personalization, the effects of context on expectations, and learned user behaviors. We conclude with implications for how these themes can inform the interaction design of voice search systems in handling non-specific music requests in voice search systems. ",
    "extra": {
      "takeaway": "We conducted a qualitative study on user motivations to make ambiguous, non-specific voice queries such as \"play music.\""
    }
  },
  {
    "title": "Measuring disruption in song similarity networks",
    "author": [
      " Felipe V Falc\u00E3o",
      " Nazareno  Andrade",
      " Flavio  Figueiredo",
      " Diego  Furtado Silva",
      " Fabio  Morais"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245356",
    "url": "https://doi.org/10.5281/zenodo.4245356",
    "ee": "http://archives.ismir.net/ismir2020/paper/000061.pdf",
    "pages": "30-37",
    "zenodo_id": 4245356,
    "dblp_key": null,
    "Abstract": "Investigating music with a focus on the similarity relations between songs, albums, and artists plays an important role when trying to understand trends in the history of music genres. In particular, representing these relations as a similarity network allows us to investigate the innovation presented by these entities in a multitude of points-of-view, including disruption. A disruptive object is one that creates a new stream of events, changing the traditional way of how a context usually works. The proper measurement of disruption remains as a task with large room for improvement, and these gaps are even more evident in the music domain, where the topic has not received much attention so far. This work builds on preliminary studies focused on the analysis of music disruption derived from metadata-based similarity networks, demonstrating that the raw audio can augment similarity information. We developed a case study based on a collection of a Brazilian local music tradition called Forr??, that emphasizes the analytical and musicological potential of the musical disruption metric to describe and explain a genre trajectory over time.",
    "extra": {
      "takeaway": "The present work tries to address the lack of efforts on describing the innovation of specific songs over time."
    }
  },
  {
    "title": "Uncovering audio patterns in music with Nonnegative Tucker Decomposition for structural segmentation",
    "author": [
      " Axel Marmoret",
      " Jeremy  Cohen",
      " Fr\u00E9d\u00E9ric  Bimbot",
      " Nancy  Bertin"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245552",
    "url": "https://doi.org/10.5281/zenodo.4245552",
    "ee": "http://archives.ismir.net/ismir2020/paper/000078.pdf",
    "pages": "788-794",
    "zenodo_id": 4245552,
    "dblp_key": null,
    "Abstract": "Recent work has proposed the use of tensor decomposition to model repetitions and to separate tracks in loop-based electronic music. The present work investigates further on the ability of Nonnegative Tucker Decompositon (NTD) to uncover musical patterns and structure in pop songs in their audio form. Exploiting the fact that NTD tends to express the content of bars as linear combinations of a few patterns, we illustrate the ability of the decomposition to capture and single out repeated motifs in the corresponding compressed space, which can be interpreted from a musical viewpoint. The resulting features also turn out to be efficient for structural segmentation, leading to experimental results on the RWC Pop data set which are potentially challenging state-of-the-art approaches that rely on extensive example-based learning schemes.",
    "extra": {
      "takeaway": "Tensor Decomposition is a promising tool for sparsifying autosimilarity matrix, revealing underlying structure of music."
    }
  },
  {
    "title": "The Jazz Harmony Treebank",
    "author": [
      " Daniel Harasim",
      " Christoph  Finkensiep",
      " Petter  Ericson",
      " Timothy J.  O'Donnell",
      " Martin  Rohrmeier"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245406",
    "url": "https://doi.org/10.5281/zenodo.4245406",
    "ee": "http://archives.ismir.net/ismir2020/paper/000080.pdf",
    "pages": "207-215",
    "zenodo_id": 4245406,
    "dblp_key": null,
    "Abstract": "Grammatical models which represent the hierarchical structure of chord sequences have proven very useful in recent analyses of Jazz harmony. A critical resource for building and evaluating such models is a ground-truth database of syntax trees that encode hierarchical analyses of chord sequences. In this paper, we introduce the Jazz Harmony Treebank (JHT),  a dataset of hierarchical analyses of complete Jazz standards. The analyses were created and checked by experts, based on lead sheets from the open iRealPro collection. The JHT is publicly available in JavaScript Object Notation (JSON), a human-understandable and machine-readable format for structured data. We additionally discuss statistical properties of the corpus and present a simple open-source web application for the graphical creation and editing of trees which was developed during the creation of the dataset.",
    "extra": {
      "takeaway": "This contribution presents a dataset of tree analyses of Jazz chord sequences by music experts in JSON format."
    }
  },
  {
    "title": "A Chorus-Section Detection Method for Lyrics Text",
    "author": [
      " Kento Watanabe",
      " Masataka  Goto"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245442",
    "url": "https://doi.org/10.5281/zenodo.4245442",
    "ee": "http://archives.ismir.net/ismir2020/paper/000088.pdf",
    "pages": "351-359",
    "zenodo_id": 4245442,
    "dblp_key": null,
    "Abstract": "This paper addresses the novel task of detecting chorus sections in English and Japanese lyrics text. Although chorus-section detection using audio signals has been studied, whether chorus sections can be detected from text-only lyrics is an open issue. Another open issue is whether patterns of repeating lyric lines such as those appearing in chorus sections depend on language. To investigate these issues, we propose a neural network-based model for sequence labeling. It can learn phrase repetition and linguistic features to detect chorus sections in lyrics text. It is, however, difficult to train this model since there was no dataset of lyrics with chorus-section annotations as there was no prior work on this task. We therefore generate a large amount of training data with such annotations by leveraging pairs of musical audio signals and their corresponding manually time-aligned lyrics; we first automatically detect chorus sections from the audio signals and then use their temporal positions to transfer them to the line-level chorus-section annotations for the lyrics. Experimental results show that the proposed model with the generated data contributes to detecting the chorus sections, that the model trained on Japanese lyrics can detect chorus sections surprisingly well in English lyrics and that patterns of repeating lyric lines are language-independent.",
    "extra": {
      "takeaway": "This paper proposes a neural network-based, sequence labeling model that detects chorus sections for lyrics text."
    }
  },
  {
    "title": "POP909: A Pop-Song Dataset for Music Arrangement Generation",
    "author": [
      " Ziyu Wang",
      " Ke  Chen",
      " Junyan  Jiang",
      " Yiyi  Zhang",
      " Maoran  Xu",
      " Shuqi  Dai",
      " Gus  Xia"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245366",
    "url": "https://doi.org/10.5281/zenodo.4245366",
    "ee": "http://archives.ismir.net/ismir2020/paper/000089.pdf",
    "pages": "38-45",
    "zenodo_id": 4245366,
    "dblp_key": null,
    "Abstract": "Music arrangement generation is a subtask of automatic music generation, which involves reconstructing and re-conceptualizing a piece with new compositional techniques. Such a generation process inevitably requires reference from the original melody, chord progression, or other structural information. Despite some promising models for arrangement, they lack more refined data to achieve better evaluations and more practical results. In this paper, we propose POP909, a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, we provide the annotations of tempo, beat, key, and chords, where the tempo curves are hand-labeled and others are done by MIR algorithms. Finally, we conduct several baseline experiments with this dataset using standard deep music generation algorithms.",
    "extra": {
      "takeaway": "We propose a novel dataset of 909 Chinese pop-song piano arrangements, which will make contribution to the music generation. "
    }
  },
  {
    "title": "Chord Jazzification: Learning Jazz Interpretations of Chord Symbols",
    "author": [
      " Tsung-Ping Chen",
      " Satoru  Fukayama",
      " Masataka  Goto",
      " Li  Su"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245444",
    "url": "https://doi.org/10.5281/zenodo.4245444",
    "ee": "http://archives.ismir.net/ismir2020/paper/000090.pdf",
    "pages": "360-367",
    "zenodo_id": 4245444,
    "dblp_key": null,
    "Abstract": "Chord symbols, typically notating the root note and the chord quality, are extensively used yet oversimplified representation of tonal harmony and chord progressions in popular music. In spite of its convenience, the chord symbol notation only provides basic information about the chordal configuration, and leaves much room for interpretation. With such limitations, an algorithm generating merely chord symbols is usually insufficient for a wide range of music genres such as jazz. To solve this problem, we propose chord jazzification, a process to generate realistic chord configurations in jazz style. With deep learning approaches, we decompose chord jazzification into coloring and voicing. Coloring concerns the choice of color tones, while voicing concerns the configurations of chords. We also create a new dataset featuring interpretations of chord symbols in pop-jazz compositions. By conducting experiments on the new dataset, we show that 1) the two-stage process outperforms an end-to-end generation approach in modeling chord configurations, and 2) attention-based models are better at capturing the structure of chord sequences in comparison with recurrent neural networks.",
    "extra": {
      "takeaway": "A framework and a dataset are proposed for the generation of Jazz harmony and for the study of chord coloring and voicing."
    }
  },
  {
    "title": "Learning Interpretable Representation for Controllable Polyphonic Music Generation ",
    "author": [
      " Ziyu Wang",
      " Dingsu  Wang",
      " Yixiao  Zhang",
      " Gus  Xia"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245518",
    "url": "https://doi.org/10.5281/zenodo.4245518",
    "ee": "http://archives.ismir.net/ismir2020/paper/000094.pdf",
    "pages": "662-669",
    "zenodo_id": 4245518,
    "dblp_key": null,
    "Abstract": "While deep generative models have become the leading methods for algorithmic composition, it remains a challenging problem to control the generation process because the latent variables of most deep-learning models lack good interpretability. Inspired by the content-style disentanglement idea, we design a novel architecture, under the VAE framework, that effectively learns two interpretable latent factors of polyphonic music: chord and texture. The current model focuses on learning 8-beat long piano composition segments. We show that such chord-texture disentanglement provides a controllable generation pathway leading to a wide spectrum of applications, including compositional style transfer, texture variation, and accompaniment arrangement. Both objective and subjective evaluations show that our method achieves a successful disentanglement and high quality controlled music generation.",
    "extra": {
      "takeaway": "We present a representation-learning architecture which learns interpretable latent factors for polyphonic music generation."
    }
  },
  {
    "title": "Connective Fusion: Learning Transformational Joining of Sequences with Application to Melody Creation",
    "author": [
      " Taketo Akama"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245360",
    "url": "https://doi.org/10.5281/zenodo.4245360",
    "ee": "http://archives.ismir.net/ismir2020/paper/000095.pdf",
    "pages": "46-53",
    "zenodo_id": 4245360,
    "dblp_key": null,
    "Abstract": "We present Connective Fusion, a music generation scheme by transformational joining of two musical sequences for creative purposes. Given two shorter sequences as inputs, our model transforms each of them such that their concatenation is more coherent to form a longer sequence, while each of the transformed shorter sequences retains meaningful similarity with the corresponding input sequence. In short, our model connects and fuses two contextually unrelated sequences in a coherent way. This transformation can be applied iteratively to gradually fuse the input sequences. The style latent space is simultaneously learned, allowing users to control how the two sequences are merged. Our approach comprises two steps of unsupervised learning: a deep generative model with a latent space is learned, followed by adversarial learning of the transformation function in the latent space. We demonstrate the usefulness of our method through the task of melody creation using a symbolic music dataset.",
    "extra": {
      "takeaway": "Connective Fusion is a new music creation scheme. It connects and fuses two unrelated musical sequences in a coherent way."
    }
  },
  {
    "title": "PIANOTREE VAE: Structured Representation Learning for Polyphonic Music",
    "author": [
      " Ziyu Wang",
      " Yiyi  Zhang",
      " Yixiao  Zhang",
      " Junyan  Jiang",
      " Ruihan  Yang",
      " Gus  Xia",
      " Junbo  Zhao"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245446",
    "url": "https://doi.org/10.5281/zenodo.4245446",
    "ee": "http://archives.ismir.net/ismir2020/paper/000096.pdf",
    "pages": "368-375",
    "zenodo_id": 4245446,
    "dblp_key": null,
    "Abstract": "The dominant approach for music representation learning involves the deep unsupervised model family variational autoencoder (VAE). However, most, if not all, viable attempts on this problem have largely been limited to monophonic music. Normally composed of richer modality and more complex musical structures, the polyphonic counterpart has yet to be addressed in the context of music representation learning. In this work, we propose the PianoTree VAE, a novel tree-structure extension upon VAE aiming to fit the polyphonic music learning. The experiments prove the validity of the PianoTree VAE via (i)-semantically meaningful latent code for polyphonic segments; (ii)-more satisfiable reconstruction aside of decent geometry learned in the latent space; (iii)-this model's benefits to the variety of the downstream music generation.",
    "extra": {
      "takeaway": "We contribute PianoTree VAE, which learns the representation of polyphonic music in a hierarchical manner."
    }
  },
  {
    "title": "Towards Multimodal MIR: Predicting individual differences from music-induced movement",
    "author": [
      " Yudhik Agrawal",
      " Samyak  Jain",
      " Emily  Carlson",
      " Petri  Toiviainen",
      " Vinoo   Alluri"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245368",
    "url": "https://doi.org/10.5281/zenodo.4245368",
    "ee": "http://archives.ismir.net/ismir2020/paper/000098.pdf",
    "pages": "54-61",
    "zenodo_id": 4245368,
    "dblp_key": null,
    "Abstract": "As the field of Music Information Retrieval grows, it is important to take into consideration the multi-modality of music and how aspects of musical engagement such as movement and gesture might be taken into account. Bodily movement is universally associated with music and reflective of important individual features related to music preference such as personality, mood, and empathy. Future multimodal MIR systems may benefit from taking these aspects into account. The current study addresses this by identifying individual differences, specifically Big Five personality traits, and scores on the Empathy and Systemizing Quotients (EQ/SQ) from participants' free dance movements. Our model successfully explored the unseen space for personality as well as EQ, SQ, which has not previously been accomplished for the latter. R2 scores for personality, EQ, and SQ were 76.3%, 77.1%, and 86.7% respectively. As a follow-up, we investigated which bodily joints were most important in defining these traits. We discuss how further research may explore how the mapping of these traits to movement patterns can be used to build a more personalized, multi-modal recommendation system, as well as potential therapeutic applications.",
    "extra": {
      "takeaway": "Music Induced Movement predicts individual differences and thus can be incorporated in the design of Mutlimodal MIR systems."
    }
  },
  {
    "title": "Attributes-Aware Deep Music Transformation",
    "author": [
      " Lisa Kawai",
      " Philippe   Esling",
      " Tatsuya  Harada"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245520",
    "url": "https://doi.org/10.5281/zenodo.4245520",
    "ee": "http://archives.ismir.net/ismir2020/paper/000099.pdf",
    "pages": "670-677",
    "zenodo_id": 4245520,
    "dblp_key": null,
    "Abstract": "Recent machine learning techniques have enabled a large variety of novel music generation processes. However, most approaches do not provide any form of interpretable control over musical attributes, such as pitch and rhythm. Obtaining control over the generation process is critically important for its use in real-life creative setups. Nevertheless, this problem remains arduous, as there are no known functions nor differentiable approximations to transform symbolic music with control of musical attributes.  In this work, we propose a novel method that enables attributes-aware music transformation from any set of musical annotations, without requiring complicated derivative implementation. By relying on an adversarial confusion criterion on given musical annotations, we force the latent space of a generative model to abstract from these features. Then, reintroducing these features as conditioning to the generative function, we obtain a continuous control over them. To demonstrate our approach, we rely on sets of musical attributes computed by the jSymbolic library as annotations and conduct experiments that show that our method outperforms previous methods in control. Finally, comparing correlations between attributes and the transformed results show that our method can provide explicit control over any continuous or discrete annotation.",
    "extra": {
      "takeaway": "Our method allows learning continuous high-level controls from simple annotations in symbolic music generation."
    }
  },
  {
    "title": "Multilingual lyrics-to-audio alignment",
    "author": [
      " Andrea Vaglio",
      " Romain  Hennequin",
      " Manuel  Moussallam",
      " Ga\u00EBl  Richard",
      " Florence  d'Alch\u00E9-Buc"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245484",
    "url": "https://doi.org/10.5281/zenodo.4245484",
    "ee": "http://archives.ismir.net/ismir2020/paper/000101.pdf",
    "pages": "512-519",
    "zenodo_id": 4245484,
    "dblp_key": null,
    "Abstract": "Lyrics-to-audio alignment methods have recently reported impressive results, opening the door to practical applications such as karaoke and within song navigation. However, most studies focus on a single language - usually English - for which annotated data are abundant. The question of their ability to generalize to other languages, especially in low (or even zero) training resource scenarios has been so far left unexplored. In this paper, we address the lyrics-to-audio alignment task in a generalized multilingual setup. More precisely, this investigation presents the first (to the best of our knowledge) attempt to create a language-independent lyrics-to-audio alignment system. Building on a RNN model trained with a CTC algorithm, we study the relevance of different intermediate representations, either character or phoneme, along with several strategies to design a training set. The evaluation is conducted on multiple languages with a varying amount of data available, from plenty to zero. Results show that learning from diverse data and using a universal phoneme set as an intermediate representation yield the best generalization performances.",
    "extra": {
      "takeaway": "This study will present the first attempt at creating a language independent lyrics synchronization system"
    }
  },
  {
    "title": "Hierarchical Musical Instrument Separation",
    "author": [
      " Ethan Manilow",
      " Gordon  Wichern",
      " Jonathan  LeRoux"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245448",
    "url": "https://doi.org/10.5281/zenodo.4245448",
    "ee": "http://archives.ismir.net/ismir2020/paper/000105.pdf",
    "pages": "376-383",
    "zenodo_id": 4245448,
    "dblp_key": null,
    "Abstract": "Many sounds that humans encounter are hierarchical in nature; a piano note is one of many played during a performance, which is one of many instruments in a band, which might be playing in a bar with other noises occurring. Inspired by this, we re-frame the musical source separation problem as hierarchical, combining similar instruments together at certain levels and separating them at other levels. This allows us to deconstruct the same mixture in multiple ways, depending on the appropriate level of the hierarchy for a given application. In this paper, we present various methods for hierarchical musical instrument separation, with some methods focusing on separating specific instruments (like guitars) and other methods that determine what to separate based on a user-supplied audio example. We additionally show that separating all hierarchy levels is possible even when training data is limited at fine-grained levels of the hierarchy.",
    "extra": {
      "takeaway": "Methods for separating musical instruments from a mixture according to an instrument hierarchy."
    }
  },
  {
    "title": "Structural Segmentation of Dhrupad Vocal Bandish Audio based on Tempo",
    "author": [
      " Rohit M A",
      " Vinutha  T P",
      " Preeti  Rao"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245522",
    "url": "https://doi.org/10.5281/zenodo.4245522",
    "ee": "http://archives.ismir.net/ismir2020/paper/000110.pdf",
    "pages": "678-684",
    "zenodo_id": 4245522,
    "dblp_key": null,
    "Abstract": "A Dhrupad vocal concert comprises a composition section that is interspersed with improvised episodes of increased rhythmic activity involving the interaction between the vocals and the percussion. Tracking the changing rhythmic density, in relation to the underlying metric tempo of the piece, thus facilitates the detection and labeling of the improvised sections in the concert structure. This work concerns the automatic detection of the musically relevant rhythmic densities as they change in time across the bandish (composition) performance. An annotated dataset of Dhrupad bandish concert sections is presented. We implement a CNN-based system, trained to detect local tempo relationships, and follow it with temporal smoothing. We also employ audio source separation as a pre-processing step to the detection of the individual surface densities of the vocals and the percussion. This helps us obtain the complete musical description of the concert sections in terms of capturing the changing rhythmic interaction of the two performers.",
    "extra": {
      "takeaway": "A SOTA tempo estimation method applied to a new musical context while adapting it to the distinct new challenges presented"
    }
  },
  {
    "title": "Moving in Time: Computational Analysis of Microtiming in Maracatu de Baque Solto",
    "author": [
      " Matthew Davies",
      " Magdalena  Fuentes",
      " Jo\u00E3o  Fonseca",
      " Luis  Aly",
      " Marco  Jer\u00F3nimo",
      " Filippo  Bonini Baraldi"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245554",
    "url": "https://doi.org/10.5281/zenodo.4245554",
    "ee": "http://archives.ismir.net/ismir2020/paper/000113.pdf",
    "pages": "795-802",
    "zenodo_id": 4245554,
    "dblp_key": null,
    "Abstract": "Maracatu de baque solto is a Carnival performance combining music, poetry, and dance, occurring in the Zona da Mata Norte region of Pernambuco (Northeast Brazil). Maracatu percussive music is strongly repetitive, and is played as loud and as fast as possible. Both from an MIR and ethnomusicological perspective this makes a complex musical scene to analyse and interpret. In this paper we focus on the extraction of microtiming profiles towards the longer term goal of understanding how rhythmic performance in Maracatu is used to promote health and well-being. To conduct this analysis we use a set of recordings acquired with contact microphones which minimise the interference between performers. Our analysis reveals that the microtiming profiles differ substantially from those observed in more widely studied South American music. In particular, we highlight the presence of dynamic microtiming profiles as well as the importance of the choice of time-keeper instrument, which dictates how the performances can be understood. Throughout this work, we emphasize the importance of a multidisciplinary approach in which MIR, audio engineering, and ethnomusicology must interact to provide meaningful insight about this music. ",
    "extra": {
      "takeaway": "Analysis of microtiming in Maracatu is extremely challenging even using separated musical sources."
    }
  },
  {
    "title": "Multilingual Music Genre Embeddings for Effective Cross-Lingual Music Item Annotation",
    "author": [
      " Elena V. Epure",
      " Guillaume  Salha",
      " Romain  Hennequin"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245556",
    "url": "https://doi.org/10.5281/zenodo.4245556",
    "ee": "http://archives.ismir.net/ismir2020/paper/000118.pdf",
    "pages": "803-810",
    "zenodo_id": 4245556,
    "dblp_key": null,
    "Abstract": "Annotating music items with music genres is crucial for music recommendation and information retrieval, yet challenging given that music genres are subjective concepts. Recently, in order to explicitly consider this subjectivity, the annotation of music items was modeled as a translation task: predict for a music item its music genres within a target vocabulary or taxonomy (tag system) from a set of music genre tags originating from other tag systems. However, without a parallel corpus, previous solutions could not handle tag systems in other languages, being limited to the English-language only. Here, by learning multilingual music genre embeddings, we enable cross-lingual music genre translation without relying on a parallel corpus. First, we apply compositionality functions on pre-trained word embeddings to represent multi-word tags. Second, we adapt the tag representations to the music domain by leveraging multilingual music genres graphs with a modified retrofitting algorithm. Experiments show that our method: 1) is effective in translating music genres across tag systems in multiple languages (English, French and Spanish); 2) outperforms the previous baseline in an English-language multi-source translation task.",
    "extra": {
      "takeaway": "In this paper, we learn multilingual music genre embeddings, to effectively perform cross-lingual music item annotation."
    }
  },
  {
    "title": "Modelling Hierarchical Key Structure With Pitch Scapes",
    "author": [
      " Robert Lieck",
      " Martin  Rohrmeier"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245558",
    "url": "https://doi.org/10.5281/zenodo.4245558",
    "ee": "http://archives.ismir.net/ismir2020/paper/000123.pdf",
    "pages": "811-818",
    "zenodo_id": 4245558,
    "dblp_key": null,
    "Abstract": "Musical form and syntax in Western classical music are hierarchically organised on different timescales. One of the most important features of this structure is the organisation of modulations between different keys throughout a piece. Music theoretical research has established taxonomies of prototypical modulation plans for different modes and musical forms. However, these prototypes still require empirical validation based on quantitative statistical methods and cannot be retrieved automatically so far.  In this paper, we present a novel method to infer prototypical modulation plans from musical corpora. A modulation plan is formalised as a transposition-invariant probabilistic model over the underlying pitch class distributions based on a hierarchical pitch scape representation. Prototypical modulation plans can be learned in an unsupervised manner by training a mixture model (similar to a Gaussian mixture model) on the data, so that different prototypes appear as distinct clusters.  We evaluate our approach by performing hierarchical clustering on a corpus of more than 150 Baroque pieces, with the extracted clusters showing excellent agreement with the most common prototypes postulated in music theory. Our method bears a great potential for modelling, analysis and discovery of hierarchical key structure and prototypes in corpora across a broad range of musical styles. An accompanying library is available at: github.com/robert-lieck/pitchscapes.",
    "extra": {
      "takeaway": "Prototypical key structures are learned from corpora with an unsupervised clustering model of pitch class distributions"
    }
  },
  {
    "title": "Voice-Leading Schema Recognition Using Rhythm and Pitch Features",
    "author": [
      " Christoph Finkensiep",
      " Ken  D\u00E9guernel",
      " Markus  Neuwirth",
      " Martin  Rohrmeier"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245482",
    "url": "https://doi.org/10.5281/zenodo.4245482",
    "ee": "http://archives.ismir.net/ismir2020/paper/000124.pdf",
    "pages": "520-526",
    "zenodo_id": 4245482,
    "dblp_key": null,
    "Abstract": "Musical schemata constitute important structural building blocks used across historical styles and periods. They consist of two or more melodic lines that are combined to form specific successions of intervals.  This paper tackles the problem of recognizing voice-leading schemata in polyphonic music. Since schema types and subtypes can be realized in a wide variety of ways on the musical surface, finding schemata in an automated fashion is a challenging task. To perform schema inference we employ a skipgram model that computes schema candidates, which are then classified using a binary classifier on musical features related to pitch and rhythm. This model is evaluated on a novel dataset of schema annotations in Mozart's piano sonatas produced by expert annotators, which is published alongside this paper. The features are chosen to encode music-theoretically predicted properties of schema instances. We assess the relevance of each feature for the classification task, thus contributing to the theoretical understanding of complex musical objects.",
    "extra": {
      "takeaway": "Voice-leading schemata can be recognized by matching interval patterns and evaluating musical features on the matches."
    }
  },
  {
    "title": "Semantically Meaningful Attributes from Co-Listen Embeddings for Playlist Exploration and Expansion",
    "author": [
      " Ayush Patwari",
      " Nicholas  Kong",
      " Jun  Wang",
      " Ullas  Gargi",
      " Michele  Covell",
      " Aren  Jansen"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245486",
    "url": "https://doi.org/10.5281/zenodo.4245486",
    "ee": "http://archives.ismir.net/ismir2020/paper/000125.pdf",
    "pages": "527-533",
    "zenodo_id": 4245486,
    "dblp_key": null,
    "Abstract": "Audio embeddings of musical similarity are often used for music recommendations and autoplay discovery. These embeddings are typically learned using co-listen data to train a deep neural network, to provide consistent tripletloss distances. Instead of directly using these co-listen-based embeddings, we explore making recommendations based on a second, smaller embedding space of human-intelligible musical attributes. To do this, we use the co-listen-based audio embeddings as inputs to small attribute classifiers, trained on a small hand-labeled dataset. These classifiers map from the original embedding space to a new interpretable attribute coordinate system that provides a more useful distance measure for downstream applications. The attributes and attribute embeddings allow us to provide a search interface and more intelligible recommendations for music curators. We examine the relative performance of these two embedding spaces (the co-listen-audio embedding and the attribute embedding) for the mathematical separation of thematic playlists. We also report on the usefulness of recommendations from the attribute-embedding space to human curators for automatically extending thematic playlists.",
    "extra": {
      "takeaway": "Musical-attribute embeddings (from co-listen embeddings) effectively distinguish and automatically extend thematic playlists."
    }
  },
  {
    "title": "Downbeat Tracking with Tempo Invariant Convolutional Neural Networks",
    "author": [
      " Bruno Di Giorgi",
      " Matthias  Mauch",
      " Mark  Levy"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245408",
    "url": "https://doi.org/10.5281/zenodo.4245408",
    "ee": "http://archives.ismir.net/ismir2020/paper/000126.pdf",
    "pages": "216-222",
    "zenodo_id": 4245408,
    "dblp_key": null,
    "Abstract": "The human ability to track musical downbeats is robust to changes in tempo, and it extends to tempi never previously encountered.  We propose a deterministic time-warping operation that enables this skill in a convolutional neural network (CNN) by allowing the network to learn rhythmic patterns independently of tempo.  Unlike conventional deep learning approaches, which learn rhythmic patterns at the tempi present in the training dataset, the patterns learned in our model are tempo-invariant, leading to better tempo generalisation and more efficient usage of the network capacity.  We test the generalisation property on a synthetic dataset created by rendering the Groove MIDI Dataset using FluidSynth, split into a training set containing the original performances and a test set containing tempo-scaled versions rendered with different SoundFonts (test-time augmentation). The proposed model generalises nearly perfectly to unseen tempi (F-measure of 0.89 on both training and test sets), whereas a comparable conventional CNN achieves similar accuracy only for the training set (0.89) and drops to 0.54 on the test set. The generalisation advantage of the proposed model extends to real music, as shown by results on the GTZAN and Ballroom datasets.",
    "extra": {
      "takeaway": "We propose a deterministic time-warping operation that allows a CNN to learn rhythmic patterns independently of tempo."
    }
  },
  {
    "title": "ASAP: a dataset of aligned scores and performances for piano transcription",
    "author": [
      " Francesco Foscarin",
      " Andrew  McLeod",
      " Philippe  Rigaux",
      " Florent  Jacquemard",
      " Masahiko  Sakai"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245490",
    "url": "https://doi.org/10.5281/zenodo.4245490",
    "ee": "http://archives.ismir.net/ismir2020/paper/000127.pdf",
    "pages": "534-541",
    "zenodo_id": 4245490,
    "dblp_key": null,
    "Abstract": "In this paper we present Aligned Scores and Performances (ASAP): a new dataset of 222 digital musical scores aligned with 1068 performances (more than 92 hours) of Western classical piano music. The scores are provided as paired MusicXML files and quantized MIDI files, and the performances as paired MIDI files and partially as audio recordings. Scores and performances are aligned with downbeat, beat, time signature, and key signature annotations. ASAP has been obtained thanks to a new annotation workflow that combines score analysis and alignment algorithms, with the goal of reducing the time for manual annotation.  The dataset itself is, to our knowledge, the largest that includes an alignment of music scores to MIDI and audio performance data. As such, it is a useful resource for a wide variety of MIR applications, from those that target the complete audio-to-score Automatic Music Transcription task, to others that target more specific aspects (e.g., key signature estimation and beat or downbeat tracking from both MIDI and audio representations).",
    "extra": {
      "takeaway": "We present a new dataset of digital music scores aligned with performances of piano classical music."
    }
  },
  {
    "title": "Tag2Risk: Harnessing Social Music Tags for Characterizing Depression Risk",
    "author": [
      " Aayush Surana",
      " Yash  Goyal",
      " Manish  Shrivastava",
      " Suvi H.  Saarikallio",
      " Vinoo   Alluri"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245450",
    "url": "https://doi.org/10.5281/zenodo.4245450",
    "ee": "http://archives.ismir.net/ismir2020/paper/000128.pdf",
    "pages": "384-391",
    "zenodo_id": 4245450,
    "dblp_key": null,
    "Abstract": "Musical preferences have been considered a mirror of the self. In this age of Big Data, online music streaming services allow us to capture ecologically valid music listening behavior and provide a rich source of information to identify several user-specific aspects. Studies have shown musical engagement to be an indirect representation of internal states including internalized symptomatology and depression. The current study aims at unearthing patterns and trends in the individuals at risk for depression as it manifests in naturally occurring music listening behavior. Mental-well being scores, musical engagement measures, and listening histories of Last.fm users (N=541) were acquired. Social tags associated with each listener's most popular tracks were analyzed to unearth the mood/emotions and genres associated with the users. Results revealed that social tags prevalent in the users at risk for depression were predominantly related to emotions depicting Sadness associated with genre tags representing neo-psychedelic-, avant garde-, dream-pop. This study will open up avenues for an MIR-based approach to characterizing and predicting risk for depression which can be helpful in early detection and additionally provide bases for designing music recommendations accordingly.",
    "extra": {
      "takeaway": "Social music tags reveal emotions and genres characteristic of music listening behavior of individuals at risk for depression"
    }
  },
  {
    "title": "Content based singing voice source separation via strong conditioning using aligned phonemes",
    "author": [
      " Gabriel Meseguer Brocal",
      " Geoffroy  Peeters"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245560",
    "url": "https://doi.org/10.5281/zenodo.4245560",
    "ee": "http://archives.ismir.net/ismir2020/paper/000131.pdf",
    "pages": "819-827",
    "zenodo_id": 4245560,
    "dblp_key": null,
    "Abstract": "Informed source separation has recently gained renewed interest with the introduction of neural networks and the availability of large multitrack datasets containing both the mixture and the separated sources. These approaches use prior information about the target source to improve separation. Historically, Music Information Retrieval ({MIR}) researchers have focused primarily on score-informed source separation, but more recent approaches explore lyrics-informed source separation. However, because of the lack of multitrack datasets with time-aligned lyrics, models use weak conditioning with the non-aligned lyrics. In this paper, we present a multimodal multitrack dataset with lyrics aligned in time at the word level with phonetic information as well as explore strong conditioning using the aligned phonemes. Our model follows a {U-Net} architecture and takes as input both the magnitude spectrogram of a musical mixture and a matrix with aligned phoneme information. The phoneme matrix is embedded to obtain the parameters that control Feature-wise Linear Modulation ({FiLM}) layers. These layers condition the {U-Net} feature maps to adapt the separation process to the presence of different phonemes via affine transformations. We show that phoneme conditioning can be successfully applied to improve singing voice source separation.",
    "extra": {
      "takeaway": "A new multimodal multitrack dataset with lyrics aligned and strong source separation conditioning using aligned phonemes"
    }
  },
  {
    "title": "BebopNet: Deep Neural Models for Personalized Jazz Improvisations",
    "author": [
      " Shunit Haviv Hakimi",
      " Nadav  Bhonker",
      " Ran  El-Yaniv"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245562",
    "url": "https://doi.org/10.5281/zenodo.4245562",
    "ee": "http://archives.ismir.net/ismir2020/paper/000132.pdf",
    "pages": "828-836",
    "zenodo_id": 4245562,
    "dblp_key": null,
    "Abstract": "A major bottleneck in the evaluation of music generation is that music appreciation is a highly subjective matter. When considering an average appreciation as an evaluation metric, user studies can be helpful. The challenge of generating personalized content, however, has been examined only rarely in the literature. In this paper, we address generation of personalized music and propose a novel pipeline for music generation that learns and optimizes user-specific musical taste. We focus on the task of symbol-based, monophonic, harmony-constrained jazz improvisations. Our personalization pipeline begins with BebopNet, a music language model trained on a corpus of jazz improvisations by Bebop giants. BebopNet is able to generate improvisations based on any given chord progression. We then assemble a personalized dataset, labeled by a specific user, and train a user-specific metric that reflects this user's unique musical taste. Finally, we employ a personalized variant of beam-search with BebopNet to optimize the generated jazz improvisations for that user. We present an extensive empirical study in which we apply this pipeline to extract individual models as implicitly defined by several human listeners. Our approach enables an objective examination of subjective personalized models whose performance is quantifiable. The results indicate that it is possible to model and optimize personal jazz preferences and offer a foundation for future research in personalized generation of art. We also briefly discuss opportunities, challenges, and questions that arise from our work, including issues related to creativity.",
    "extra": {
      "takeaway": "Generating personalized harmony-constrained jazz improvisations by learning and optimizing user-specific musical preference."
    }
  },
  {
    "title": "A Simple Method for User-Driven Music Thumbnailing",
    "author": [
      " Arianne N. van Nieuwenhuijsen",
      " John Ashley  Burgoyne",
      " Frans  Wiering",
      " Mick  Sneekes"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245410",
    "url": "https://doi.org/10.5281/zenodo.4245410",
    "ee": "http://archives.ismir.net/ismir2020/paper/000133.pdf",
    "pages": "223-230",
    "zenodo_id": 4245410,
    "dblp_key": null,
    "Abstract": "More and more music is becoming available digitally, increasing the need to navigate through large numbers of audio tracks easily. One approach for improving the browsing experience is music thumbnailing: the procedure of finding a continuous fragment that can represent the whole musical piece. This paper proposes a human-centred approach to creating thumbnails based on listeners' perception, directly asking listeners to identify the most characteristic fragment. We carried out a user study to assign representativeness scores to multiple fragments from a selection of popular music tracks. To strengthen the results, we performed a replication of the same user study with new participants and a different set of music. Thereafter, we used audio features, the segmentation algorithm, and participants' overall familiarity with the songs to predict representativeness scores. The results suggest that neither segmentation nor familiarity have a significant impact on users' thumbnail preferences: even segments with starting points that pay no regard to song structure can be suitable thumbnails. Three high-level audio characteristics, however, do impact the perceived representativeness of a fragment: Raw Intensity, Melodic Conventionality, and Conventionally of Intensity. Based on these findings, we propose a new, easy-to-apply method for music thumbnailing.",
    "extra": {
      "takeaway": "We propose a user-driven music thumbnailing method based on easily computable audio features and a scoring strategy."
    }
  },
  {
    "title": "PROGRAMMING INEQUALITY: GENDER REPRESENTATION ON CANADIAN COUNTRY RADIO (2005-2019)",
    "author": [
      " Jada E. Watson"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245452",
    "url": "https://doi.org/10.5281/zenodo.4245452",
    "ee": "http://archives.ismir.net/ismir2020/paper/000134.pdf",
    "pages": "392-399",
    "zenodo_id": 4245452,
    "dblp_key": null,
    "Abstract": "In May 2015, a consultant for country radio revealed a decades' long practice of limiting space for songs by female artists. He encouraged program directors to avoid playing songs by women back-to-back and advocated for programming their songs at 13-15% of station playlists. His words sparked debate within the industry and drew attention to growing inequalities on radio and within the genre. The majority of these discussions have centered on US country radio, with limited attention to the growing imbalance on the format in Canada. While country format radio in both countries subscribe to a practice of gender-based programming, Canadian program directors are governed by the federal Broadcasting Act, which regulates dissemination of Canadian content. Using metadata extracted from one of the main radio monitoring services - Mediabase, this paper examines gender-related trends on Canadian country format radio between 2005 and 2019. Through data-driven analysis of Mediabase's weekly re-ports, this paper shows declining representation of songs by women on Canadian country radio and addresses the impact of Canadian content regulations on this process. ",
    "extra": {
      "takeaway": "Programming practices on Canadian country radio have contributed to a growing crisis of gender inequality."
    }
  },
  {
    "title": "Score-Informed Source Separation of Choral Music",
    "author": [
      " Matan Gover",
      " Philippe  Depalle"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245412",
    "url": "https://doi.org/10.5281/zenodo.4245412",
    "ee": "http://archives.ismir.net/ismir2020/paper/000135.pdf",
    "pages": "231-239",
    "zenodo_id": 4245412,
    "dblp_key": null,
    "Abstract": "Choral music recordings are a particularly challenging target for source separation due to the choral blend and the inherent acoustical complexity of the 'choral timbre'. Due to the scarcity of publicly available multi-track choir recordings, we create a dataset of synthesized Bach chorales. We apply data augmentation to alter the chorales so that they more faithfully represent music from a broader range of choral genres. For separation we employ Wave-U-Net, a time-domain convolutional neural network (CNN) originally proposed for vocals and accompaniment separation. We show that Wave-U-Net outperforms a baseline implemented using score-informed NMF (non-negative matrix factorization). We introduce score-informed Wave-U-Net to incorporate the musical score into the separation process. We experiment with different score conditioning methods and show that conditioning on the score leads to improved separation results. We propose a 'score-guided' model variant in which separation is guided by the score alone, bypassing the need to specify the identity of the extracted source. Finally, we evaluate our models (trained on synthetic data only) on real choir recordings and find that in the absence of a large training set of real recordings, NMF still performs better than Wave-U-Net in this setting. To our knowledge, this paper is the first to study source separation of choral music.",
    "extra": {
      "takeaway": "Source separation of choral music: Separation quality is improved by conditioning a time-domain CNN on the musical score."
    }
  },
  {
    "title": "Can't trust the feeling? How open data reveals unexpected behavior of high-level music descriptors",
    "author": [
      " Cynthia C. S. Liem",
      " Chris  Mostert"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245414",
    "url": "https://doi.org/10.5281/zenodo.4245414",
    "ee": "http://archives.ismir.net/ismir2020/paper/000137.pdf",
    "pages": "240-247",
    "zenodo_id": 4245414,
    "dblp_key": null,
    "Abstract": "Copyright restrictions prevent the widespread sharing of commercial music audio. Therefore, the availability of resharable pre-computed music audio features has become critical. In line with this, the AcousticBrainz platform offers a dynamically growing, open and community-contributed large-scale resource of locally computed low-level and high-level music descriptors. Beyond enabling research reuse, the availability of such an open resource allows for renewed reflection on the music descriptors we have at hand: while they were validated to perform successfully under lab conditions, they now are being run 'in the wild'. Their response to these more ecological conditions can shed light on the degree to which they truly had construct validity. In this work, we seek to gain further understanding into this, by analyzing high-level classifier-based music descriptor output in AcousticBrainz. While no hard ground truth is available on what the true value of these descriptors should be, some oracle information can still be derived, relying on semantic redundancies between several descriptors, and multiple feature submissions being available for the same recording. We report on multiple unexpected patterns found in the data, indicating that the descriptor values should not be taken as absolute truth, and hinting at directions for more comprehensive descriptor testing that are overlooked in common machine learning evaluation and quality assurance setups.",
    "extra": {
      "takeaway": "When run 'in the wild' by the community, high-level music descriptors may not perform the way they did in the lab."
    }
  },
  {
    "title": "Improved Handling of Repeats and Jumps in Audio--Sheet Image Synchronization",
    "author": [
      " Mengyi Shan",
      " Timothy  Tsai"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245358",
    "url": "https://doi.org/10.5281/zenodo.4245358",
    "ee": "http://archives.ismir.net/ismir2020/paper/000139.pdf",
    "pages": "62-69",
    "zenodo_id": 4245358,
    "dblp_key": null,
    "Abstract": "This paper studies the problem of automatically generating Youtube piano score following videos given an audio recording and raw sheet music images.  Whereas previous works focus on synthetic sheet music where the data has been cleaned and preprocessed, we instead focus on developing a system that can cope with the messiness of raw, unprocessed sheet music PDFs from IMSLP.  We investigate how well existing systems cope with real scanned sheet music, filler pages and unrelated pieces or movements, and discontinuities due to jumps and repeats.  We find that a significant bottleneck in system performance is handling jumps and repeats correctly.  In particular, we find that a previously proposed Jump DTW algorithm does not perform robustly when jump locations are unknown a priori.  We propose a novel alignment algorithm called Hierarchical DTW that can handle jumps and repeats even when jump locations are not known.  It first performs alignment at the feature level on each sheet music line, and then performs a second alignment at the segment level.  By operating at the segment level, it is able to encode domain knowledge about how likely a particular jump is.  Through carefully controlled experiments on unprocessed sheet music PDFs from IMSLP, we show that Hierarachical DTW significantly outperforms Jump DTW in handling various types of jumps.",
    "extra": {
      "takeaway": "We propose a novel alignment algorithm for synchronizing audio and sheet images in the presence of jumps and repeats."
    }
  },
  {
    "title": "Zero-Shot Singing Voice Conversion",
    "author": [
      " Shahan Nercessian"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245370",
    "url": "https://doi.org/10.5281/zenodo.4245370",
    "ee": "http://archives.ismir.net/ismir2020/paper/000142.pdf",
    "pages": "70-76",
    "zenodo_id": 4245370,
    "dblp_key": null,
    "Abstract": "In this paper, we propose the use of speaker embedding networks to perform zero-shot singing voice conversion, and suggest two architectures for its realization.  The use of speaker embedding networks not only enables the capability to adapt to new voices on-the-fly, but also allows for model training on unlabeled data.  This not only facilitates the collection of suitable singing voice data, but also allows networks to be pretrained on large speech corpora before being refined on singing voice datasets, improving network generalization.  We illustrate the effectiveness of the proposed zero-shot singing voice conversion algorithms by both qualitative and quantitative means.",
    "extra": {
      "takeaway": "Singing voice conversion that can adapt to new voices on-the-fly and allows for model training on unlabeled data."
    }
  },
  {
    "title": "Dance Beat Tracking from Visual Information Alone",
    "author": [
      " Fabrizio Pedersoli",
      " Masataka  Goto"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245456",
    "url": "https://doi.org/10.5281/zenodo.4245456",
    "ee": "http://archives.ismir.net/ismir2020/paper/000144.pdf",
    "pages": "400-408",
    "zenodo_id": 4245456,
    "dblp_key": null,
    "Abstract": "We propose and explore the novel task of dance beat tracking, which can be regarded as a fundamental topic in the Dance Information Retrieval (DIR) research field. Dance beat tracking aims at detecting musical beats from a dance video by using its visual information without using its audio information (i.e., dance music). The visual analysis of dances is important to achieve general machine understanding of dances, not limited to dance music. As a sub-area of Music Information Retrieval (MIR) research, DIR also shares similar goals with MIR and needs to extract various high-level semantics from dance videos. While audio-based beat tracking has been thoroughly studied in MIR, there has not been visual-based beat tracking for dance videos.  We approach dance beat tracking as a time series classification problem and conduct several experiments using a Temporal Convolutional Neural Network (TCN) using the AIST Dance Video Database. We evaluate the proposed solution considering different data splits based on either \"dancer\" or \"music\". Moreover, we propose a periodicity-based loss that considerably improves the overall beat tracking performance according to several evaluation metrics. ",
    "extra": {
      "takeaway": "We propose and explore a novel task of dance beat tracking that detects musical beats from video frames of dance videos."
    }
  },
  {
    "title": "Music SketchNet: Controllable Music Generation via Factorized Representations of Pitch and Rhythm",
    "author": [
      " Ke Chen",
      " Cheng-i  Wang",
      " Taylor  Berg-Kirkpatrick",
      " Shlomo  Dubnov"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245372",
    "url": "https://doi.org/10.5281/zenodo.4245372",
    "ee": "http://archives.ismir.net/ismir2020/paper/000146.pdf",
    "pages": "77-84",
    "zenodo_id": 4245372,
    "dblp_key": null,
    "Abstract": "Drawing an analogy with automatic image completion systems, we propose Music SketchNet, a neural network framework that allows users to specify partial musical ideas guiding automatic music generation. We focus on generating the missing measures in incomplete monophonic musical pieces, conditioned on surrounding context, and optionally guided by user-specified pitch and rhythm snippets. First, we introduce SketchVAE, a novel variational autoencoder that explicitly factorizes rhythm and pitch contour to form the basis of our proposed model. Then we introduce two discriminative architectures, SketchInpainter and SketchConnector, that in conjunction perform the guided music completion, filling in representations for the missing measures conditioned on surrounding context and user-specified snippets. We evaluate SketchNet on a standard dataset of Irish folk music and compare with models from recent works. When used for music completion, our approach outperforms the state-of-the-art both in terms of objective metrics and subjective listening tests. Finally, we demonstrate that our model can successfully incorporate user-specified snippets during the generation process. ",
    "extra": {
      "takeaway": "We propose a novel model to allow users to control the pitch and rhythm attributes inside the music in the generation process"
    }
  },
  {
    "title": "How Music Fans Shape Commercial Music Services:  A Case Study of BTS and ARMY",
    "author": [
      " Jin Ha Lee",
      " Anh Thu  Nguyen"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245564",
    "url": "https://doi.org/10.5281/zenodo.4245564",
    "ee": "http://archives.ismir.net/ismir2020/paper/000147.pdf",
    "pages": "837-845",
    "zenodo_id": 4245564,
    "dblp_key": null,
    "Abstract": "Much of the existing research on user aspects in the music information retrieval field tends to focus on general user needs or behavior related to music information seeking, music listening and sharing, or other use of commercial music services. However, we have a limited understanding of the personal and social contexts of music fans who enthusiastically support musicians and are often avid users of commercial music services. In this study, we aim to better understand the contextual complexities surrounding music fans through a case study of the group BTS and its fan community, ARMY. In particular, we are interested in discovering factors that influence the interactions of music fans with music services, especially in the current environment where the prevalence of social media and other tools/technologies influences musical enjoyment. Through virtual ethnography and content analysis, we identified four factors that affect music fans' interactions with commercial music services: 1) perception of music genres, 2) participatory fandom, 3) desire for agency and transparency, and 4) importance of non-musical factors. The discussion of each aspect is followed by design implications for commercial music services to consider.",
    "extra": {
      "takeaway": "We discuss four factors that affect music fans' interactions with commercial music services via a case study of BTS and ARMY."
    }
  },
  {
    "title": "Artist gender representation in music streaming",
    "author": [
      " Avriel C. Epps-Darling",
      " Henriette  Cramer",
      " Romain  Takeo Bouyer"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245416",
    "url": "https://doi.org/10.5281/zenodo.4245416",
    "ee": "http://archives.ismir.net/ismir2020/paper/000148.pdf",
    "pages": "248-254",
    "zenodo_id": 4245416,
    "dblp_key": null,
    "Abstract": "This study examines gender representation in current music streaming, utilizing one of the world's largest streaming services. First, we found listeners generally stream fewer female or mixed-gender creator groups than male artists, with differences per genre. Second, while still relatively low, we found that recommendation-based streaming has a slightly higher proportion of female creators than \"organic\" listening (i.e., tracks that are not recommended by editors or algorithms). Third, we examined streaming data from 200,000 US users to determine the proportion of female artists in organic and recommended streams over a 28-day period and the relationship between recommended streams and users' future organic listening. The proportion of female artists in recommended streaming appears predictive of the proportion of female artists in organic streaming; these effects are moderated by gender and age. Fourth, this study also samples creators across different popularity levels, seeing more female and multi-gender groups at lower levels than in the middle tiers. However, solo female artists are better represented again in the superstars category, suggesting influence of selected superstars and genres. We conclude by discussing potential avenues in algorithmic auditing.",
    "extra": {
      "takeaway": "Streaming services may be able to challenge structural inequities by spotlighting underrepresented artists in recommendations"
    }
  },
  {
    "title": "Mood Classification Using Listening Data",
    "author": [
      " Filip Korzeniowski",
      " Oriol  Nieto",
      " Matthew  McCallum",
      " Minz  Won",
      " Sergio  Oramas",
      " Erik  Schmidt"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245488",
    "url": "https://doi.org/10.5281/zenodo.4245488",
    "ee": "http://archives.ismir.net/ismir2020/paper/000150.pdf",
    "pages": "542-549",
    "zenodo_id": 4245488,
    "dblp_key": null,
    "Abstract": "The mood of a song is a highly relevant feature for exploration and recommendation in large collections of music. These collections tend to require automatic methods for predicting such moods. In this work, we show that listening-based features outperform content-based ones when classifying moods: embeddings obtained through matrix factorization of listening data appear to be more informative of a track mood than embeddings based on its audio content. To demonstrate this, we compile a subset of the Million Song Dataset, totalling 67k tracks, with expert annotations of 188 different moods collected from AllMusic. Our results on this novel dataset not only expose the limitations of current audio-based models, but also aim to foster further reproducible research on this timely topic",
    "extra": {
      "takeaway": "Listening data outperforms audio embeddings when estimating the moods of a given track"
    }
  },
  {
    "title": "Exploring Aligned Lyrics-informed Singing Voice Separation",
    "author": [
      " Chang-Bin Jeon",
      " Hyeong-Seok  Choi",
      " Kyogu  Lee"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245526",
    "url": "https://doi.org/10.5281/zenodo.4245526",
    "ee": "http://archives.ismir.net/ismir2020/paper/000155.pdf",
    "pages": "685-692",
    "zenodo_id": 4245526,
    "dblp_key": null,
    "Abstract": "In this paper, we propose a method of utilizing aligned lyrics as additional information to improve the performance of singing voice separation. We have combined the highway network-based lyrics encoder into Open-unmix separation network and show that the model trained with the aligned lyrics indeed results in a better performance than the model that was not informed. The question now remains whether the increase of performance is actually due to the phonetic contents that lie in the informed aligned lyrics or not. To this end, we investigated the source of performance increase in multifaceted ways by observing the change of performance when incorrect lyrics were given to the model. Experiment results show that the model can use not only just vocal activity information but also the phonetic contents from the aligned lyrics.",
    "extra": {
      "takeaway": "We proposed the methods of improving singing voice separation by using aligned-lyrics information for the first time."
    }
  },
  {
    "title": "Sesquialtera in the Colombian bambuco: Perception and estimation of beat and meter",
    "author": [
      " Estefan\u00EDa Cano",
      " Fernando  Mora \u00C1ngel",
      " Gustavo Adolfo  L\u00F3pez Gil",
      " Jos\u00E9 Ricardo  Zapata",
      " Antonio  Escamilla",
      " Juan Fernando  Alzate Londo\u00F1o",
      " Moises  Betancur Pel\u00F1ez"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245454",
    "url": "https://doi.org/10.5281/zenodo.4245454",
    "ee": "http://archives.ismir.net/ismir2020/paper/000156.pdf",
    "pages": "409-415",
    "zenodo_id": 4245454,
    "dblp_key": null,
    "Abstract": "The bambuco, one of the national rhythms of Colombia, is characterized by the presence of sesquialteras or the superposition of rhythmic elements from two meters. In this work, we analyze sesquialteras in bambucos from two perspectives. First, we analyze the perception of beat and meter by asking 10 Colombian musicians to perform beat annotations in a dataset of bambucos. Results show  great diversity in the annotations: a total of five different meters or meter combinations were found in the annotations, with each bambuco in the study being annotated in at least two different meters.  Second, we perform a beat tracking analysis in a dataset of bambucos with two state-of-the-art algorithms.  Given that the algorithms used in the analysis were designed to deal with the rhythmic regularity of a single meter, it is not surprising that tracking performance is not very high (~42% mean F-measure). However, a deeper analysis of  the onset detection functions used for beat tracking, indicate that there is enough information on the signal level to characterize the bi-metric behavior of bambucos. With this in mind, we highlight possibilities for computational analysis of rhythm in bambucos.",
    "extra": {
      "takeaway": "Percpetual and computational studies confirm the presence of musical elements from two meter in the Colombian bambuco."
    }
  },
  {
    "title": "Joint analysis of mode and playing technique in Guqin performance with machine learning",
    "author": [
      " Yu-Fen Huang",
      " Jeng-I  Liang",
      " I-Chieh  Wei",
      " Li  Su"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245378",
    "url": "https://doi.org/10.5281/zenodo.4245378",
    "ee": "http://archives.ismir.net/ismir2020/paper/000157.pdf",
    "pages": "85-92",
    "zenodo_id": 4245378,
    "dblp_key": null,
    "Abstract": "Music is hierarchically structured, in which the global attributes (e.g., the determined tonal structure, musical form) dominate the distribution of local elements (e.g., pitch, playing technique arrangement). Existing methods for instrumental playing technique detection mostly focus on the local features extracted from audio. However, we argue that structural information is critical for both global and local tasks, particularly considering the characteristics of Guqin music. Incorporating mode and playing technique analysis, this study demonstrates that the structural relationship between notes is crucial for detecting mode, and such information also provides extra guidance for the playing technique detection in local-level. In this study, a new dataset is compiled for Guqin performance analysis. The mode detection is achieved by pattern matching, and the predicted results are conjoined with audio features to be inputted into a neural network for playing technique detection. Advanced techniques are developed to optimize the extracted pitch contour from the audio. It is manifest in the results that the global and local features are inter-connected in Guqin music. Our analysis identifies key components affecting the recognition of mode and playing technique, and challenging cases resulting from unique properties in Guqin audio signal are discussed for further research.",
    "extra": {
      "takeaway": "Structural information is critical for both global- and local-level tasks (such as mode and playing technique detection)"
    }
  },
  {
    "title": "Score following with hidden tempo using a switching state-space model",
    "author": [
      " Yucong Jiang"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245528",
    "url": "https://doi.org/10.5281/zenodo.4245528",
    "ee": "http://archives.ismir.net/ismir2020/paper/000159.pdf",
    "pages": "693-699",
    "zenodo_id": 4245528,
    "dblp_key": null,
    "Abstract": "A score-following program traces the notes in a musical score during a performance. This capability is essential to many meaningful applications that synchronize audio with a score in an on-line fashion. Existing algorithms often stumble on certain difficult cases, one of which is piano music. This paper presents a new method to tackle such cases. The method treats tempo as a variable rather than a constant (with constraints), allowing the program to adapt to live performance variations. This is first expressed by a Kalman filter model at the note level, and then by an almost equivalent switching state-space model at the audio frame level. The latter contains both discrete and continuous hidden variables, and is computationally intractable. We show how certain reasonable approximations make the computation manageable. This new method is tested on a dataset of 50 piano excerpts. Compared with a previously established state-of-the-art algorithm, the new method shows more stable and accurate results: it reduces fatal score-following errors, and improves accuracy from 65.0% to 69.1%.",
    "extra": {
      "takeaway": "Improve score following by tracking tempo using a switching state-space model."
    }
  },
  {
    "title": "Semi-supervised learning using teacher-student models for vocal melody extraction",
    "author": [
      " Sangeun Kum",
      " Jing-Hua  Lin",
      " Li  Su",
      " Juhan  Nam"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245374",
    "url": "https://doi.org/10.5281/zenodo.4245374",
    "ee": "http://archives.ismir.net/ismir2020/paper/000160.pdf",
    "pages": "93-100",
    "zenodo_id": 4245374,
    "dblp_key": null,
    "Abstract": "The lack of labeled data is a major obstacle in many music information retrieval tasks such as melody extraction, where labeling is extremely laborious or costly. Semi-supervised learning (SSL) provides a solution to alleviate the issue by leveraging a large amount of unlabeled data. In this paper, we propose an SSL method using teacher-student models for vocal melody extraction. The teacher model is pre-trained with labeled data and guides the student model to make identical predictions given unlabeled input in a self-training setting. We examine three setups of teacher-student models with different data augmentation schemes and loss functions. Also, considering the scarcity of labeled data in the test phase, we artificially generate large-scale testing data with pitch labels from unlabeled data using an analysis-synthesis method. The results show that the SSL method significantly increases the performance against supervised learning only and the improvement depends on the teacher-student models, the size of unlabeled data, the number of self-training iterations, and other training details. We also find that it is essential to ensure that the unlabeled audio has vocal parts. Finally, we show that the proposed SSL method allows a simple convolutional recurrent neural network model to achieve performance comparable to state-of-the-arts.",
    "extra": {
      "takeaway": "We propose an semi-supervised learning method using teacher-student models for vocal melody extraction."
    }
  },
  {
    "title": "Unsupervised Disentanglement of Pitch and Timbre for Isolated Musical Instrument Sounds",
    "author": [
      " Yin-Jyun Luo",
      " Kin Wai  Cheuk",
      " Tomoyasu  Nakano",
      " Masataka  Goto",
      " Dorien  Herremans"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245532",
    "url": "https://doi.org/10.5281/zenodo.4245532",
    "ee": "http://archives.ismir.net/ismir2020/paper/000162.pdf",
    "pages": "700-707",
    "zenodo_id": 4245532,
    "dblp_key": null,
    "Abstract": "Disentangling factors of variation aims to uncover latent variables that underlie the process of data generation. In this paper, we propose a framework that achieves unsupervised pitch and timbre disentanglement for isolated musical instrument sounds without relying on data annotations or pre-trained neural networks. Our framework, based on variational auto-encoders, takes as input a spectral frame, and encodes pitch and timbre as categorical and continuous variables, respectively. The input is then reconstructed by combining those variables. Under an unsupervised training setting, a major challenge is that encoders are tasked to capture factors of interest with distinct latent representations, without access to the corresponding ground-truth labels. We therefore introduce auxiliary tasks and objectives which leverage pitch shifting as a strategy to create surrogate labels, thereby encouraging the disentanglement of pitch and timbre. Through an ablation study we analyze the impact of the proposed objectives. The evaluation shows the efficacy of the proposed framework for learning disentangled representations, and verifies its applicability to unsupervised pitch classification and conditional spectral synthesis.",
    "extra": {
      "takeaway": "We have achieved unsupervised pitch and timbre disentanglement by a specific design of architecture and auxiliary objectives."
    }
  },
  {
    "title": "Automatic Rank-Ordering of Singing Vocals with Twin-Neural Network",
    "author": [
      " Chitralekha Gupta",
      " Lin  Huang",
      " Haizhou  Li"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245458",
    "url": "https://doi.org/10.5281/zenodo.4245458",
    "ee": "http://archives.ismir.net/ismir2020/paper/000165.pdf",
    "pages": "416-423",
    "zenodo_id": 4245458,
    "dblp_key": null,
    "Abstract": "When making judgements, humans are known to be better at choosing a preferred option amongst a small number of options, rather than giving an absolute ranking of all the options. This preference-based judgment rank-ordering method is called Best-Worst Scaling (BWS). Inspired by this concept, we propose a preference-based framework to generate a relative rank-ordering of singing vocals, and therefore, singers. We adopt a twin-neural network (Siamese) that learns to choose a preferred candidate in terms of singing quality between two inputs. With a few such pairwise comparisons, this method generates a relative rank-order of a complete list of singers. Additionally, we incorporate a knowledge-based musically-relevant pitch histogram representation, as a conditioning vector, to provide explicit musical information to the network. The experiments show that this method is able to reliably evaluate singing quality and rank-order singing vocals, independent of the song or the singer. The results suggest that the twin-neural network learns the underlying discerning properties relevant to singing quality, instead of being specific to the content of a song or singer.",
    "extra": {
      "takeaway": "We modify a Siamese network for the purpose of song-independent rank-ordering of singers according to their singing quality."
    }
  },
  {
    "title": "Human-AI Co-Creation in Songwriting",
    "author": [
      " Cheng-Zhi Anna Huang",
      " Hendrik Vincent  Koops",
      " Ed  Newton-Rex",
      " Monica   Dinculescu",
      " Carrie  Cai"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245530",
    "url": "https://doi.org/10.5281/zenodo.4245530",
    "ee": "http://archives.ismir.net/ismir2020/paper/000167.pdf",
    "pages": "708-716",
    "zenodo_id": 4245530,
    "dblp_key": null,
    "Abstract": "Machine learning is challenging the way we make music. Although research in deep generative models has dramatically improved the capability and fluency of music models, recent work has shown that it can be challenging for humans to partner with this new class of algorithms. In this paper, we present findings on what 13 musician/developer teams, a total of 61 users, needed when co-creating a song with AI, the challenges they faced, and how they leveraged and repurposed existing characteristics of AI to overcome some of these challenges. Many teams adopted modular approaches, such as independently running multiple smaller models that align with the musical building blocks of a song, before re-combining their results. As ML models are not easily steerable, teams also generated massive numbers of samples and curated them post-hoc, or used a range of strategies to direct the generation or algorithmically ranked the samples. Ultimately, teams not only had to manage the ``flare and focus'' aspects of the creative process, but also juggle that with a parallel process of exploring and curating multiple ML models and outputs. These findings reflect a need to design machine learning-powered music interfaces that are more decomposable, steerable, interpretable, and adaptive, which in return will enable artists to more effectively explore how AI can extend their personal expression.  ",
    "extra": {
      "takeaway": "We provide insight on the current state of how humans partner with AI in songwriting and discuss future directions."
    }
  },
  {
    "title": "Analysis of Song/Artist Latent Features and Its Application for Song Search",
    "author": [
      " Kosetsu Tsukuda",
      " Masataka  Goto"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245538",
    "url": "https://doi.org/10.5281/zenodo.4245538",
    "ee": "http://archives.ismir.net/ismir2020/paper/000172.pdf",
    "pages": "717-725",
    "zenodo_id": 4245538,
    "dblp_key": null,
    "Abstract": "For recommending songs to a user, one effective approach is to represent artists and songs with latent vectors and predict the user's preference toward the songs. Although the latent vectors represent the characteristics of artists and songs well, they have typically been used only for computing the preference score. In this paper, we discuss how we can leverage these vectors for realizing applications that enable users to search for songs from new perspectives. To this end, by embedding song/artist vectors into the same feature space, we first propose two concepts of artist-song relationships: overall similarity and prominent affinity. Overall similarity is the degree to which the characteristics of a song are similar overall to the characteristics of the artist; while prominent affinity is the degree to which a song prominently represents the characteristics of the artist. By using Last.fm play logs for two years, we analyze the characteristics of the concepts. Moreover, based on the analysis results, we propose three applications for song search. Through case studies, we demonstrate that our proposed applications are beneficial for searching for songs according to the users' various search intents.",
    "extra": {
      "takeaway": "We analyze song/artist latent features created from play logs and propose song retrieval applications based on the features."
    }
  },
  {
    "title": "The MIDI Degradation Toolkit: Symbolic Music Augmentation and Correction",
    "author": [
      " Andrew McLeod",
      " James  Owers",
      " Kazuyoshi  Yoshii"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245566",
    "url": "https://doi.org/10.5281/zenodo.4245566",
    "ee": "http://archives.ismir.net/ismir2020/paper/000182.pdf",
    "pages": "846-852",
    "zenodo_id": 4245566,
    "dblp_key": null,
    "Abstract": "In this paper, we introduce the MIDI Degradation Toolkit (MDTK), containing functions which take as input a musical excerpt (a set of notes with pitch, onset time, and duration), and return a ``degraded'' version of that excerpt with some error (or errors) introduced. Using the toolkit, we create the Altered and Corrupted MIDI Excerpts dataset version 1.0 (ACME v1.0), and propose four tasks of increasing difficulty to detect, classify, locate, and correct the degradations. We hypothesize that models trained for these tasks can be useful in (for example) improving automatic music transcription performance if applied as a post-processing step. To that end, MDTK includes a script that measures the distribution of different types of errors in a transcription, and creates a degraded dataset with similar properties. MDTK's degradations can also be applied dynamically to a dataset during training (with or without the above script), generating novel degraded excerpts each epoch. MDTK could also be used to test the robustness of any system designed to take MIDI (or similar) data as input (e.g. systems designed for voice separation, metrical alignment, or chord detection) to such transcription errors or otherwise noisy data. The toolkit and dataset are both publicly available online, and we encourage contribution and feedback from the community.",
    "extra": {
      "takeaway": "We present a toolkit to introduce errors into MIDI excerpts, useful for (e.g.) data augmentation for AMT."
    }
  },
  {
    "title": "MusPy: A Toolkit for Symbolic Music Generation",
    "author": [
      " Hao-Wen Dong",
      " Ke  Chen",
      " Julian  McAuley",
      " Taylor  Berg-Kirkpatrick"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245380",
    "url": "https://doi.org/10.5281/zenodo.4245380",
    "ee": "http://archives.ismir.net/ismir2020/paper/000187.pdf",
    "pages": "101-108",
    "zenodo_id": 4245380,
    "dblp_key": null,
    "Abstract": "In this paper, we present MusPy, an open source Python library for symbolic music generation. MusPy provides easy-to-use tools for essential components in a music generation system, including dataset management, data I/O, data preprocessing and model evaluation. In order to showcase its potential, we present statistical analysis of the eleven datasets currently supported by MusPy. Moreover, we conduct a cross-dataset generalizability experiment by training an autoregressive model on each dataset and measuring held-out likelihood on the others---a process which is made easier by MusPy's dataset management system. The results provide a map of domain overlap between various commonly used datasets and show that some datasets contain more representative cross-genre samples than others. Along with the dataset analysis, these results might serve as a guide for choosing datasets in future research. Source code and documentation are available at https://github.com/salu133445/muspy .",
    "extra": {
      "takeaway": "We present a new library for symbolic music generation with tools for dataset managing, data processing and model evaluation."
    }
  },
  {
    "title": "Ultra-light deep MIR by trimming lottery tickets",
    "author": [
      " Philippe  Esling",
      " Th\u00E9is  Bazin",
      " Adrien  Bitton",
      " Tristan J. J.  Carsault",
      " Ninon  Devis"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245492",
    "url": "https://doi.org/10.5281/zenodo.4245492",
    "ee": "http://archives.ismir.net/ismir2020/paper/000188.pdf",
    "pages": "550-557",
    "zenodo_id": 4245492,
    "dblp_key": null,
    "Abstract": "Current state-of-art results in Music Information Retrieval are largely dominated by deep learning approaches. These provide unprecedented accuracy across all discriminative tasks. However, the consistently overlooked downside of these models is their stunningly massive complexity, which seems concomitantly crucial to their success.   In this paper, we address this issue by developing a new approach based on the recent lottery ticket hypothesis. We modify the original lottery approach to allow for explicitly removing parameters, through structured trimming of entire units, instead of simply masking individual weights. This allows to obtain models which are effectively lighter in terms of size, memory and number of operations.  We show that our proposal allows to remove up to 95% of the models parameters without loss of accuracy, leading to ultra-light deep MIR models. We confirm the surprising result that, at smaller compression ratios (removing up to 90% of the network), lighter models consistently outperform their heavier counterpart. We exhibit these results on a large array of MIR tasks including audio classification, pitch recognition, chord extraction, drum transcription and onset estimation. These resulting ultra-light deep models for MIR can run on CPU, and can even fit on embedded devices with minimal degradation of accuracy. ",
    "extra": {
      "takeaway": "Our method can remove up to 85% of the weights in deep networks applied to MIR tasks without loss of accuracy"
    }
  },
  {
    "title": "Data Cleansing with Contrastive Learning for Vocal Note Event Annotations",
    "author": [
      " Gabriel Meseguer Brocal",
      " Rachel  Bittner",
      " Simon  Durand",
      " Brian  Brost"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245420",
    "url": "https://doi.org/10.5281/zenodo.4245420",
    "ee": "http://archives.ismir.net/ismir2020/paper/000189.pdf",
    "pages": "255-262",
    "zenodo_id": 4245420,
    "dblp_key": null,
    "Abstract": "Data cleansing is a well studied strategy for cleaning erroneous labels in datasets, which has not yet been widely adopted in Music Information Retrieval. Previously proposed data cleansing models do not consider structured (e.g. time varying) labels, such as those common to music data. We propose a novel data cleansing model for time-varying, structured labels which exploits the local structure of the labels, and demonstrate its usefulness for vocal note event annotations in music. Our model is trained in a contrastive learning manner by automatically contrasting likely correct labels pairs against local deformations of them. We demonstrate that the accuracy of a transcription model improves greatly when trained using our proposed data cleaning strategy compared with the accuracy when trained using the original dataset. Additionally we use our model to estimate the annotation error rates in the DALI dataset, and highlight other potential uses for this type of model.",
    "extra": {
      "takeaway": "a new data filtering method which exploits the structure of time-varying music labels, improving downstream performance"
    }
  },
  {
    "title": "Joyful for you and tender for us: the influence of individual characteristics and language on emotion labeling and classification",
    "author": [
      " Juan S. G\u00F3mez-Ca\u00F1n",
      " Estefan\u00EDa  Cano",
      " Perfecto  Herrera",
      " Emilia  G\u00F3mez"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245568",
    "url": "https://doi.org/10.5281/zenodo.4245568",
    "ee": "http://archives.ismir.net/ismir2020/paper/000190.pdf",
    "pages": "853-860",
    "zenodo_id": 4245568,
    "dblp_key": null,
    "Abstract": "Tagging a musical excerpt with an emotion label may result in a vague and ambivalent exercise. This subjectivity entangles several high-level music description tasks when the computational models built to address them produce predictions on the basis of a \"ground truth\". In this study, we investigate the relationship between emotions perceived in pop and rock music (mainly in Euro-American styles) and personal characteristics from the listener, using language as a key feature. Our goal is to understand the influence of lyrics comprehension on music emotion perception and use this knowledge to improve Music Emotion Recognition (MER) models.  We systematically analyze over 30K annotations of 22 musical fragments to assess the impact of individual differences on agreement, as defined by Krippendorff's coefficient. We employ personal characteristics to form group-based annotations by assembling ratings with respect to listeners' familiarity, preference, lyrics comprehension, and music sophistication. Finally, we study our group-based annotations in a two-fold approach: (1) assessing the similarity within annotations using manifold learning algorithms and unsupervised clustering, and (2) analyzing their performance by training classification models with diverse \"ground truths\". Our results suggest that a) applying a broader categorization of taxonomies and b) using multi-label, group-based annotations based on language, can be beneficial for MER models.",
    "extra": {
      "takeaway": "We study the effect of annotator agreement on the Music Emotion Recognition task, using language as a key grouping factor."
    }
  },
  {
    "title": "A Neural Approach for Full-Page Optical Music Recognition of Mensural Documents",
    "author": [
      " Francisco J. Castellanos",
      " Jorge  Calvo-Zaragoza",
      " Jose M.  Inesta"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245494",
    "url": "https://doi.org/10.5281/zenodo.4245494",
    "ee": "http://archives.ismir.net/ismir2020/paper/000207.pdf",
    "pages": "558-565",
    "zenodo_id": 4245494,
    "dblp_key": null,
    "Abstract": "The digitization of the content within musical manuscripts allows the possibility of preserving, disseminating, and exploiting that cultural heritage. The automation of this process has been object of study for a long time in the field of Optical Music Recognition (OMR), with a wide variety of proposed solutions. Currently, there is a tendency to use machine learning strategies based on neural networks because of their high performance and flexibility to adapt to different scenarios by changing only the training data. However, most of the recent literature addresses only specific parts of the traditional OMR workflow such as music object detection or symbol classification. In this paper, we progress one step further by proposing a full-page OMR system for Mensural notation scores that consists of simply two processes, which are enough to extract the symbolic music information from a full page. More precisely, our pipeline uses Selectional Auto-Encoders to extract single staff regions, combined with end-to-end staff-level recognition based on Convolutional Recurrent Neural Networks for retrieving the music notation. The results confirm the adequacy of our method, reporting a successful behavior on two Mensural collections (Capitan and Seils datasets) with a straightforward implementation.",
    "extra": {
      "takeaway": "The work presents a full-page OMR system for Mensural manuscripts that consists of simply two processes."
    }
  },
  {
    "title": "Multidimensional similarity modelling of complex drum loops using the GrooveToolbox",
    "author": [
      " Fred Bruford",
      " Olivier  Lartillot",
      " SKoT  McDonald",
      " Mark B.  Sandler"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245418",
    "url": "https://doi.org/10.5281/zenodo.4245418",
    "ee": "http://archives.ismir.net/ismir2020/paper/000211.pdf",
    "pages": "263-270",
    "zenodo_id": 4245418,
    "dblp_key": null,
    "Abstract": "The GrooveToolbox is a new Python library implementing numerous algorithms, both novel and pre-existing, for the analysis of symbolic drum loops, including rhythm features, similarity metrics and microtiming features. As part of the GrooveToolbox we introduce two new metrics of rhythm similarity and four new features for describing the perceptually salient properties of microtiming deviations in drum loops. Based on a two-part perceptual evaluation, we show these four new microtiming features can each correlate to similarity perception, and be used along with rhythm similarity metrics to improve personalized similarity models for complex drum loops. A new measure of structural rhythmic similarity is also shown to correlate more strongly to similarity perception of drum loops than the more commonly used Hamming distance. These results point to the potential application of the GrooveToolbox and its new features in drum loop analysis for intelligent music production tools. The GrooveToolbox may be found at:  https://github.com/fredbru/GrooveToolbox ",
    "extra": {
      "takeaway": "New features modelling rhythmic structure and microtiming deviations improve drum loop similarity models"
    }
  },
  {
    "title": "Modeling perception with hierarchical prediction: Auditory segmentation with deep predictive coding locates candidate evoked potentials in EEG",
    "author": [
      " Andr\u00E9 Ofner",
      " Sebastian  Stober"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245496",
    "url": "https://doi.org/10.5281/zenodo.4245496",
    "ee": "http://archives.ismir.net/ismir2020/paper/000219.pdf",
    "pages": "566-573",
    "zenodo_id": 4245496,
    "dblp_key": null,
    "Abstract": "The human response to music combines low-level expectations that are driven by the perceptual characteristics of audio with high-level expectations from the context and the listener's expertise. This paper discusses surprisal based music representation learning with a hierarchical predictive neural network. In order to inspect the cognitive validity of the network's predictions along their time-scales, we use the network's prediction error to segment electroencephalograms (EEG) based on the audio signal. Using the NMED-T dataset on passive natural music listening we explore the automatic segmentation of audio and EEG into events using the suggested model. By averaging only the EEG signal at predicted locations, we were able to visualize auditory evoked potentials connected to local and global musical structures. This indicates the potential of unsupervised predictive learning with deep neural networks as means to retrieve musical structure from audio and as a basis to uncover the corresponding cognitive processes in the human brain.",
    "extra": {
      "takeaway": "Deep hierarchical predictive models of music perception allow to segment audio and EEG simultaneously."
    }
  },
  {
    "title": "Music FaderNets: Controllable Music Generation Based On High-Level Features via Low-Level Feature Modelling",
    "author": [
      " Hao Hao Tan",
      " Dorien  Herremans"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245376",
    "url": "https://doi.org/10.5281/zenodo.4245376",
    "ee": "http://archives.ismir.net/ismir2020/paper/000222.pdf",
    "pages": "109-116",
    "zenodo_id": 4245376,
    "dblp_key": null,
    "Abstract": "High-level musical qualities (such as emotion) are often abstract, subjective, and hard to quantify. Given these difficulties, it is not easy to learn good feature representations with supervised learning techniques, either because of the insufficiency of labels, or the subjectiveness (and hence large variance) in human-annotated labels. In this paper, we present a framework that can learn high-level feature representations with a limited amount of data, by first modelling their corresponding quantifiable low-level attributes. We refer to our proposed framework as Music FaderNets, which is inspired by the fact that low-level attributes can be continuously manipulated by separate \"sliding faders\" through feature disentanglement and latent regularization techniques. High-level features are then inferred from the low-level representations through semi-supervised clustering using Gaussian Mixture Variational Autoencoders (GM-VAEs). Using arousal as an example of a high-level feature, we show that the \"faders\" of our model are disentangled and change linearly w.r.t. the modelled low-level attributes of the generated output music. Furthermore, we demonstrate that the model successfully learns the intrinsic relationship between arousal and its corresponding low-level attributes (rhythm and note density), with only 1% of the training set being labelled. Finally, using the learnt high-level feature representations, we explore the application of our framework in style transfer tasks across different arousal states. The effectiveness of this approach is verified through a subjective listening test.",
    "extra": {
      "takeaway": "Music generation with \"faders\" controlling low-level features, and infer presets to represent abstract, high-level features"
    }
  },
  {
    "title": "Deconstruct, Analyse, Reconstruct: How to improve Tempo, Beat, and Downbeat Estimation",
    "author": [
      " Sebastian B\u00F6ck",
      " Matthew  Davies"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245498",
    "url": "https://doi.org/10.5281/zenodo.4245498",
    "ee": "http://archives.ismir.net/ismir2020/paper/000223.pdf",
    "pages": "574-582",
    "zenodo_id": 4245498,
    "dblp_key": null,
    "Abstract": "In this paper, we undertake a critical assessment of a state-of-the-art deep neural network approach for computational rhythm analysis. Our methodology is to deconstruct this approach, analyse its constituent parts, and then reconstruct it. To this end, we devise a novel multi-task approach for the simultaneous estimation of tempo, beat, and downbeat. In particular, we seek to embed more explicit musical knowledge into the design decisions in building the network. We additionally reflect this outlook when training the network, and include a simple data augmentation strategy to increase the network's exposure to a wider range of tempi, and hence beat and downbeat information. Via an in-depth comparative evaluation, we present state-of-the-art results over all three tasks, with performance increases of up to 6% points over existing systems.",
    "extra": {
      "takeaway": "Musically informed tuning of machine learning state-of-the-art system gives up to 10% points improvement."
    }
  },
  {
    "title": "Neural Loop Combiner: Neural Network Models for Assessing the Compatibility of Loops",
    "author": [
      " Bo-Yu Chen",
      " Jordan B. L.  Smith",
      " Yi-Hsuan  Yang"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245462",
    "url": "https://doi.org/10.5281/zenodo.4245462",
    "ee": "http://archives.ismir.net/ismir2020/paper/000225.pdf",
    "pages": "424-431",
    "zenodo_id": 4245462,
    "dblp_key": null,
    "Abstract": "Music producers who use loops may have access to thousands in loop libraries, but finding ones that are compatible is a time-consuming process; we hope to reduce this burden with automation. State-of-the-art systems for estimating compatibility, such as AutoMashUpper, are mostly rule-based and could be improved on with machine learning. To train a model, we need a large set of loops with ground truth compatibility values. No such dataset exists, so we extract loops from existing music to obtain positive examples of compatible loops, and propose and compare various strategies for choosing negative examples. For reproducibility, we curate data from the Free Music Archive. Using this data, we investigate two types of model architectures for estimating the compatibility of loops: one based on a Siamese network, and the other a pure convolutional neural network (CNN). We conducted a user study in which participants rated the quality of the combinations suggested by each model, and found the CNN to outperform the Siamese network. Both model-based approaches outperformed the rule-based one. We have opened source the code for building the models and the dataset.",
    "extra": {
      "takeaway": "We realize a model-based approach with NN for assessing the compatibility of loops and show it's better than a rule-based one"
    }
  },
  {
    "title": "Rule Mining for Local Boundary Detection in Melodies",
    "author": [
      " Peter Van Kranenburg"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245422",
    "url": "https://doi.org/10.5281/zenodo.4245422",
    "ee": "http://archives.ismir.net/ismir2020/paper/000226.pdf",
    "pages": "271-278",
    "zenodo_id": 4245422,
    "dblp_key": null,
    "Abstract": "The task of melodic segmentation is a long-standing MIR task that has not yet been solved. In this paper, a rule mining algorithm is employed to find rule sets that classify notes within their local context as phrase boundaries. Both the discovered rule set and a Random Forest Classifier trained on the same data set outperform previous methods on the task of melodic segmentation of melodies from the Essen Folk Song Collection, the Meertens Tune Collections, and the set of Bach Chorales. By inspecting the rules, some important clues are revealed about what constitutes a melodic phrase boundary, notably a prevalence of rhythm features over pitch features.",
    "extra": {
      "takeaway": "A rule mining approach to melodic segmentation reveals musical clues for phrase boundaries and outperforms previous methods."
    }
  },
  {
    "title": "Butter Lyrics Over Hominy Grit: Comparing Audio and Psychology-Based Text Features in MIR Tasks",
    "author": [
      " Jaehun Kim",
      " Andrew M.  Demetriou",
      " Sandy  Manolios",
      " M. Stella  Tavella",
      " Cynthia C. S.  Liem"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245574",
    "url": "https://doi.org/10.5281/zenodo.4245574",
    "ee": "http://archives.ismir.net/ismir2020/paper/000228.pdf",
    "pages": "861-868",
    "zenodo_id": 4245574,
    "dblp_key": null,
    "Abstract": "Psychology research has shown that song lyrics are a rich source of data, yet they are often overlooked in the field of MIR compared to audio. In this paper, we provide an initial assessment of the usefulness of features drawn from lyrics for various fields, such as MIR and Music Psychology. To do so, we assess the performance of lyric-based text features on 3 MIR tasks, in comparison to audio features. Specifically, we draw sets of text features from the field of Natural Language Processing and Psychology. Further, we estimate their effect on performance while statistically controlling for the effect of audio features, by using a hierarchical regression statistical model. Lyric-based features show a small but statistically significant effect, that anticipates further research. Implications and directions for future studies are discussed.",
    "extra": {
      "takeaway": "We explore the use of Psychology and NLP-based text features extracted from Lyrics, and their usefulness in 3 MIR tasks."
    }
  },
  {
    "title": "Mode classification and natural units in plainchant",
    "author": [
      " Bas Cornelissen",
      " Willem  Zuidema",
      " John Ashley  Burgoyne"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245572",
    "url": "https://doi.org/10.5281/zenodo.4245572",
    "ee": "http://archives.ismir.net/ismir2020/paper/000232.pdf",
    "pages": "869-875",
    "zenodo_id": 4245572,
    "dblp_key": null,
    "Abstract": "Many musics across the world are structured around multiple modes, which hold a middle ground between scales and melodies. We study whether we can classify mode in a corpus of 20,865 medieval plainchant melodies from the Cantus database. We revisit the traditional 'textbook' classification approach (using the final, the range and initial note) as well as the only prior computational study we are aware of, which uses pitch profiles. Both approaches work well, but largely reduce modes to scales and ignore their melodic character. Our main contribution is a model that reaches 93-95% F1 score on mode classification, compared to 86-90% using traditional pitch-based musicological methods. Importantly, it reaches 81-83% even when we discard all absolute pitch information and reduce a melody to its contour. The model uses tf-idf vectors and strongly depends on the choice of units: i.e., how the melody is segmented. If we borrow the syllable or word structure from the lyrics, the model outperforms all of our baselines. This suggests that, like language, music is made up of 'natural' units, in our case between the level of notes and complete phrases, a finding that may well be useful in other musics.",
    "extra": {
      "takeaway": "We present a highly accurate model of mode classification in plainchant that also reveals the importance of natural units."
    }
  },
  {
    "title": "Data Quality Matters: Iterative Corrections on a Corpus of Mendelssohn String Quartets and Implications for MIR Analysis",
    "author": [
      " Jacob deGroot-Maggetti",
      " Timothy R  de Reuse",
      " Laurent  Feisthauer",
      " Samuel  Howes",
      " Yaolong  Ju",
      " Suzuka  Kokubu",
      " Sylvain  Margot",
      " N\u00E9stor  N\u00E1poles L\u00F3pez",
      " Finn  Upham"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245460",
    "url": "https://doi.org/10.5281/zenodo.4245460",
    "ee": "http://archives.ismir.net/ismir2020/paper/000235.pdf",
    "pages": "432-438",
    "zenodo_id": 4245460,
    "dblp_key": null,
    "Abstract": "In this paper, we describe a workflow of successive corrections on Optical Music Recognition (OMR) generated MusicXML files and their respective outputs under Music Information Retrieval tasks. The original OMR-generated files of six Mendelssohn String Quartets were initially corrected by individual members of this interdisciplinary group, then reviewed by others to further standardize the quality and music analysis priorities of the team.  Four MIR tasks are applied to each round of corrections on this collection: cadence detection, chord labeling, key finding, and monophonic pattern discovery. We measure changes in the outputs of these four MIR tasks from one round of correction to the next in order to evaluate the impact of corrections. Results show that expert revision is more beneficial to some MIR tasks than to others.  The resulting corpus of curated MusicXML files is available as an open-source repository under a Creative Commons Attribution 4.0 International License for further MIR research.",
    "extra": {
      "takeaway": "When using OMR to build datasets for MIR research, additional rounds of manual review benefit some tasks more than others."
    }
  },
  {
    "title": "CONLON: A Pseudo-Song Generator Based on a New Pianoroll, Wasserstein Autoencoders, and Optimal Interpolations",
    "author": [
      " Luca Angioloni",
      " Valentijn  Borghuis",
      " Lorenzo   Brusci",
      " Paolo  Frasconi"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245576",
    "url": "https://doi.org/10.5281/zenodo.4245576",
    "ee": "http://archives.ismir.net/ismir2020/paper/000236.pdf",
    "pages": "876-883",
    "zenodo_id": 4245576,
    "dblp_key": null,
    "Abstract": "We introduce CONLON, a pattern-based MIDI generation method that employs a new lossless pianoroll-like data description in which velocities and durations are stored in separate channels. CONLON uses Wasserstein autoencoders as the underlying generative model. Its generation strategy is similar to interpolation, where MIDI pseudo-songs are obtained by concatenating patterns decoded from smooth trajectories in the embedding space, but aims to produce a smooth result in the pattern space by computing optimal trajectories as the solution of a widest-path problem. A set of surveys enrolling 69 professional musicians shows that our system, when trained on datasets of carefully selected and coherent patterns, is able to produce pseudo-songs that are musically consistent and potentially useful for professional musicians. Additional materials can be found at https://paolo-f.github.io/CONLON/ .",
    "extra": {
      "takeaway": "A combination of novel ingredients enables effective generation of mainstream music that is appreciated by musicians"
    }
  },
  {
    "title": "Combining musical features for cover detection",
    "author": [
      " Guillaume Doras",
      " Furkan  Yesiler",
      " Joan  Serra",
      " Emilia  G\u00F3mez",
      " Geoffroy  Peeters"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245424",
    "url": "https://doi.org/10.5281/zenodo.4245424",
    "ee": "http://archives.ismir.net/ismir2020/paper/000239.pdf",
    "pages": "279-286",
    "zenodo_id": 4245424,
    "dblp_key": null,
    "Abstract": "Recent works have addressed the automatic cover detection problem from a metric learning perspective. They employ different input representations, aiming to exploit melodic or harmonic characteristics of songs and yield promising performances. In this work, we propose a comparative study of these different representations and show that systems combining melodic and harmonic features drastically outperform those relying on a single input representation. We illustrate how these features complement each other with both quantitative and qualitative analyses. We finally investigate various fusion schemes and propose methods yielding state-of-the-art performances on two publicly-available large datasets.",
    "extra": {
      "takeaway": "We present a cover detection system combining melodic and harmonic features yielding SoA results on large public datasets."
    }
  },
  {
    "title": "The Freesound Loop Dataset and Annotation Tool",
    "author": [
      " Antonio Ramires",
      " Frederic  Font",
      " Dmitry  Bogdanov",
      " Jordan B. L.  Smith",
      " Yi-Hsuan  Yang",
      " Joann  Ching",
      " Bo-Yu  Chen",
      " Yueh-Kao  Wu",
      " Hsu  Wei-Han",
      " Xavier  Serra"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245430",
    "url": "https://doi.org/10.5281/zenodo.4245430",
    "ee": "http://archives.ismir.net/ismir2020/paper/000241.pdf",
    "pages": "287-294",
    "zenodo_id": 4245430,
    "dblp_key": null,
    "Abstract": "Music loops are essential ingredients in electronic music production, and there is a high demand for pre-recorded loops in a variety of styles. Several commercial and community databases have been created to meet this demand, but most of them are not suitable for research due to their strict licensing. In this paper, we present the Freesound Loop Dataset (FSLD), a new large-scale dataset of music loops annotated by experts. The loops originate from Freesound, a community database of audio recordings released under Creative Commons licenses, so the audio in our dataset may be redistributed. The annotations include instrument, meter, key and genre tags. We describe the methodology used to assemble and annotate the data, and report on the distribution of tags in the data and inter-annotator agreement. We also present to the community an online loop annotator tool that we developed. To illustrate the usefulness of FSLD, we present short case studies on using it to estimate tempo and key, generate new loops, and evaluate a loop separation algorithm. We anticipate that the community will find yet more uses for the data, in applications from automatic loop characterisation to algorithmic composition.",
    "extra": {
      "takeaway": "We present a new dataset of audio loops from Freesound with annotations on instrumentation, tempo, key and genre."
    }
  },
  {
    "title": "Less is more: Faster and better music version identification with embedding distillation",
    "author": [
      " Furkan Yesiler",
      " Joan  Serra",
      " Emilia  G\u00F3mez"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245570",
    "url": "https://doi.org/10.5281/zenodo.4245570",
    "ee": "http://archives.ismir.net/ismir2020/paper/000244.pdf",
    "pages": "884-892",
    "zenodo_id": 4245570,
    "dblp_key": null,
    "Abstract": "Version identification systems aim to detect different renditions of the same underlying musical composition (loosely called cover songs). By learning to encode entire recordings into plain vector embeddings, recent systems have made significant progress in bridging the gap between accuracy and scalability, which has been a key challenge for nearly two decades. In this work, we propose to further narrow this gap by employing a set of data distillation techniques that reduce the embedding dimensionality of a pre-trained state-of-the-art model. We compare a wide range of techniques and propose new ones, from classical dimensionality reduction to more sophisticated distillation schemes. With those, we obtain 99% smaller embeddings that, moreover, yield up to a 3% accuracy increase. Such small embeddings can have an important impact in retrieval time, up to the point of making a real-world system practical on a standalone laptop.",
    "extra": {
      "takeaway": "We propose a set of methods to reduce the size of the embeddings obtained with pre-trained version retrieval systems."
    }
  },
  {
    "title": "Should we consider the users in contextual music auto-tagging models?",
    "author": [
      " Karim M. Ibrahim",
      " Elena V.  Epure",
      " Geoffroy  Peeters",
      " Ga\u00EBl  Richard"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245426",
    "url": "https://doi.org/10.5281/zenodo.4245426",
    "ee": "http://archives.ismir.net/ismir2020/paper/000245.pdf",
    "pages": "295-301",
    "zenodo_id": 4245426,
    "dblp_key": null,
    "Abstract": "Music tags are commonly used to describe and categorize music. Various auto-tagging models and datasets have been proposed for the automatic music annotation with tags. However, the past approaches often neglect the fact that many of these tags largely depend on the user, especially the tags related to the context of music listening. In this paper, we address this problem by proposing a user-aware music auto-tagging system and evaluation protocol. Specifically, we use both the audio content and user information extracted from the user listening history to predict contextual tags for a given user/track pair. We propose a new dataset of music tracks annotated with contextual tags per user. We compare our model to the traditional audio-based model and study the influence of user embeddings on the classification quality. Our work shows that explicitly modeling the user listening history into the automatic tagging process could lead to more accurate estimation of contextual tags.",
    "extra": {
      "takeaway": "We study the influence of including user information on the performance of music auto-taggers in predicting contextual tags."
    }
  },
  {
    "title": "Exploring Acoustic Similarity for Novel Music Recommendation",
    "author": [
      " Derek S. Cheng",
      " Thorsten  Joachims",
      " Douglas  Turnbull"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245500",
    "url": "https://doi.org/10.5281/zenodo.4245500",
    "ee": "http://archives.ismir.net/ismir2020/paper/000246.pdf",
    "pages": "583-589",
    "zenodo_id": 4245500,
    "dblp_key": null,
    "Abstract": "Most commercial music services rely on collaborative filtering to recommend artists and songs. While this method is effective for popular artists with large fanbases, it can present difficulties for recommending novel, lesser known artists due to a relative lack of user preference data. In this paper, we therefore seek to understand how content-based approaches can be used to more effectively recommend songs from these lesser known artists. Specifically, we conduct a user study to answer three questions. Firstly, do most users agree which songs are most acoustically similar? Secondly, is acoustic similarity a good proxy for how an individual might construct a playlist or recommend music to a friend? Thirdly, if so, can we find acoustic features that are related to human judgments of acoustic similarity? To answer these questions, our study asked 117 test subjects to compare two unknown candidate songs relative to a third known reference song. Our findings show that 1) judgments about acoustic similarity are fairly consistent, 2) acoustic similarity is highly correlated with playlist selection and recommendation, but not necessarily personal preference, and 3) we identify a subset of acoustic features from the Spotify Web API that is particularly predictive of human similarity judgments.",
    "extra": {
      "takeaway": "People judge acoustic similarity consistently, and it is also highly correlated with recommendation and playlist selection."
    }
  },
  {
    "title": "Generating Music with a Self-Correcting Non-Chronological Autoregressive Model",
    "author": [
      " Wayne Chi",
      " Prachi  Kumar",
      " Suri  Yaddanapudi",
      " Suresh  Rahul",
      " Umut  Isik"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245578",
    "url": "https://doi.org/10.5281/zenodo.4245578",
    "ee": "http://archives.ismir.net/ismir2020/paper/000247.pdf",
    "pages": "893-900",
    "zenodo_id": 4245578,
    "dblp_key": null,
    "Abstract": "We describe a novel approach for generating music using a self-correcting, non-chronological, autoregressive model. We represent music as a sequence of edit events, each of which denotes either the addition or removal of a note---even a note previously generated by the model. During inference, we generate one edit event at a time using direct ancestral sampling. Our approach allows the model to fix previous mistakes such as incorrectly sampled notes and prevent accumulation of errors which autoregressive models are prone to have. Another benefit of our approach is a finer degree of control during human and AI collaboration as our approach is notewise online. We show through quantitative metrics and human survey evaluation that our approach generates better results than orderless NADE and Gibbs sampling approaches.",
    "extra": {
      "takeaway": "We describe an autoregressive approach to generating music that is able to self-correct previous predictions."
    }
  },
  {
    "title": "Multiple F0 Estimation in Vocal Ensembles using Convolutional Neural Networks",
    "author": [
      " Helena Cuesta",
      " Brian  McFee",
      " Emilia  G\u00F3mez"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245434",
    "url": "https://doi.org/10.5281/zenodo.4245434",
    "ee": "http://archives.ismir.net/ismir2020/paper/000254.pdf",
    "pages": "302-309",
    "zenodo_id": 4245434,
    "dblp_key": null,
    "Abstract": "This paper addresses the extraction of multiple F0 values from polyphonic and a cappella vocal performances using convolutional neural networks (CNNs). We address the major challenges of ensemble singing, i.e., all melodic sources are vocals and singers sing in harmony. We build upon an existing architecture to produce a pitch salience function of the input signal, where the harmonic constant-Q transform (HCQT) and its associated phase differentials are used as an input representation.  The pitch salience function is subsequently thresholded to obtain a multiple F0 estimation output. For training, we build a dataset that comprises several multi-track datasets of vocal quartets with F0 annotations. This work proposes and evaluates a set of CNNs for this task in diverse scenarios and data configurations, including recordings with additional reverb. Our models outperform a state-of-the-art method intended for the same music genre when evaluated with an increased F0 resolution, as well as a general-purpose method for multi-F0 estimation. We conclude with a discussion on future research directions.",
    "extra": {
      "takeaway": "We present a set of CNNs for multiple F0 estimation in polyphonic vocal music, i.e., SATB vocal quartets."
    }
  },
  {
    "title": "DRUMGAN: SYNTHESIS OF DRUM SOUNDS WITH TIMBRAL FEATURE CONDITIONING USING GENERATIVE ADVERSARIAL NETWORKS",
    "author": [
      " Javier Nistal",
      " Stefan  Lattner",
      " Ga\u00EBl  Richard"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245504",
    "url": "https://doi.org/10.5281/zenodo.4245504",
    "ee": "http://archives.ismir.net/ismir2020/paper/000255.pdf",
    "pages": "590-597",
    "zenodo_id": 4245504,
    "dblp_key": null,
    "Abstract": "Synthetic creation of drum sounds (e.g., in drum machines) is commonly performed using analog or digital synthesis, allowing a musician to sculpt the desired timbre modifying various parameters. Typically, such parameters control low-level features of the sound and often have no musical meaning or perceptual correspondence. With the rise of Deep Learning, data-driven processing of audio emerges as an alternative to traditional signal processing. This new paradigm allows controlling the synthesis process through learned high-level features or by conditioning a model on musically relevant information. In this paper, we apply a Generative Adversarial Network to the task of audio synthesis of drum sounds. By conditioning the model on perceptual features computed with a publicly available feature-extractor, intuitive control is gained over the generation process. The experiments are carried out on a large collection of kick, snare, and cymbal sounds. We show that, compared to a specific prior work based on a U-Net architecture, our approach considerably improves the quality of the generated drum samples, and that the conditional input indeed shapes the perceptual characteristics of the sounds. Also, we provide audio examples and release the code used in our experiments.",
    "extra": {
      "takeaway": "We present a conditional GAN for the synthesis of drum sounds offering high-level control over perceptual timbral features"
    }
  },
  {
    "title": "Microtask Crowdsourcing for Music Score Transcriptions: An Experiment with Error Detection",
    "author": [
      " Ioannis Petros Samiotis",
      " Sihang  Qiu",
      " Andrea  Mauri",
      " Cynthia C. S.  Liem",
      " Christoph  Lofi",
      " Alessandro   Bozzon"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245580",
    "url": "https://doi.org/10.5281/zenodo.4245580",
    "ee": "http://archives.ismir.net/ismir2020/paper/000257.pdf",
    "pages": "901-907",
    "zenodo_id": 4245580,
    "dblp_key": null,
    "Abstract": "Human annotation is still an essential part of modern transcription workflows for digitizing music scores, either as a standalone approach where a single expert annotator transcribes a complete score, or for supporting an automated Optical Music Recognition (OMR) system. Research on human computation has shown the effectiveness of crowdsourcing for scaling out human work by defining a large number of microtasks which can easily be distributed and executed. However, microtask design for music transcription is a research area that remains unaddressed. This paper focuses on the design of a crowdsourcing task to detect errors in a score transcription which can be deployed in either automated or human-driven transcription workflows. We conduct an experiment where we study two design parameters: 1) the size of the score to be annotated and 2) the modality in which it is presented in the user interface. We analyze the performance and reliability of non-specialised crowdworkers on Amazon Mechanical Turk with respect to these design parameters, differentiated by worker experience and types of transcription errors. Results are encouraging, and pave the way for scalable and efficient crowd-assisted music transcription systems.",
    "extra": {
      "takeaway": "With proper microtask design, non-expert crowd can prove to be a valuable source to improve music score transcriptions"
    }
  },
  {
    "title": "A Deep Learning Based Analysis-Synthesis Framework For Unison Singing",
    "author": [
      " Pritish Chandna",
      " Helena  Cuesta",
      " Emilia  G\u00F3mez"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245502",
    "url": "https://doi.org/10.5281/zenodo.4245502",
    "ee": "http://archives.ismir.net/ismir2020/paper/000259.pdf",
    "pages": "598-604",
    "zenodo_id": 4245502,
    "dblp_key": null,
    "Abstract": "Unison singing is the name given to an ensemble of singers simultaneously singing the same melody and lyrics. While each individual singer in a unison sings the same principle melody, there are slight timing and pitch deviations between the singers, which, along with the ensemble of timbres, give the listener a perceived sense of \"unison\". In this paper, we present a study of unison singing in the context of choirs; utilising some recently proposed deep-learning based methodologies, we analyse the fundamental frequency (F0) distribution of the individual singers in recordings of unison mixtures. Based on the analysis, we propose a system for synthesising a unison signal from an a cappella input and a single voice prototype representative of a unison mixture. We use subjective listening test to evaluate perceptual factors of our proposed system for synthesis, including quality, adherence to the melody as well the degree of perceived unison.",
    "extra": {
      "takeaway": "Analysis of unison mixtures and synthesis of unison mixture from a single voice and a single voice prototype for unison"
    }
  },
  {
    "title": "Essentia.js: A JavaScript Library for Music and Audio Analysis on the Web",
    "author": [
      " Albin Correya",
      " Dmitry  Bogdanov",
      " Luis  Joglar-Ongay",
      " Xavier  Serra"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245510",
    "url": "https://doi.org/10.5281/zenodo.4245510",
    "ee": "http://archives.ismir.net/ismir2020/paper/000260.pdf",
    "pages": "605-612",
    "zenodo_id": 4245510,
    "dblp_key": null,
    "Abstract": "Open-source software libraries for audio/music analysis and feature extraction have a significant impact on the development of Audio Signal Processing and Music Information Retrieval (MIR) systems. Despite the abundance of such tools on the native computing platforms, there is a lack of an extensive and easy-to-use reference library for audio feature extraction on the Web. In this paper, we present Essentia.js, an open-source JavaScript (JS) library for audio and music analysis on both web clients and JS-based servers. Along with the Web Audio API, it can be used for efficient and robust real-time audio feature extraction on the web browsers. Essentia.js is modular, lightweight, and easy-to-use, deploy, maintain, and integrate into the existing plethora of JS libraries and Web technologies. It is powered by a WebAssembly back-end of the Essentia C++ library, which facilitates a JS interface to a wide range of low-level and high-level audio features. It also provides a higher-level JS API and add-on MIR utility modules along with extensive documentation, usage examples, and tutorials. We benchmark the proposed library on two popular web browsers, Node.js engine, and Android devices, comparing it to the native performance of Essentia and Meyda JS library.",
    "extra": {
      "takeaway": "We present Essentia.js, an open-source JavaScript library for audio and music analysis, powered by WebAssembly."
    }
  },
  {
    "title": "Detecting Collaboration Profiles in Success-based Music Genre Networks",
    "author": [
      " Gabriel Oliveira",
      " Mariana  Santos",
      " Danilo B.  Seufitelli",
      " Anisio  Lacerda",
      " Mirella M.  Moro"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245534",
    "url": "https://doi.org/10.5281/zenodo.4245534",
    "ee": "http://archives.ismir.net/ismir2020/paper/000275.pdf",
    "pages": "726-732",
    "zenodo_id": 4245534,
    "dblp_key": null,
    "Abstract": "We analyze and identify collaboration profiles in success-based music genre networks. Such networks are built upon data recently collected from both global and regional Spotify weekly charts.  Overall, our findings reveal an increase in the number of distinct successful genres from high-potential markets, pointing out that local repertoire is more important than ever on building the global music ecosystem. We also detect collaboration patterns mapped into four different profiles: Solid, Regular, Bridge and Emerging, wherein the two first depict higher average success. These findings indicate  great opportunities for the music industry by revealing the driving power of inter-genre collaborations within regional and global markets.",
    "extra": {
      "takeaway": "Using Social and Cluster Analyses, we reveal the driving power of inter-genre collaborations for regional and global markets."
    }
  },
  {
    "title": "Deep Learning Based Source Separation Applied To Choir Ensembles",
    "author": [
      " Darius Petermann",
      " Pritish  Chandna",
      " Helena  Cuesta",
      " Jordi  Bonada",
      " Emilia  G\u00F3mez"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245536",
    "url": "https://doi.org/10.5281/zenodo.4245536",
    "ee": "http://archives.ismir.net/ismir2020/paper/000276.pdf",
    "pages": "733-739",
    "zenodo_id": 4245536,
    "dblp_key": null,
    "Abstract": "Choral singing is a widely practiced form of ensemble singing wherein a group of people sing simultaneously in polyphonic harmony. The most commonly practiced setting for choir ensembles consists of four parts; Soprano, Alto, Tenor and Bass (SATB), each with its own range of fundamental frequencies (F0s). The task of source separation for this choral setting entails separating the SATB mixture into the constituent parts. Source separation for musical mixtures is well studied and many deep learning based methodologies have been proposed for the same. However, most of the research has been focused on a typical case which consists in separating vocal, percussion and bass sources from a mixture, each of which has a distinct spectral structure. In contrast, the simultaneous and harmonic nature of ensemble singing leads to high structural similarity and overlap between the spectral components of the sources in a choral mixture, making source separation for choirs a harder task than the typical case. This, along with the lack of an appropriate consolidated dataset has led to a dearth of research in the field so far. In this paper we first assess how well some of the recently developed methodologies for musical source separation perform for the case of SATB choirs. We then propose a novel domain-specific adaptation for conditioning the recently proposed U-Net architecture for musical source separation using the fundamental frequency contour of each of the singing groups and demonstrate that our proposed approach surpasses results from domain-agnostic architectures.",
    "extra": {
      "takeaway": "Adaptation and evaluation of recently proposed deep learning based source separation methodologies for SATB choirs"
    }
  },
  {
    "title": "On the Characterization of Expressive Performance in Classical Music: First Results of the Con Espressione Game",
    "author": [
      " Carlos Eduardo Cancino-Chac\u00F3n",
      " Silvan  Peter",
      " Shreyan  Chowdhury",
      " Anna  Aljanaki",
      " Gerhard  Widmer"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245506",
    "url": "https://doi.org/10.5281/zenodo.4245506",
    "ee": "http://archives.ismir.net/ismir2020/paper/000279.pdf",
    "pages": "613-620",
    "zenodo_id": 4245506,
    "dblp_key": null,
    "Abstract": "A piece of music can be expressively performed, or interpreted, in a variety of ways. With the help of an online questionnaire, the Con Espressione Game, we collected some 1,500 descriptions of expressive character relating to 45 performances of 9 excerpts from classical piano pieces, played by different famous pianists. More specifically, listeners were asked to describe, using freely chosen words (preferably: adjectives), how they perceive the expressive character of the different performances. In this paper, we offer a first account of this new data resource for expressive performance research, and provide an exploratory analysis, addressing three main questions: (1) how similarly do different listeners describe a performance of a piece? (2) what are the main dimensions (or axes) for expressive character emerging from this?; and (3) how do measurable parameters of a performance (e.g., tempo, dynamics) and mid- and high-level features that can be predicted by machine learning models (e.g., articulation, arousal) relate to these expressive dimensions? The dataset that we publish along with this paper was enriched by adding hand-corrected score-to-performance alignments, as well as descriptive audio features such as tempo and dynamics curves.",
    "extra": {
      "takeaway": "We present a dataset for studying the way listeners describe the expressive character of the different performances."
    }
  },
  {
    "title": "Few-shot Drum Transcription in Polyphonic Music",
    "author": [
      " Yu Wang",
      " Justin  Salamon",
      " Mark  Cartwright",
      " Nicholas J.  Bryan",
      " Juan P.  Bello"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245384",
    "url": "https://doi.org/10.5281/zenodo.4245384",
    "ee": "http://archives.ismir.net/ismir2020/paper/000281.pdf",
    "pages": "117-124",
    "zenodo_id": 4245384,
    "dblp_key": null,
    "Abstract": "Data-driven approaches to automatic drum transcription (ADT) are often limited to a predefined, small vocabulary of percussion instrument classes. Such models cannot recognize out-of-vocabulary classes nor are they able to adapt to finer-grained vocabularies. In this work, we address open vocabulary ADT by introducing few-shot learning to the task. We train a Prototypical Network on a synthetic dataset and evaluate the model on multiple real-world ADT datasets with polyphonic accompaniment. We show that, given just a handful of selected examples at inference time, we can match and in some cases outperform a state-of-the-art supervised ADT approach under a fixed vocabulary setting. At the same time, we show that our model can successfully generalize to finer-grained or extended vocabularies unseen during training, a scenario where supervised approaches cannot operate at all. We provide a detailed analysis of our experimental results, including a breakdown of performance by sound class and by polyphony.",
    "extra": {
      "takeaway": "We propose few-shot learning to tackle open-vocabulary automatic drum transcription in polyphonic music."
    }
  },
  {
    "title": "dMelodies: A Music Dataset for Disentanglement Learning",
    "author": [
      " Ashis Pati",
      " Siddharth Kumar  Gururani",
      " Alexander  Lerch"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245382",
    "url": "https://doi.org/10.5281/zenodo.4245382",
    "ee": "http://archives.ismir.net/ismir2020/paper/000300.pdf",
    "pages": "125-133",
    "zenodo_id": 4245382,
    "dblp_key": null,
    "Abstract": "Representation learning focused on disentangling the underlying factors of variation in given data has become an important area of research in machine learning. However, most of the studies in this area have relied on datasets from the computer vision domain and thus, have not been readily extended to music. In this paper, we present a new symbolic music dataset that will help researchers working on disentanglement problems demonstrate the efficacy of their algorithms on diverse domains. This will also provide a means for evaluating algorithms specifically designed for music. To this end, we create a dataset comprising of 2-bar monophonic melodies where each melody is the result of a unique combination of nine latent factors that span ordinal, categorical, and binary types. The dataset is large enough (\u007E 1.3 million data points) to train and test deep networks for disentanglement learning. In addition, we present benchmarking experiments using popular unsupervised dis- entanglement algorithms on this dataset and compare the results with those obtained on an image-based dataset.",
    "extra": {
      "takeaway": "Presents an algorithmically created music dataset with a diverse set of latent factors designed for disentanglement learning."
    }
  },
  {
    "title": "Metric learning vs classification for disentangled music representation learning",
    "author": [
      " Jongpil Lee",
      " Nicholas J.  Bryan",
      " Justin  Salamon",
      " Zeyu  Jin",
      " Juhan  Nam"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245468",
    "url": "https://doi.org/10.5281/zenodo.4245468",
    "ee": "http://archives.ismir.net/ismir2020/paper/000304.pdf",
    "pages": "439-445",
    "zenodo_id": 4245468,
    "dblp_key": null,
    "Abstract": "Deep representation learning offers a powerful paradigm for mapping input data onto an organized embedding space and is useful for many music information retrieval tasks. Two central methods for representation learning include deep metric learning and classification, both having the same goal of learning a representation that can generalize well across tasks. Along with generalization, the emerging concept of disentangled representations is also of great interest, where multiple semantic concepts (e.g., genre, mood, instrumentation) are learned jointly but remain separable in the learned representation space. In this paper we present a single representation learning framework that elucidates the relationship between metric learning, classification, and disentanglement in a holistic manner. For this, we (1) outline past work on the relationship between metric learning and classification, (2) extend this relationship to multi-label data by exploring three different learning approaches and their disentangled versions, and (3) evaluate all models on four tasks (training time, similarity retrieval, auto-tagging, and triplet prediction). We find that classification-based models are generally advantageous for training time, similarity retrieval, and auto-tagging, while deep metric learning exhibits better performance for triplet-prediction. Finally, we show that our proposed approach yields state-of-the-art results for music auto-tagging.",
    "extra": {
      "takeaway": "We propose a unified framework for metric learning and classification for disentangled music representation learning."
    }
  },
  {
    "title": "Score-informed Networks for Music Performance Assessment",
    "author": [
      " Jiawen Huang",
      " Yun-Ning  Hung",
      " Ashis  Pati",
      " Siddharth Kumar  Gururani",
      " Alexander  Lerch"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245582",
    "url": "https://doi.org/10.5281/zenodo.4245582",
    "ee": "http://archives.ismir.net/ismir2020/paper/000307.pdf",
    "pages": "908-915",
    "zenodo_id": 4245582,
    "dblp_key": null,
    "Abstract": "The assessment of music performances in most cases takes into account the underlying musical score being performed. While there have been several automatic approaches for objective music performance assessment (MPA) based on extracted features from both the performance audio and the score, deep neural network-based methods incorporating score information into MPA models have not yet been investigated. In this paper, we introduce three different models capable of score-informed performance assessment. These are (i) a convolutional neural network that utilizes a simple time-series input comprising of aligned pitch contours and score, (ii) a joint embedding model which learns a joint latent space for pitch contours and scores, and (iii) a distance matrix-based convolutional neural network which utilizes patterns in the distance matrix between pitch contours and musical score to predict assessment ratings. Our results provide insights into the suitability of different architectures and input representations and demonstrate the benefits of score-informed models as compared to score-independent models.",
    "extra": {
      "takeaway": "In this paper, we introduce three different models capable of score-informed performance assessment."
    }
  },
  {
    "title": "The multiple voices of musical emotions: source separation for improving music emotion recognition models and their interpretability",
    "author": [
      " Jacopo de Berardinis",
      " Angelo  Cangelosi",
      " Eduardo  Coutinho"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245428",
    "url": "https://doi.org/10.5281/zenodo.4245428",
    "ee": "http://archives.ismir.net/ismir2020/paper/000308.pdf",
    "pages": "310-317",
    "zenodo_id": 4245428,
    "dblp_key": null,
    "Abstract": "Despite the manifold developments in music emotion recognition and related areas, estimating the emotional impact of music still poses many challenges. These are often associated to the complexity of the acoustic codes to emotion and the lack of large amounts of data with robust golden standards. In this paper, we propose a new computational model (EmoMucs) that considers the role of different musical voices in the prediction of the emotions induced by music. We combine source separation algorithms for breaking up music signals into independent song elements (vocals, bass, drums, other) and end-to-end state-of-the-art machine learning techniques for feature extraction and emotion modelling (valence and arousal regression). Through a series of computational experiments on a benchmark dataset using source-specialised models trained independently and different fusion strategies, we demonstrate that EmoMucs outperforms state-of-the-art approaches with the advantage of providing insights into the relative contribution of different musical elements to the emotions perceived by listeners.",
    "extra": {
      "takeaway": "Music source separation for emotion recognition brings more accurate predictions and increased interpretability."
    }
  },
  {
    "title": "Hierarchical Timbre-painting and Articulation Generation",
    "author": [
      " Michael M. Michelashvili",
      " Lior  Wolf"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245584",
    "url": "https://doi.org/10.5281/zenodo.4245584",
    "ee": "http://archives.ismir.net/ismir2020/paper/000310.pdf",
    "pages": "916-922",
    "zenodo_id": 4245584,
    "dblp_key": null,
    "Abstract": "We present a fast and high-fidelity method for music generation, based on specified f0 and loudness, such that the synthesized audio mimics the timbre and articulation of a target instrument. The generation process consists of learned source-filtering networks, which reconstruct the signal at increasing resolutions. The model optimizes a multi-resolution spectral loss as the reconstruction loss, an adversarial loss to make the audio sound more realistic, and a perceptual f0 loss to align the output to the desired input pitch contour. The proposed architecture enables high-quality fitting of an instrument, given a sample that can be as short as a few minutes, and the method demonstrates state-of-the-art timbre transfer capabilities. Code and audio samples are shared at https://github.com/mosheman5/timbre_painting.",
    "extra": {
      "takeaway": "Combining Hierarchical source-filtering networks for efficient and high-fidelity audio generation"
    }
  },
  {
    "title": "User Insights on Diversity in Music Recommendation Lists",
    "author": [
      " Kyle Robinson",
      " Dan  Brown",
      " Markus  Schedl"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245464",
    "url": "https://doi.org/10.5281/zenodo.4245464",
    "ee": "http://archives.ismir.net/ismir2020/paper/000311.pdf",
    "pages": "446-453",
    "zenodo_id": 4245464,
    "dblp_key": null,
    "Abstract": "While many researchers have proposed various ways of quantifying recommendation list diversity, these approaches have had little input from users on their own perceptions and preferences in seeking diversity. Through an exploratory user study, we provide a better understanding of how users view the concept of diversity in music recommendations, and how they might optimise levels of intra-list diversity themselves. In our study, 17 participants interacted with and rated the suggestions from two different recommendation systems. One provided static top-7 collaborative filtering recommendations, and the other provided an interactive slider to re-rank these recommendations based on a continuous diversity scale. We also asked participants a series of free-form questions on music discovery and diversity in semi-structured interviews. User-preferred levels of diversity varied widely both within and between subjects. Although most users agreed that diversity is beneficial in music discovery, they also noted a risk of dissatisfaction from too much diversity. A key finding is that preference for diversification was often linked to user mood. Participants also expressed a clear distinction between diversity within existing preferences, and outside of existing preferences. These ideas of inner and outer diversity are not well defined within the bounds of current diversity metrics, and we discuss their implications.",
    "extra": {
      "takeaway": "When considering levels of diversity in music recommendations it is important to consider user preference and mood."
    }
  },
  {
    "title": "From Music Ontology towards Ethno-Music-Ontology",
    "author": [
      " Polina Proutskova",
      " Anja  Volk",
      " Peyman  Heydarian",
      " Gyorgy  Fazekas"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245586",
    "url": "https://doi.org/10.5281/zenodo.4245586",
    "ee": "http://archives.ismir.net/ismir2020/paper/000323.pdf",
    "pages": "923-931",
    "zenodo_id": 4245586,
    "dblp_key": null,
    "Abstract": "This paper presents exploratory work investigating the suitability of the Music Ontology - the most widely used formal specification of the music domain - for modelling non-Western musical traditions. Four contrasting case studies from a variety of musical cultures are analysed: Dutch folk song research, reconstructive performance of rural Russian traditions, contemporary performance and composition of Persian classical music, and recreational use of a personal world music collection. We propose semantic models describing the respective do- mains and examine the applications of the Music Ontology for these case studies: which concepts can be successfully reused, where they need adjustments, and which parts of the reality in these case studies are not covered by the Mu- sic Ontology. The variety of traditions, contexts and modelling goals covered by our case studies sheds light on the generality of the Music Ontology and on the limits of generalisation \"for all musics\" that could be aspired for on the Semantic Web.",
    "extra": {
      "takeaway": "Investigating whether the Music Ontology can be employed to model use cases from non-Western musical traditions"
    }
  },
  {
    "title": "A Combination of Local Approaches for Hierarchical Music Genre Classification",
    "author": [
      " Antonio R. Parmezan",
      " Diego  Furtado Silva",
      " Gustavo  Batista"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245540",
    "url": "https://doi.org/10.5281/zenodo.4245540",
    "ee": "http://archives.ismir.net/ismir2020/paper/000324.pdf",
    "pages": "740-747",
    "zenodo_id": 4245540,
    "dblp_key": null,
    "Abstract": "Labeling a music recording according to its genre is an intuitive and familiar way to describe its content. Music genres are valuable information, especially for music organization, personalized listening experience, and playlist generation. Automatically classifying music genres is a challenging endeavor due to the inherent ambiguity and subjectivity. Most efforts on music genre classification consider the complete independence between labels. However, music genres typically respect a hierarchical structure based on the influences or origins of each style. Conversely, many of the methods available for hierarchical classification are based on assumptions about the class hierarchy, such as the need for multiple children in each hierarchy's node, which may limit their use in music applications. Also, the local classifier per node approach that would be the most suitable for this scenario is costly regarding time and memory. In this paper, we present two local hierarchical classification approaches and show how to combine them to obtain a single one that is more robust and faithful to the music genre classification scenario. We evaluate our proposal in a music dataset hierarchically labeled with 120 music genres. As shown, compared to state-of-the-art approaches, our approach has a lower computational cost and can achieve competitive performances.",
    "extra": {
      "takeaway": "Two hierarchical classification approaches are combined to obtain a single one aimed at the musical genre classification."
    }
  },
  {
    "title": "Modeling Music and Code Knowledge to Support a Co-creative AI Agent for Education",
    "author": [
      " Jason Smith",
      " Erin  Truesdell",
      " Jason  Freeman",
      " Brian  Magerko",
      " Kristy  Boyer",
      " Tom  Mcklin"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245386",
    "url": "https://doi.org/10.5281/zenodo.4245386",
    "ee": "http://archives.ismir.net/ismir2020/paper/000326.pdf",
    "pages": "134-141",
    "zenodo_id": 4245386,
    "dblp_key": null,
    "Abstract": "EarSketch is an online environment for learning introductory computing concepts through code-driven, sample-based music production. This paper details the design and implementation of a module to perform code and music analyses on projects on the EarSketch platform. This analysis module combines inputs in the form of symbolic metadata, audio feature analysis, and user code to produce comprehensive models of user projects. The module performs a detailed analysis of the abstract syntax tree of a user's code to model use of computational concepts. It uses music information retrieval (MIR) and symbolic metadata to analyze users' musical design choices. These analyses produce a model containing users' coding and musical decisions, as well as qualities of the algorithmic music created by those decisions. The models produced by this module will support future development of CAI, a Co-creative Artificial Intelligence. CAI is designed to collaborate with learners and promote increased competency and engagement with topics in the EarSketch curriculum. Our module combines code analysis and MIR to further the educational goals of CAI and EarSketch and to explore the application of multimodal analysis tools to education.",
    "extra": {
      "takeaway": "Addition to the EarSketch educational tool, analyzes user code and composition to model understanding of code/music concepts."
    }
  },
  {
    "title": "Multitask Learning  for Instrument Activation Aware Music Source Separation",
    "author": [
      " Yun-Ning Hung",
      " Alexander  Lerch"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245548",
    "url": "https://doi.org/10.5281/zenodo.4245548",
    "ee": "http://archives.ismir.net/ismir2020/paper/000334.pdf",
    "pages": "748-755",
    "zenodo_id": 4245548,
    "dblp_key": null,
    "Abstract": "Music source separation is a core task in music information retrieval which has seen a dramatic improvement in the past years. Nevertheless, most of the existing systems focus exclusively on the problem of source separation itself and ignore the utilization of other~---possibly related---~MIR tasks which could lead to additional quality gains. In this work, we propose a novel multitask structure to investigate using instrument activation information to improve source separation performance. Furthermore, we investigate our system on six independent instruments, a more realistic scenario than the three instruments included in the widely-used MUSDB dataset, by leveraging a combination of the MedleyDB and Mixing Secrets datasets. The results show that our proposed multitask model outperforms the baseline Open-Unmix model on the mixture of Mixing Secrets and MedleyDB dataset while maintaining comparable performance on the MUSDB dataset.",
    "extra": {
      "takeaway": "A multitask learning model which incorporates instrument activation detection with source separation."
    }
  },
  {
    "title": "The Jazz Transformer on the Front Line: Exploring the Shortcomings of AI-composed Music through Quantitative Measures",
    "author": [
      " Shih-Lun Wu",
      " Yi-Hsuan  Yang"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245390",
    "url": "https://doi.org/10.5281/zenodo.4245390",
    "ee": "http://archives.ismir.net/ismir2020/paper/000339.pdf",
    "pages": "142-149",
    "zenodo_id": 4245390,
    "dblp_key": null,
    "Abstract": "This paper presents the Jazz Transformer, a generative model that utilizes a neural sequence model called the Transformer-XL for modeling lead sheets of Jazz music. Moreover, the model endeavors to incorporate structural events present in the Weimar Jazz Database (WJazzD) for inducing structures in the generated music. While we are able to reduce the training loss to a low value, our listening test suggests however a clear gap between the average ratings of the generated and real compositions. We therefore go one step further and conduct a series of computational analysis of the generated compositions from different perspectives. This includes analyzing the statistics of the pitch class, grooving, and chord progression, assessing the structureness of the music with the help of the fitness scape plot, and evaluating the model's understanding of Jazz music through a MIREX-like continuation prediction task. Our work presents in an analytical manner why machine-generated music to date still falls short of the artwork of humanity, and sets some goals for future work on automatic composition to further pursue.",
    "extra": {
      "takeaway": "Transformers are actually not good enough for music composition. We tell you why using a set of objective metrics."
    }
  },
  {
    "title": "Polyphonic Piano Transcription Using Autoregressive Multi-State Note Model",
    "author": [
      " Taegyun Kwon",
      " Dasaem  Jeong",
      " Juhan  Nam"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245466",
    "url": "https://doi.org/10.5281/zenodo.4245466",
    "ee": "http://archives.ismir.net/ismir2020/paper/000341.pdf",
    "pages": "454-461",
    "zenodo_id": 4245466,
    "dblp_key": null,
    "Abstract": "Recent advances in polyphonic piano transcription have been made primarily by a deliberate design of neural network architectures that detect different note states such as onset or sustain and model the temporal evolution of the states. The majority of them, however, use separate neural networks for each note state, thereby optimizing multiple loss functions, and also they handle the temporal evolution of note states by abstract connections between the state-wise neural networks or using a post-processing module. In this paper, we propose a unified neural network architecture where multiple note states are predicted as a softmax output with a single loss function and the temporal order is learned by an auto-regressive connection within the single neural network. This compact model allows to increase note states without architectural complexity. Using the MAESTRO dataset, we examine various combinations of multiple note states including on, onset, sustain, re-onset, offset, and off. We also show that the autoregressive module effectively learns inter-state dependency of notes. Finally, we show that our proposed model achieves performance comparable to state-of-the-arts with fewer parameters.  ",
    "extra": {
      "takeaway": "We propose an end-to-end AR transcription predict a softmax output of multi-note-states using a simple uni-directional CRNN"
    }
  },
  {
    "title": "Exact, Parallelizable Dynamic Time Warping Alignment with Linear Memory",
    "author": [
      " Christopher J. Tralie",
      " Elizabeth  Dempsey"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245470",
    "url": "https://doi.org/10.5281/zenodo.4245470",
    "ee": "http://archives.ismir.net/ismir2020/paper/000343.pdf",
    "pages": "462-469",
    "zenodo_id": 4245470,
    "dblp_key": null,
    "Abstract": "Audio alignment is a fundamental preprocessing step in many MIR pipelines. For two audio clips with M and N frames, respectively, the most popular approach, dynamic time warping (DTW), has O(MN) requirements in both memory and computation, which is prohibitive for frame-level alignments at reasonable rates. To address this, a variety of memory efficient algorithms exist to approximate the optimal alignment under the DTW cost. To our knowledge, however, no exact algorithms exist that are guaranteed to break the quadratic memory barrier.  In this work, we present a divide and conquer algorithm that computes the exact globally optimal DTW alignment using O(M+N) memory. Its runtime is still O(MN), trading off memory for a 2x increase in computation.  However, the algorithm can be parallelized up to a factor of min(M, N) with the same memory constraints, so it can still run more efficiently than the textbook version with an adequate GPU. We use our algorithm to compute exact alignments on a collection of orchestral music, which we use as ground truth to benchmark the alignment accuracy of several popular approximate alignment schemes at scales that were not previously possible.",
    "extra": {
      "takeaway": "We design a globally exact, parallelizable algorithm for dynamic time warping alignment that uses only linear memory"
    }
  },
  {
    "title": "Automatic Composition of Guitar Tabs by Transformers and Groove Modeling",
    "author": [
      " Yu-Hua Chen",
      " Yu-Siang  Huang",
      " Wen-Yi  Hsiao",
      " Yi-Hsuan  Yang"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245542",
    "url": "https://doi.org/10.5281/zenodo.4245542",
    "ee": "http://archives.ismir.net/ismir2020/paper/000349.pdf",
    "pages": "756-763",
    "zenodo_id": 4245542,
    "dblp_key": null,
    "Abstract": "Recent years have witnessed great progress in using deep learning algorithms to learn to compose music in the form of a MIDI file.  However, whether such algorithms apply equally well to compose guitar tabs, which are quite different from MIDIs, remain relatively unexplored. To address this, we build a model for composing fingerstyle guitar tabs with a neural sequence model architecture called the Transformer-XL. With this model, we investigate the following research questions. First, whether the neural net generates note sequences with meaningful fingering (i.e., string-fret combinations), which is important for tabs but not for MIDIs. Second, whether it generates compositions with coherent rhythmic grooving, which is crucial for fingerstyle guitar music. And, finally, how pleasant the composed music is in comparison to real, human-made compositions. Our work provides preliminary empirical evidence for the promise of deep learning for guitar tab composition, and suggests areas for future study.",
    "extra": {
      "takeaway": "We study whether a transformer-based neural net can compose fingerstyle guitar tabs with reasonable grooving and fingering."
    }
  },
  {
    "title": "A Computational Analysis of Real-World DJ Mixes using Mix-To-Track Subsequence Alignment",
    "author": [
      " Taejun Kim",
      " Minsuk  Choi",
      " Evan  Sacks",
      " Yi-Hsuan  Yang",
      " Juhan  Nam"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245544",
    "url": "https://doi.org/10.5281/zenodo.4245544",
    "ee": "http://archives.ismir.net/ismir2020/paper/000352.pdf",
    "pages": "764-770",
    "zenodo_id": 4245544,
    "dblp_key": null,
    "Abstract": "A DJ mix is a sequence of music tracks concatenated seamlessly, typically rendered for audiences in a live setting by a DJ on stage. As a DJ mix is produced in a studio or the live version is recorded for music streaming services, computational methods to analyze DJ mixes, for example, extracting track information or understanding DJ techniques, have drawn research interests. Many of previous works are, however, limited to identifying individual tracks in a mix or segmenting it, and the sizes of the datasets are usually small. In this paper, we provide an in-depth analysis of DJ music by aligning a mix to its original music tracks. We set up the subsequence alignment such that the audio features are less sensitive to the tempo or key change of the original track in a mix. This approach provides temporally tight mix-to-track matching from which we can obtain cue-points, transition length, mix segmentation, and musical changes in DJ performance. Using 1,557 mixes from 1001Tracklists including 13,728 tracks and 20,765 transitions, we conduct the proposed analysis and show a wide range of statistics, which may elucidate the creative process of DJ music making.",
    "extra": {
      "takeaway": "An in-depth analysis of the creative process of DJ mixing using mix-to-track subsequence alignment on 1,557 real-world mixes."
    }
  },
  {
    "title": "Explaining Perceived Emotion Predictions in Music: An Attentive Approach",
    "author": [
      " Sanga Chaki",
      " Pranjal  Doshi",
      " Sourangshu  Bhattacharya",
      " Priyadarshi  Patnaik"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245388",
    "url": "https://doi.org/10.5281/zenodo.4245388",
    "ee": "http://archives.ismir.net/ismir2020/paper/000354.pdf",
    "pages": "150-156",
    "zenodo_id": 4245388,
    "dblp_key": null,
    "Abstract": "Dynamic prediction of perceived emotions of music is a challenging problem with interesting applications. Utilization of relevant context in audio sequence is essential for effective prediction. Existing methods have used LSTMs with modest success. In this work we describe three attentive LSTM based approaches for dynamic emotion prediction from music clips. We validate our models through extensive experimentation on standard dataset annotated with arousal-valence values in continuous time, and choose the best performer. We find that the LSTM based attention models perform better than the state of the art transformers for the dynamic emotion prediction task, both in terms of R2 and Kendall-Tau metrics. We explore individual smaller feature sets in search of a more effective one and to understand how different features contribute to perceived emotion. The spectral features are found to perform at par with the generic ComPare feature set [1]. Through attention map analysis we visualize how attention is distributed over music clips' frames for emotion prediction. It is observed that the models attend to frames which contribute to changes in reported arousal-valence values and chroma to produce better emotion predictions, effectively capturing long-term dependencies.",
    "extra": {
      "takeaway": "Attentive LSTMs perform better in dynamic emotion regression in music. Attention maps add interpretability to results. "
    }
  },
  {
    "title": "Bistate reduction and comparison of drum patterns",
    "author": [
      " Olivier Lartillot",
      " Fred  Bruford"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245432",
    "url": "https://doi.org/10.5281/zenodo.4245432",
    "ee": "http://archives.ismir.net/ismir2020/paper/000359.pdf",
    "pages": "318-324",
    "zenodo_id": 4245432,
    "dblp_key": null,
    "Abstract": "This paper develops the hypothesis that symbolic drum patterns can be represented in a reduced form as a simple oscillation between two states, a Low state (commonly associated with kick drum events) and a High state (often associated with either snare drum or high hat). Both an onset time and an accent time is associated to each state. The systematic inference of the reduced form is formalized. This enables the specification of a rhythmic structural similarity measure on drum patterns, where reduced patterns are compared through alignment. The two-state representation allows a low computational cost alignment, once the complex topological formalization is fully taken into account. A comparison with the Hamming distance, as well as similarity ratings collected from listeners on a drum loop dataset, indicates that the bistate reduction enables to convey subtle aspects that goes beyond surface-level comparison of rhythmic textures. ",
    "extra": {
      "takeaway": "Reduction of drum loops in the form of 2-state oscillation. Similarity measure based on alignment between state transitions."
    }
  },
  {
    "title": "Pandemics, music, and collective sentiment: evidence from the outbreak of COVID-19",
    "author": [
      " Meijun Liu",
      " Eva  Zangerle",
      " Xiao  Hu",
      " Alessandro  Melchiorre",
      " Markus  Schedl"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245394",
    "url": "https://doi.org/10.5281/zenodo.4245394",
    "ee": "http://archives.ismir.net/ismir2020/paper/000362.pdf",
    "pages": "157-165",
    "zenodo_id": 4245394,
    "dblp_key": null,
    "Abstract": "The COVID-19 pandemic causes a massive global health crisis and produces substantial economic and social distress, which in turn may cause stress and anxiety among people. Real-world events play a key role in shaping collective sentiment in a society. As people listen to music daily everywhere in the world, the sentiment of music being listened to can reflect the mood of the listeners and serve as a measure of collective sentiment. However, the exact relationship between real-world events and the sentiment of music being listened to is not clear. Driven by this research gap, we use the unexpected outbreak of COVID-19 as a natural experiment to explore how users' sentiment of music being listened to evolves before and during the outbreak of the pandemic. We employ causal inference approaches on an extended version of the LFM-1b dataset of listening events shared on Last.fm, to examine the impact of the pandemic on the sentiment of music listened to by users in different countries. We find that, after the first COVID-19 case in a country was confirmed, the sentiment of artists users listened to becomes more negative. This negative effect is pronounced for males while females' music emotion is less influenced by the outbreak of the COVID-19 pandemic. We further find a negative association between the number of new weekly COVID-19 cases and users' music sentiment. Our results provide empirical evidence that public sentiment can be monitored based on collective music listening behaviors, which can contribute to research in related disciplines.",
    "extra": {
      "takeaway": "We apply causal inference approaches on the LFM-1b dataset of listening events shared on Last.fm, to examine the impact of th"
    }
  },
  {
    "title": "Towards a Formalization of Musical Rhythm",
    "author": [
      " Martin Rohrmeier"
    ],
    "year": 2020,
    "doi": "10.5281/zenodo.4245508",
    "url": "https://doi.org/10.5281/zenodo.4245508",
    "ee": "http://archives.ismir.net/ismir2020/paper/000368.pdf",
    "pages": "621-629",
    "zenodo_id": 4245508,
    "dblp_key": null,
    "Abstract": "Temporality lies at the very heart of music, and the play with rhythmic and metrical structures constitutes a major device across musical styles and genres. Rhythmic and metrical structure are closely intertwined, particularly in the tonal idiom. While there have been many approaches for modeling musical tempo, beat and meter and their inference, musical rhythm and its complexity have been comparably less explored and formally modeled. The model formulates a generative grammar of symbolic rhythmic musical structure and its internal recursive substructure. The approach characterizes rhythmic groups in alignment with meter in terms of the recursive subdivision of temporal units, as well as dependencies established by recursive operations such as preparation and different kinds of shifting (such as anticipation and delay). The model is formulated in terms of an abstract context-free grammar and applies for monophonic rhythms and harmonic rhythm. ",
    "extra": {
      "takeaway": "The paper proposes a formalization of rhythmic structure in terms of a formal grammar "
    }
  }
]
